% !TEX root = ../main.tex

\chapter{Practical Implementation}
\label{ch:practical}

\emph{Part of this research has been described in a journal article in Digital Creativity in 2013, and I presented a paper at the Creativity and Cognition conference 2013 in Sydney.}

\grule % chktex 1

\todo{style inline code}
\todo{run code on laptop and get snippets of all variable contents, e.g.\ faustroll, froll\_dict, \ldots}
\todo{give examples of different results if using different base documents!}
\todo{add section about which pieces of code are not written by me}


The website \url{http://pata.physics.wtf} showcases the current proof-of-concept algorithms. This chapter gives an overview of the structure of the website and the development process.

Typically, software development is divided into so-called front and back ends. The frontend includes web design and web development and is meant to provide an interface for the end-user to communicate with the backend which involves a server, an application and a database (although this is not completely true in this project).

The frontend design is created using the \textbf{w3.css} stylesheet as a basis. The website is mostly responsive, meaning it can be viewed well on phones, tablets and screens (the poems and image spirals for example unfortunately have a fixed width which does not scale down well). The site contains various scripts written in \textbf{Javascript} (e.g. scramble letters, randomise poem, send email and tabbed content).\footnote{frontend links: \url{http://www.w3schools.com/w3css/}, \url{https://www.javascript.com/}}

The backend relies heavily on a \textbf{Python} framework called \textbf{Flask}. Most of the code is written in Python although some parts require a specific templating language called \textbf{Jinja} which renders content into HTML. The application uses several \acrshort{api}'s (Microsoft Translator, Bing, YouTube, Flickr, Getty and WordNet) and is version controlled using \textbf{Git}.\footnote{backend links: \url{https://www.python.org/}, \url{http://flask.pocoo.org/}, \url{http://jinja.pocoo.org/}, \url{https://git-scm.com/}}

The folder structure is as follows:

- app\\
--- static\\
----- css\\
----- images\\
----- corpus\\
--- templates\\
- .git\\
- dev.py\\
- guni.py\\
- live.py\\
- .gitignore\\
- README.md\\
- TODO.txt

\todo{folder structure}

\begin{fcom}
  To provide a short overview, the tools’s workflow can be described as follows:
  \begin{enumerate}
    \item Tokenise texts and remove stopwords to build index,
    \item a query triggers the three pataphysical algorithms,
    \item each algorithm finds results for the query,
    \item retrieve some words before/after match for context, and
    \item render the resulting sentences.
  \end{enumerate}
\end{fcom}

\todo{add audio? update this section depending on what i do}

From the homepage users can choose between text, image and video search. Then they can enter a query --- in the case of text search this should be single words only, image and video search support multi word queries.


\section{Corpus}

Instead of crawling the Internet the present tool uses a local collection of texts in its text-search. The corpus used resembles the fictional library of ``equivalent books'' from Alfred Jarry's \emph{Exploits and Opinions of Dr.\ Faustroll, $'$Pataphysician} \citeyear[p.10-12]{Jarry1996}\footnote{``In addition, three prints hanging on the walls, a poster by TOULOUSE-LAUTREC, \emph{Jane Avril}; one by BONNARD, advertising the \emph{Revue Blanche}; a portrait of Doctor Faustroll, by AUBREY BEARDSLEY\@; and an old picture, which appeared to us to be valueless, \emph{Saint Cado}, issued by the Oberthuer printing house of Rennes.''\parencite[p.12]{Jarry1996}}. In principle the \hypertarget{corpus}{corpus}\label{ref:corpus} is just a folder within the tool's directory structure which contains the following files:

\begin{enumerate}[start=0]
\item Alfred Jarry: \emph{Exploits and Opinions of Dr.\ Faustroll, $'$Pataphysician}
\item Edgar Allen Poe: \emph{Collected Works}
\item Cyrano de Bergerac: \emph{A Voyage to the Moon}
\item Saint Luke: \emph{The Gospel}
\item Leon Bloy: \emph{Le Desespere} (French)
\item Samuel Taylor Coleridge: \emph{The Rime of the Ancient Mariner}
\item Georges Darien: \emph{Le Voleur} (French)
\item Marceline Desbordes-Valmore: \emph{Le Livre des Meres et des Enfants} (French)
\item Max Elskamp: \emph{Enluminures} (French)
\item Jean-Pierre Claris de Florian: \emph{Les Deux Billets} (French)
\item \emph{One Thousand and One Nights}
\item Christian Grabbe: \emph{Scherz, Satire, Ironie und tiefere Bedeutung} (German)
\item Gustave Kahn: \emph{Le Conte de l'Or et Du Silence} (French)
\item Le Comte de Lautreamont: \emph{Les Chants de Maldoror} (French)
\item Maurice Maeterlinck: \emph{Aglavaine and Selysette}
\item Stephane Mallarme: \emph{Verse and Prose} (French)
\item Catulle Mendes: \emph{The Mirror} and \emph{la Divina Aventure} (English and Spanish)
\item Homer: \emph{The Odyssey}
\item Josephin Peladan: \emph{Babylon} (EMPTY FILE)\footnote{I have not been able to find any source texts online.\label{emptyfile}}
\item Francois Rabelais: \emph{Gargantua and Pantagruel}
\item Jean de Chilra: \emph{L'Heure Sexuelle} (EMPTY FILE)\textsuperscript{\ref{emptyfile}}
\item Henri de Regnier: \emph{La Canne de Jaspe} (EMPTY FILE)\textsuperscript{\ref{emptyfile}}
\item Arthur Rimbaud: \emph{Poesies Completes} (French)
\item Marcel Schwob: \emph{Der Kinderkreuzzug} (German)
\item Alfred Jarry: \emph{Ubu Roi} (French)
\item Paul Verlaine: \emph{Poems}
\item Emile Verhaeren: \emph{Poems}
\item Jules Verne: \emph{A Journey to the Centre of the Earth}
\end{enumerate}

The original list as it appears in ``Faustroll'' is shown below. Only three of the items have not been found as a resource. Some others have been approximated by using another text by the same author for example. Most of these were sourced from \textbf{Project Gutenberg}\footnote{See \url{https://www.gutenberg.org/}}\footnote{\textbf{A note on copyright:} Duration of copyright: §5. ``For literary, dramatic, musical or artistic works 70 years from the end of the calendar year in which the last remaining author of the work dies.'' (\url{https://www.copyrightservice.co.uk/ukcs/docs/edupack.pdf}) Maurice Maeterlinck and Marguerite Vallette-Eymery (a.k.a. Rachilde or Jean de Chilra) died less than 70 years ago and their work should still be under copyright. Alfred Jarry in the Simon Watson Taylor translation is a derivative work and is probably also still protected.  (\url{http://www.copyrightservice.co.uk/copyright/p22_derivative_works}) \emph{Fair dealing}: §7. ``Private and research study purposes'', so for the purposes of this project copyright should not apply.} in their original languages.

\begin{enumerate}
\item BAUDELAIRE, a volume of E.A. POE translations.
\item BERGERAC, \emph{Works}, volume II, containing the \emph{Histrory of the States and Empires of the Sun}, and the \emph{History of Birds}.
\item \emph{The Gospel according to} SAINT LUKE, in Greek.
\item BLOY, \emph{The Ungrateful Beggar}.
\item COLERIDGE, \emph{The Rime of the ancient Mariner}.
\item DARIEN, \emph{The Thief}.
\item DESBORDES-VALMORE, \emph{The Oath of the Little Men}.
\item ELSKAMP, \emph{Illuminated Designs}.
\item An odd volume of the \emph{Plays} of FLORIAN\@.
\item An odd volume of \emph{The Thousand and One Nights}, in the GALLAND translation.
\item GRABBE, \emph{Scherz, Satire, Ironie und tiefere Bedeutung}, comedy in three acts.
\item KAHN, \emph{The Tale of Gold and of Silence}.
\item LAUTREAMONT, \emph{The Lays of Maldoror}.
\item MAETERLINCK, \emph{Aglavaine and Selysette}.
\item MALLARME, \emph{Verse and Prose}.
\item MENDES, \emph{Gog}.
\item \emph{The Odyssey}, Teubner's edition.
\item PELADAN, \emph{Babylon}.
\item RABELAIS\@.
\item JEAN DE CHILRA, \emph{The Sexual Hour}.
\item HENRI DE REGNIER, \emph{The Jasper Cane}.
\item RIMBAUD, \emph{The Illuminations}.
\item SCHWOB, \emph{The Childrens' Crusade}.
\item Ubu Roi.
\item VERLAINE, \emph{Wisdom}.
\item VERHAEREN, \emph{The Hallucinated Landscapes}.
\item VERNE, \emph{Voyage to the Center of the Earth}.
\end{enumerate}

\begin{figure}
\centering
\begin{minipage}{.45\linewidth}
  \includegraphics[width=\linewidth]{JaneAvril}
  \caption[Toulouse-Lautrec's ``Jane Avril'']{Toulouse-Lautrec's ``Jane Avril''}
\label{fig:toulouse}
\end{minipage}
\hspace{.05\linewidth}
\begin{minipage}{.45\linewidth}
  \includegraphics[width=\linewidth]{RevueBlanche}
  \caption[Bonnard's ``Revue Blanche'']{Bonnard's ``Revue Blanche''}
\label{fig:bonnard}
\end{minipage}
\vspace{.05\linewidth}
\begin{minipage}{.45\linewidth}
  \includegraphics[width=\linewidth]{DocteurFaustroll}
  \caption[Aubrey Beardsley's ``Docteur Faustroll'']{Aubrey Beardsley's ``Docteur Faustroll''}
\label{fig:beardsley}
\end{minipage}
\hspace{.05\linewidth}
\begin{minipage}{.45\linewidth}
  \includegraphics[width=\linewidth]{SaintCado}
  \caption[Oberthuer's ``Saint Cado'']{Oberthuer's ``Saint Cado''}
\label{fig:oberthuer}
\end{minipage}
\end{figure}


\section{Setup}

When the server is first started various setup functions are executed before any HTML is rendered. The search algorithms are triggered once a user enters a search term into the query field on any of the text, image or video pages.

Each plain text file in the corpus is added to the internal library one by one. Source~\ref{code:addtocorpus} shows how this is done. The \py{PlaintextCorpusReader} is a feature of the \gls{nltk} Python library\footnote{\url{http://www.nltk.org/}} for \acrlong{nlp}.

\begin{listing}
  \begin{minted}{python}
    library = PlaintextCorpusReader(corpus_root, '.*\.txt')
    l_00 = library.words('00.faustroll.txt')
    l_01 = library.words('01.poe1.txt')
    ...
    l_27 = library.words('27.verne.txt')
  \end{minted}
\caption{Adding text files to the corpus library.}
\label{code:addtocorpus}
\end{listing}

The \py{setupcorpus} function (see source~\ref{code:setupcorpus}) is called for each of the text files in the corpus to populate the index data structure \py{l_dict}.

\begin{minted}{c}
  l_dict = dictionary { dictionary { list [ ] } }
\end{minted}

A dictionary in Python is what is known as an `associative array' in other languages. Essentially they are unordered sets of \textbf{key: value} pairs. The \py{l_dict} used here is a dictionary where each key has another dictionary as it's value. Each nested dictionary has a list as the value for each key.

\begin{listing}
  \begin{minted}{python}
    # $f$ = input text file variable
    # $l$ = stopwords file variable
    def setupcorpus(f, l):
        # $x$ = counter/position
        # $w$ = word in file $f$
        for x, w in enumerate(f):
            if w.isalpha() and (w.lower() not in l):
                y = 'l_' + (re.search(r"((\d\d).(\w)+.txt)", f.fileid)).group(2)
                l_dict[w.lower()][y].append(x)
  \end{minted}
\caption{`setupcorpus' function to process the corpus and create the index.}
\label{code:setupcorpus}
\end{listing}

Line 6 in source~\ref{code:setupcorpus} starts looping through file \py{f}. Line 7 checks if the current word \py{w} contains anything other than alphabetical characters and whether or not \py{w} is contained in the relevant stopword file \py{l} (for a list of english stopwords see appendix~\ref{app:code}). If both of those conditions are true variable \py{y} is created on line 8 (such as `l\_00' based on `00.faustroll.txt') and \py{w} is added to \py{l_dict} together with the file \py{y} and the current position \py{x} on line 9. After all files are processed, the index looks like this:

\begin{minted}{c}
  {
    word1: {fileA: [pos1, pos2, ...], fileB: [pos], ...},
    word2: {fileC: [pos1, pos2], fileK: [pos], ...},
    ...
  }
\end{minted}

Using one of the terms from figure~\ref{termdocs} on page~\pageref{termdocs}, here are their entries in the index file (the files are represented by their number in the \hyperlink{corpus}{corpus} (see page~\pageref{ref:corpus}), i.e. \textbf{l\_00} is the `Faustroll' file, \textbf{l\_01} is the `Poe' file, etc.). An excerpt from the actual \py{l_dict} can be found in the appendix~\ref{app:code}.

\begin{minted}{c}
  {
    doctor: {
      l_00: [253, 583, 604, 606, 644, 1318, 1471, 1858, 2334, 2431, 2446, 3039, 4743, 5034, 5107, 5437, 5824, 6195, 6228, 6955, 7305, 7822, 7892, 10049, 10629, 11055, 11457, 12059, 13978, 14570, 14850, 15063, 15099, 15259, 15959, 16193, 16561, 16610, 17866, 19184, 19501, 19631, 21806, 22570, 24867],
      l_01: [96659, 294479, 294556, 294648, 296748, 316773, 317841, 317854, 317928, 317990, 318461, 332118, 338470, 340548, 341252, 383921, 384136, 452830, 453015, 454044, 454160, 454421, 454596, 454712, 454796, 454846, 455030, 455278, 455760, 455874, 456023, 456123, 456188, 456481, 456796, 457106, 457653, 457714, 457823, 457894, 458571, 458918, 458998, 459654, 459771, 490749],
      l_02: [11476, 12098, 28151, 36270],
      l_10: [53085, 53118, 53220, 53266, 53364, 53469, 53573, 53592, 53621, 53718, 54873, 55262, 55525, 55577, 55614, 55683, 55741, 56058, 62709, 113969, 114131, 114405, 114794],
      l_19: [14928, 15702, 49560, 82710, 167218, 180210, 189817, 189908, 190020, 190235, 190905, 199430, 226663, 275454, 275928, 278097, 287375, 291383, 304731, 306055, 324757, 330488],
      l_27: [16270, 79245]
    }, ...
  }
\end{minted}


\section{Text}

After the setup stage is completed and the webpage is fully loaded, user input in the form of a text query is required to trigger the three pataphysical algorithms.

Image and Video search do not use all three algorithms --- where relevant this is highlighted in each section. Generally the following descriptions refer to the text search functionality.

\todo{Explain difference in Text, Image and Video}


\subsection{Clinamen}

The clinamen is the unpredictable swerve that Bök calls ``the smallest possible aberration that can make the greatest possible difference'' \parencite{Boek2002}.

In simple terms, the clinamen algorithm works in two steps:
\begin{enumerate}
  \item get clinamen words based on dameraulevenshtein and faustroll,
  \item get sentences from corpus that match clinamen words.
\end{enumerate}

\todo{find ref for dameraulevenshtein in baeza-yates book?}

It uses the `faustroll' text by Alfred Jarry \citeyear{Jarry1996} as a base document and the Damerau-Levenshtein algorithm \parencite{Damerau1964, Levenshtein1966}, which measures the distance between two strings (with 0 indicating equality), to find words that are similar but not quite the same. The distance is calculated using insertion, deletion, substitution of a single character, or transposition of two adjacent characters. This means that we are basically forcing the program to return matches that are of distance two or one, meaning they have two or one spelling errors in them.

\begin{listing}
  \begin{minted}{python}
    # String $w$ = query word
    # Int $i$ = assigned distance
    def clinamen(w, i):
        words = set([item for item in l_00 if dameraulevenshtein(w, item) <= i])
        out, sources, total = get_results(words, 'Clinamen')
        return out, words, sources, total
  \end{minted}
\caption{Clinamen function}
\label{code:clinamen}
\end{listing}

Source~\ref{code:clinamen} line 4 creates the set of clinamen words using a list comprehension. It retrieves matches from the `faustroll' file \py{l_00} with the condition that they are of Damerau-Levenshtein distance \py{i} or less to the query term \py{w} (see appendix~\ref{app:code}). Duplicates are removed. Line 5 then makes a call to the generic \py{get_results} function to get all relevant result sentences, the list of source files and the total number of results.

\begin{listing}
  \begin{minted}{python}
    # $ws$ = list of words
    # String $a$ = name of algorithm
    def get_results(ws, a):
        total = 0
        out, sources = set(), set()
        for w in ws:
            files = l_dict[w]
            # file $e$, list of positions $ps$
            for e, ps in files.items():
                f = get_title(e)
                sources.add(f)
                sent = pp_sent(w.lower(), e, ps)
                # $o$ = triple of (file, sentence, algorithm)
                o = (f, sent, a)
                if sent != [] and o not in out:
                    total += 1
                    out.add(o)
        return out, sources, total
  \end{minted}
\caption{`get\_results' function to get all sentences for a list of words.}
\label{code:getresults}
\end{listing}

The \py{get_results} function (see source~\ref{code:getresults}) is used by all three algorithms (clinamen, syzygy and antinomy). Given the nested structure of the index \py{l_dict}, the function loops through each of the words passed to it as parameter \py{ws} first and then each file. Line 7 retrieves the dictionary of files from \py{l_dict}. Line 10 gets the author and full title of file \py{e} and adds it to the list of sources in line 11. Line 12 makes use of yet another function called \py{pp_sent} (see source~\ref{code:ppsent}) to get an actual sentence fragment for the current word \py{w} in file \py{e}, which is then added to the output.

\begin{listing}
  \begin{minted}{python}
    # String $w$ = lowercase word
    # String $f$ = name of the file
    # List $ps$ = list of positions of $w$ in $f$
    def pp_sent(w, f, ps):
        # $pos$ = the FIRST OCCURANCE of $w$ in $f$
        out, pos = [], ps[0]
        # $ff$ = the variable for file $f$
        ff = eval(f)
        pos_b, pos_a = pos, pos
        punct = [',', '.', '!', '?', '(', ')', ':', ';', '\n', '-', '_']
        for i in range(1, 10):
            if ff[pos - i] in punct:
                pos_b = pos - (i - 1)
                break
            else:
                if ff[pos - 5]:
                    pos_b = pos - 5
                else:
                    pos_b = pos
        for j in range(1, 10):
            if ff[pos + j] in punct:
                pos_a = pos + j
                break
            else:
                if ff[pos + 5]:
                    pos_a = pos + 5
                else:
                    pos_a = pos
        if pos_b >= 0 and pos_a <= len(ff):
            pre = ' '.join(ff[pos_b:pos])
            post = ' '.join(ff[pos+1:pos_a])
            out = (pre, w, post)
        return out
  \end{minted}
\caption{`pp\_sent' function to retrieve a sentence from a file.}
\label{code:ppsent}
\end{listing}

In function \py{pp_sent} (source~\ref{code:ppsent}) line 6 is important to note because it is a key functionality point. Even though the index \py{l_dict} stores a full list of all possible positions of a given word in each file, the \py{pp_sent} function \textbf{only retrieves the sentence of the very first occurance of the word} rather than each one. This decision was taken to avoid overcrowding of results for the same keyword.

Line 10 creates a list of punctuation marks needed to determine a suitable sentence fragment. Lines 11--19 and 20--28 set the \py{pos_b} (position \textbf{b}efore) and \py{pos_a} (position \textbf{a}fter) variables respectively. These positions can be up to 10 words before and after the keyword \py{w} depending on the sentence structure. In line 30 the actual sentence fragment up to the keyword is retrieved, while in line 31 the fragment just after the keyword is retrieved. \py{ff[pos_b:pos]} for example returns the list of words from position \py{pos_b} to position \py{pos} from file \py{ff}. The built-in Python \py{.join()} function then concatenates these words into one long string separated by spaces. On line 32 a triple containing the pre-sentence, keyword and post-sentence is set as the output and then returned.

The image/video searches don't use the clinamen function at all.


\subsection{Syzygy}

The syzygy surprises and confuses. It originally comes from astronomy and denotes the alignment of three celestial bodies in a straight line. In a pataphysical context it is the pun. It usually describes a conjunction of things, something unexpected and surprising. Unlike serendipity, a simple chance encounter, the syzygy has a more scientific purpose.

In simple terms, the syzygy algorithm works in two steps:
\begin{enumerate}
  \item get syzygy words based on synsets and hypo-, hyper- and holonyms from WordNet,
  \item get sentences from corpus that match syzygy words.
\end{enumerate}

\begin{listing}
  \begin{minted}{python}
    # $w$ = input query term
    def syzygy(w):
        words = set()
        wordsets = wn.synsets(w)
        for ws in wordsets:
            words.update(get_nym('hypo', ws))
            words.update(get_nym('hyper', ws))
            words.update(get_nym('holo', ws))
        out, sources, total = get_results(words, 'Syzygy')
        return out, words, sources, total
  \end{minted}
\caption{Syzygy function.}
\label{code:syzygy}
\end{listing}

The syzygy function makes heavy use of WordNet \parencite{Miller1995} through the \gls{nltk} Python library to find suitable results. Specifically, as shown in source~\ref{code:syzygy}, the algorithm fetches the set of synonyms (synsets) on line 4. It then loops through all individual items \py{ws} in the list of synonyms \py{wordsets} in line 5--8. It finds any hyponyms, hypernyms or holonyms for each \py{ws} (each of which denotes some sort of relationship or membership with its parent synonym) using the \py{get_nym} function.

\todo{explain reasoning behind algorithms like this for all:}
This mimics a syzygy alignment of three words in a line (query $\to$ synonym $\to$ hypo/hyper/holonym).

Line 9 makes use of the \py{get_results} function (see source~\ref{code:getresults}) in the same was as the clinamen function does.

\todo{rewrite getnym function to automatically get all three without the ifs}

The image and video searches both use the syzygy function as part of their \py{pataphysicalise} function (see source~\ref{code:pataph}).


\subsection{Antinomy}

The antimony, in a pataphysical sense, is the mutually incompatible.

In simple terms, the antinomy algorithm works in two steps:
\begin{enumerate}
  \item get antinomy words based on synsets and antonyms from WordNet,
  \item get sentences from corpus that match antinomy words.
\end{enumerate}

\begin{listing}
  \begin{minted}{python}
    # $w$ = input query term
    def antinomy(w):
        words = set()
        wordsets = wn.synsets(w)
        for ws in wordsets:
            anti = ws.lemmas()[0].antonyms()
            if len(anti) > 0:
                for a in anti:
                    if str(a.name()) != w:
                        words.add(str(a.name()))
        out, sources, total = get_results(words, 'Antinomy')
        return out, words, sources, total
  \end{minted}
\caption{Antinomy function.}
\label{code:antinomy}
\end{listing}

For the antinomy we simply used WordNet's antonyms (opposites) (see source~\ref{code:antinomy}). This algorithm is very similar to the algorithm for the syzygy. It finds all antonyms through WordNet and retrieves result sentences using the \py{get_results} function.


\section{Image/Video}

In simple terms, the image and video search works in three steps:
\begin{enumerate}
  \item pataphysicalise query terms using syzygy algorithm
  \item translate each pataphysicalised term
  \item retrieve images/videos using \acrshort{api} calls
\end{enumerate}

The \py{pataphysicalise} function (see source~\ref{code:pataph}) transforms the original query terms ready for the next step. In line 5 the \py{syzygy} algorithm (source~\ref{code:syzygy}) is used to make this transformation. Given that the image and video search allows multi-word queries and the \py{syzygy} function returns several new words per query terms, this creates a long list of entries. On top of that the output is the inner product (line 8) of all these results. The purpose of producing so many pataphysicalisations is to find more results using the \glspl{api}.

\begin{listing}
  \begin{minted}{python}
    # $words$ = query terms
    def pataphysicalise(words):
        sys_ws = []
        for word in words:
            _, w, _, _ = syzygy(word)
            if len(w) > 0:
                sys_ws.append(list(w))
        out = itertools.product(*sys_ws)
        return list(out)
  \end{minted}
\caption{Function to pataphysicalise image and video query terms.}
\label{code:pataph}
\end{listing}

The next step is to translate the pataphysicalised search terms as shown in source~\ref{code:transent}.

\todo{print examples of pataphysicalise function in action}

\begin{listing}
  \begin{minted}{python}
    def transent(sent):
        translator = Translator(microsoft_id, microsoft_secret)
        french = translator.translate(sent, "fr")
        japanese = translator.translate(french, "ja")
        patawords = translator.translate(japanese, "en")
        translations = (french, japanese, patawords)
        return translations
  \end{minted}
\caption{Translation function.}
\label{code:transent}
\end{listing}


\section{Results}

Once the three algorithms have produced their respective results, the page displaying these results can be rendered. This is done using the templating language Jinja and HTML.

\begin{figure}[htb] % (here, top, bottom, page)
  \centering
  \includegraphics[height=0.6\textheight]{images/prototype01}
\caption[Prototype 1 screenshot]{Prototype 1 screenshot}
\label{img:Prototype1xx}
\end{figure}

The text results page has three options for how the results are presented, with `Poetry --- Queneau' being the default.
\begin{description}
  \item [Poetry] Displayed in sonnet style (two quatrains and two tercets) if possible, although no rhyming pattern is used.\footnote{\url{https://en.wikipedia.org/wiki/Sonnet}}
    \begin{itemize}
      \item Queneau --- Each line can be changed manually.
      \item Random --- The whole poem can be randomised.
    \end{itemize}
  \item [Sources] Ordered by source text.
  \item [Algorithms] Ordered by algorithm.
\end{description}

The image and video results pages work the same way. They both have two display options, with the `Spiral' option being the default.
\begin{description}
  \item [Spiral] Displayed square images/videos as a golden spiral.
  \item [List] Displayed as a simple list.
\end{description}


\section{Prototypes}

\subsection{Prototype 1}

\todo{get new screenshots for prototype 1}
\todo{don't mention James?}

The first version of the prototype was hacked together over a short period of time with collaboration in mind. It was originally build to demonstrate the three algorithms in action before James' architecture was finished. The design of the website was simple and plain.

Results were displayed in three sets per algorithm. Each keyword was preceded and followed by exactly 5 words respectively.

\begin{figure}[htb] % (here, top, bottom, page)
  \centering
  \includegraphics[height=0.6\textheight]{images/prototype01}
\caption[Prototype 1 screenshot]{Prototype 1 screenshot}
\label{img:Prototype1x}
\end{figure}


\subsection{Prototype 2}

{\Huge \url{pata.fania.eu}}

This version introduced the move from Django to Flask. It also included the first major re-design of the website.


\begin{figure}[htb] % (here, top, bottom, page)
  \centering
  \includegraphics[width=\linewidth]{images/prototype02}
\caption[Prototype 2 screenshot]{Prototype 2 screenshot}
\label{img:Prototype2x}
\end{figure}




\begin{alltt}
# TEXT

## setup

1.\ read in faustroll book
2.\ create `froll_dict' dictionary from text
3.\ remove stopwords and numbers from `froll_dict'

## text algorithm

1.\ get query term
2.\ execute three functions:

  2a.\ syzygy algorithm
    1.\ get list of synonyms
    2.\ for each synonym do the following:
      a.\ find hyponyms; if a hyponym occurs in `froll_dict' then add to the output list
      b.\ find hypernyms; if a hypernym occurs in `froll_dict' then add to the output list
      c.\ find holonyms; if a holonym occurs in `froll_dict' then add to the output list
    3.\ return list of syzygy words

  2b.\ antinomy algorithm
    1.\ get list of synonyms
    2.\ for each synonym do the following:
      a.\ find antonyms; if a antonym occurs in `froll_dict' then add to the output list
    3.\ return list of antinomy words

  2c.\ clinamen algorithm
    1.\ find list of words within `froll_dict' that have a `dameraulevenshtein distance' of 1 or 2 (meaning, there are 1 or 2 spelling errors)
    2.\ return list of clinamen words

3.\ get sentences for all three output lists

  3a.\ if the word appears in faustroll then find the nearest 5 words before and after the word
  3b.\ return list of sentences

4.\ render results as html


---

# IMAGES

## setup

- microsoft translate API key
- flickr API key
- (bing image search API key) --- not used atm

## image algorithm

1.\ get query word
2.\ get one syzygy word using syzygy algorithm 2a above
3.\ translation party
  3a.\ translate english to french
  3b.\ translate french to japanese
  3c.\ translate japanese to english
4.\ get images
  4a.\ search flickr for 10 matches to english translation
  4b.\ get metadata for each
  4c.\ add title, thumb, link to output list
5.\ return output list
6.\ render results as html

---

# VIDEOS

## setup

- microsoft translate API key
- youtube API stuff
- (bing video search API key) --- not used atm

## video algorithm

1.\ get query word
2.\ get one syzygy word using syzygy algorithm
3.\ translation party
  3a.\ translate english to french
  3b.\ translate french to japanese
  3c.\ translate japanese to english
4.\ get videos
  4a.\ search YouTube for 10 matches to english translation
  4b.\ get metadata for each
  4c.\ add title, thumb, link to output list
5.\ return output list
6.\ render as html
\end{alltt}
