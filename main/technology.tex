% !TEX root = ../main.tex

\chapter{Technology}
\label{ch:technology}

\startcontents[chapters]

\vfill

\begin{alltt}\sffamily
On entering his study his steward presented him,
and commanding the field of Battle,
he invited me to study under him in his home in the fatherland,
and fatness of an historiated field of cabbages.

Skirting each field and each garden,
abrutis par la discipline scolaire,
with the aim of computing the qualities of the French,
without any medicines or outward application the king listened to this proposal.

Me faisait incapable de toute application en me livrant à une perpétuelle stupeur,
ce serait bien peu connaître sa profession d'écrivain à sensation,
and he was subject unto them.

Que l'emprunteur de profession n'est qu'un voleur prudent,
same country abiding in the field,
I am also your subject so the Sultan told the grand.
\end{alltt}

\newpage
\minicontents
\spirals

Knowledge needed to understand project:
\begin{itemize}
  \item Search engines
  \item index
  \item corpus
  \item query --- expansion etc
  \item results
  \item searching vs browsing
  \item Web programming
  \item
\end{itemize}

\todo{update all graphics with inkscape}

\section{Information Retrieval}

\begin{quotation}
  Information retrieval deals with the representation, storage, organisation of, and access to information items such as documents, Web pages, online catalogs, structured and semi-structured records, multimedia objects. The representation and organisation of the information items should be such as to provide the users with easy access to information of their interest. \sourceatright{\autocite{Baeza-Yates2011}}
\end{quotation}

In simple terms, a typical search process can be described as follows. A user is looking for some information so she or he types a search term or a question into the text box of a search engine. The system analyses this query and retrieves any matches from the index, which is kept up to date by a Web crawler. A ranking algorithm then decides in what order to return the matching results and displays them for the user. In reality of course this process involves many more steps and level of detail, but it provides a sufficient enough overview. See figure~\ref{fig:SEA}.

\begin{figure}[!htbp]
  \centering
  \input{images/architecture.pdf_tex}
  \caption[Search Engine Architecture]{Abstract search engine architecture}
\label{fig:SEA}
\end{figure}

Most big Web search engines like Google, Baidu or Bing focus on usefulness and relevance of their results.\autocite{Google2012, Baidu2012, Microsoft2012a} Google uses over 200 signals \autocite{Google2012} that influence the ranking of Web pages including their original PageRank algorithm \autocite{Brin1998, Brin1998b}.

Any \gls{ir} process is constrained by factors like subject, context, time, cost, system and user knowledge \autocite{Marchionini1988}. Such constraints should be taken into consideration in the development of any search tool. A Web crawler needs resources to crawl around the Web, language barriers may exist, the body of knowledge might not be suitable for all queries, the system might not be able to cater for all types of queries (e.g.\ multi-word queries), or the user might not be able to understand the user interface, and many more. It is therefore imperative to eliminate certain constraining factors (for example by choosing a specific target audience or filtering the amount of information gathered by a crawler from Web pages).


\paragraph{Crawler}

The crawler, sometimes called spider, indexer or bot, is a program that processes and archives information about every available webpage it can find. It does this by looking at given `seed' pages and searching them for hyperlinks. It then follows all of these links and repeats the process over and over. The Googlebot\footnote{\href{https://support.google.com/webmasters/answer/182072}{Googlebot} (\url{https://support.google.com/webmasters/answer/182072})} and the Bingbot\footnote{\href{http://www.bing.com/webmaster/help/which-crawlers-does-bing-use-8c184ec0}{Bingbot} (\url{http://www.bing.com/webmaster/help/which-crawlers-does-bing-use-8c184ec0})} are well-known examples.


\paragraph{Index}

An index is a list of keywords (called the dictionary or vocabulary) together with a list (called postings list) that indicates the documents in which the terms occurs. One way to practically implement this is to create a \gls{tdm}. In this case $f_{i,j}$ is the frequency of term $k_{i}$ in document $d_{j}$.

\begin{equation}
  \bbordermatrix{~ & d_1 & d_2 \cr % chktex 41
        k_1 & f_{1,1} & f_{1,2} \cr % chktex 41
        k_2 & f_{2,1} & f_{2,2} \cr % chktex 41
        k_3 & f_{3,1} & f_{3,2}}
\label{eq:tdm}
\end{equation}
% \myequations{Term-document matrix}

\todo{example TDM for faustroll sentence?}

\begin{figure}[!htbp]
\[
  \bbordermatrix{
    ~ & Faustroll & Gospel & Voyage \cr % chktex 41
    Faustroll & 77 & 0 & 0 \cr % chktex 41
    father & 1 & 28 & 2 \cr % chktex 41
    time & 34 & 16 & 129 \cr % chktex 41
    purpose & 2 & 0 & 3 \cr % chktex 41
    little & 28 & 16 & 81 \cr % chktex 41
    background & 0 & 0 & 0 \cr % chktex 41
    water & 29 & 7 & 120 \cr % chktex 41
    doctor & 30 & 0 & 0 \cr % chktex 41
    without & 27 & 7 & 117 \cr % chktex 41
    skiff & 35 & 0 & 0 \cr % chktex 41
    bishop & 27 & 0 & 2 \cr % chktex 41
    God & 25 & 123 & 2 \cr % chktex 41
    substance & 8 & 3 & 1 \cr % chktex 41
    issue & 0 & 2 & 2 \cr % chktex 41
    watch & 5 & 3 & 6
  }
\]
\caption[Various wordcounts]{Various wordcounts in Faustroll, Gospel and Voyage}
\label{termdocs}
\end{figure}

Total wordcount of files: Faustroll=131891, Gospel=139669, Voyage=497295.

\todo{cross references with hyperlink hypertarget}
The dictionary is usually \hyperlink{stemming}{preprocessed} to eliminate punctuation and stop-words (e.g. I, a, and, be, by, for, the, on, etc.) that would be useless in everyday text search engines. For specific domains it even makes sense to build a `controlled vocabulary' which can be seen as a domain specific taxonomy and are very useful for query expansion.

%
% s3://aws-publicdatasets/cc-index/collections/[CC-MAIN-YYYY-WW]/indexes/
% s3://aws-publicdatasets/cc-index/collections/[CC-MAIN-2015-22]/indexes/segments/1432207930423.94/warc/CC-MAIN-20150521113210-00247-ip-10-180-206-219.ec2.internal.warc.gz
%
% https://gist.github.com/Smerity/e750f0ef0ab9aa366558#file-bbc-wet
%
% common-crawl/crawl-data/CC-MAIN-2015-22/segments/1432207930423.94/warc/CC-MAIN-20150521113210-00247-ip-10-180-206-219.ec2.internal.warc.gz


\paragraph{Ranking}

Ranking is the process of ordering search results using a given weight. One simple method of ranking is the so-called \acrlong{tf}-\acrlong{idf} or \acrshort{tf}-\acrshort{idf} for short. Given a \gls{tf} weight of $tf_{i,j}$ and a \gls{idf} weight of $idf_j$ it is defined as $tf_{i,j}\times idf_j$.

\begin{equation}
  w_{i,j} =
  \begin{cases}
  (1+\log f_{i,j})\times \log\frac{N}{df_i} & \text{if} \ f_{i,j} > 0 \\
  0 & \text{otherwise}
  \end{cases}
  \label{eq:wij}
\end{equation}
% \myequations{TF-IDF Ranking}

Where $w_{i,j}$ is the weight associated with $(k_i,d_j)$. Using this formula ensures that rare terms have a higher weight and more so if they occur a lot in one document.

The \gls{tf} $tf_{i,j}$ is calculated and normalised using a log function as:
$1+\log f_{i,j} \ \text{if} \ f_{i,j} > 0 \ \text{or} \ 0 \ \text{otherwise}$.

The total \gls{tf} $F_i$ is calculated as $\sum_{j=1}^{N}f_{i,j}$, where $F_i$ is the total frequency of term $k_i$ in the collection and $f_{i,j}$ is the frequency of occurrence of term $k_i$ in document $d_j$ and $N$ is the total number of documents.

The \gls{idf} $idf_j$ weight is calculated as $\log \frac{N}{df_i}$, where the document frequency $df_i$ is the number of documents in a collection that contain a term $k_i$ and $idf_i$ is the \gls{idf} of term $k_i$. The more often a term occurs in different documents the lower the \gls{idf}.

% \begin{equation}
%   tf_{i,j}=
%   \begin{cases}
%   1+\log f_{i,j} & \text{if} \ f_{i,j} > 0\\
%   0 & \text{otherwise}
%   \end{cases}
%   \label{eq:tfij}
% \end{equation}
% % \myequations{tfij}

% \begin{equation}
%   F_i = \sum_{j=1}^{N}f_{i,j}
%   \label{eq:Fi}
% \end{equation}
% % \myequations{Fi}

% \begin{equation}
%   idf_j = \log \frac{N}{df_i}
%   \label{eq:idfj}
% \end{equation}
% % \myequations{idfj}


\subsection{Searching vs. Browsing}

\todo{rewrite to match current style}

What do we actually mean by searching? Usually it implies that there is something to be found, an \gls{in}; although that doesn’t necessarily mean that the searcher knows what he or she is looking for or how to conduct the search and satisfy that need.

From the users' point of view the search process can be broken down into four activities \autocite{Sutcliffe1998} reminiscent of classic problem solving techniques \autocite{Polya1957}:

\begin{description}
  \item [Problem identification] \gls{in},
  \item [Need articulation] \gls{in} in natural language terms,
  \item [Query formulation] translate \gls{in} into query terms, and
  \item [Results evaluation] compare against \gls{in}.
\end{description}

This model poses problems when we consider a situation where an \gls{in} cannot easily be articulated or in fact is not existent and the user is not looking for anything. This is not the only constraining factor though and Marchionini and Shneiderman have pointed out that `the setting within which information-seeking takes place constrains the search process' \autocite{Marchionini1988} and they laid out a framework with the following main elements.

\begin{itemize}
  \item Setting (the context of the search and external factors such as time, cost)
  \item Task domain (the body of knowledge, the subject)
  \item Search system (the database or web search engine)
  \item User (the user’s experience)
  \item Outcomes (the assessment of the results/answers)
\end{itemize}

Searching can be thought of in two ways, information lookup (\textbf{searching}) and exploratory search (\textbf{browsing}) \autocite{DeVries1993, Marchionini2006}. A situation where an \gls{in} cannot easily be articulated or in fact is not existent (the user is not looking for anything specific) can be considered a typical case of exploratory search and describes the kind of search that is most suited to our proposed tool. The former can be understood as a type of simple question answering while the latter is a more general and broad knowledge acquisition process without a clear goal.

Current web search engines are tailored for information lookup. They do really well in answering simple factoid questions relating to numbers, dates or names (e.g.\ fact retrieval, navigation, transactions, verification) but not so well in providing answers to questions that are semantically vague or require certain extend of interpretation or prediction (e.g.\ analysis, evaluation, forecasting, transformation).

When it comes to exploratory search though, the user’s success in finding the right information depends a lot more on constraining factors such as those mentioned earlier and can sometimes benefit from a combination of information lookup and exploring \autocite{Marchionini2006}.

\begin{quotation}
  Much of the search time in learning search tasks is devoted to examining and comparing results and reformulat-ing queries to discover the boundaries of meaning for key concepts. Learning search tasks are best suited to combinations of browsing and analytical strategies, with lookup searches embedded to get one into the correct neighbourhood for exploratory browsing. \sourceatright{\autocite{Marchionini2006}}
\end{quotation}

De Vries called this form of browsing an `enlargement of the problem space', where the problem space refers to the resources that possibly contain the answers/solutions to the information need \autocite{DeVries1993}. This is a somewhat similar idea to that of Boden’s conceptual spaces which she called the `territory of structural possibilities' and exploration of that space `exploratory creativity' \autocite{Boden2003}.

All of these ideas, however, seem to be concerned with how users interact with a search system, rather than how the system acts itself. So we need to shift our perspective and think about how a search tool can be more supportive for exploratory search directly and by what means.


\subsection{IR Models}

\begin{fcom}
  \gls{ir} models describe ranking algorithms formally. ???
\end{fcom}

There are different models for different needs, for example a multimedia system is going to be different than a text based system, or a Web based system is going to be different than an offline database system. Even within one such category there could more than one model. Take text based search systems for example. Text can be unstructured or semi-structured. Web pages are typically semi-structured. They contain a title, different sections or paragraphs and so on. An unstructured page would have no such differentiations but only contain simple text.  Classic example models are set theoretic, algebraic and probabilistic. The PageRank algorithm by Google is a link-based retrieval model.

The notation for \gls{ir} models is as follows \autocite[adapted from][p.58]{Baeza-Yates2011}:

An \gls{ir} model is a quadruple $[D,Q,F,R(q_i, d_j)]$ where:\\
\itab{$D$} \tab{is the set of documents,}\\
\itab{$Q$} \tab{is the set of queries,}\\
\itab{$F$} \tab{is the framework e.g.\ sets, Boolean relations, vectors}\\
\itab{}    \tab{linear algebra\ldots}\\
\itab{$R(q_i, d_j)$}
            \tab{is the ranking function, where $q_i \in Q$ and $d_j \in D$,}\\
\itab{$t$} \tab{is the number of index terms in a document collection,}\\
\itab{$V =\{k_1,\ldots, k_t\}$}
            \tab{is the set of all distinct index terms in a document}\\
\itab{}     \tab{collection (vocabulary).}

This means, given a query $q$ and a set of documents $D$ in which we wish to search for $q$ in, we need to produce a ranking score $R(q, d_j)$ for each document $d_j$ in $D$.

\todo{decide on which method for highlighting words --- italic or apostophe}


\subsubsection{The Boolean Model}

One such ranking score is the Boolean model. The similarity of document $d_j$ to query $q$ is defined as follows (quoted from \autocite[p.65]{Baeza-Yates2011})

\begin{equation}
  sim(d_j,q) =
  \begin{cases}
  1 & \text{if} \ \exists \ c(q) \ | \ c(q) = c(d_j)\\
  0 & \text{otherwise}
  \end{cases}
  \label{eq:sim}
\end{equation}
% \myequations{sim}

A `conjunctive component' describes which terms occur in a document and which ones do not. E.g.\ for vocabulary $V =\{k_{1},\ldots, k_{t}\}$, if the terms $[ k_{1},k_{2},k_{3}]$ occur in document $d_{j}$ then the conjunctive component would be $(1,1,1)$, or $(1,0,0)$ if only term $k_{1}$ appears in $d_{j}$.

% \begin{description}
%   \item [$c(d)$] is the term conjunctive component for document $d$
%   \item [$c(q)$] is the term conjunctive component for query $q$
% \end{description}

\itab{$c(d)$}\tab{is the term conjunctive component for document $d$}\\
\itab{$c(q)$}\tab{is the term conjunctive component for query $q$}

Sometimes things are not quite black and white though and we need to weigh the importance of words somehow. The easiest way to do that is by looking at the frequency in which a word occurs.



\subsubsection{The Vector Model}

The vector model allows a more flexible scoring since it basically computes the various degrees of similarity between documents (taken from \autocite[p.78]{Baeza-Yates2011}).

\begin{equation}
  \begin{split}
  \vec{d_j} &= (w_{1,j}, w_{2,j}, \ldots, w_{t,j})\\
  \vec{q} &= (w_{1,q}, w_{2,q}, \ldots, w_{t,q})
  \end{split}
  \label{eq:vector}
\end{equation}
% \myequations{vector}

\begin{figure}[!htbp] % (here, top, bottom, page)
  \centering
  \begin{tikzpicture}
  \draw
  (3,0) coordinate (a) node[right] {$q$}
  -- (0,0) coordinate (b) node[left] {}
  -- (2,2) coordinate (c) node[above right] {$d_j$}
  pic["$\theta$", draw=black, angle eccentricity=1.2, angle radius=1cm]
  {angle=a--b--c};
  \end{tikzpicture}
\caption[Vector Model]{The Vector Model}
\label{fig:VM}
\end{figure}

Where $t$ is the total number of terms in the index and $w_{i,j}$ is the TF-IDF weight for each component of the vector. The similarity between the document and the query vector is the cosine of $\theta$.

\begin{equation}
  \begin{split}
  sim(d_j,q) &= \frac{\vec{d_j} \ \cdot \ \vec{q}}{|\vec{d_j}| \times |\vec{q}|}\\
  &= \frac{\sum_{i=1}^{t}w_{i,j} \times w_{i,q}}
  {\sqrt{\sum_{i=1}^{t}w_{i,j}^{2}} \times \sqrt{\sum_{i=1}^{t}w_{i,q}^{2}}}
  \end{split}
  \label{eq:sim2}
\end{equation}
% \myequations{sim2}

Here is an example algorithm for computing this score taken from \autocite[p.125]{Manning2009}.

\begin{listing}[htb]
  \begin{minted}{python}
    CosineScore (q)
      float Scores[N] = 0
      for each d
      do Initialise Length[d] to the length of document d
      for each query term t
      do calculate wt,q and fetch postings list for t
        for each pair (d, tft,d) in postings list
        do add wft,d to Scores[d]
      Read the array Length[d]
      for each d
      	do Divide Scores[d] by Length[d]
      	return Top K components of Scores[]
  \end{minted}
\caption[Pseudo-code for computing vector scores]{Pseudo-code for computing vector scores}
\label{code:VectorScores}
\end{listing}

Where,\\
\itab{$q$} \tab{is the query}\\
\itab{$N$} \tab{is the total number of documents}\\
\itab{$d$} \tab{is a document}\\
\itab{$t$} \tab{is a query term}\\
\itab{$wt_q$} \tab{is the weight of the term in the query}\\
\itab{$tft_d$} \tab{is the term frequency of $t$ in $d$}\\
\itab{$wft_d$} \tab{is the $tf-idf$ weight of t in $d$}\\
\itab{$K$} \tab{is the number of results we want}\\
\itab{$postings list$} \tab{is the list of all ($d$, $tft_d$) for a given $t$.}

There are several other common \gls{ir} models that I won't discuss in detail here. These include the probabilistic, set-based, extended Boolean and fuzzy set {\sloppy \autocite{Miyamoto2010, Miyamoto1988, Srinivasan2001, Widyantoro2001, Miyamoto1986}} models or latent semantic indexing \autocite{Deerwester1990}, neural network models and others \autocite{Macdonald2009, Schuetze1998, Schuetze}.



\subsubsection*{Architecture}





\subsubsection*{Search Algorithms}


\subsection{Ranking}

Ranking signals contribute to the improvement of the ranking process. These can be content signals or structural signals. Content signals are referring to anything that is concerned with the text and content of a page. This could be simple word counts or the format of text such as headings and font weights. The structural signals are more concerned about the linked structure of pages. They look at incoming and outgoing links on pages. There are also Web usage signals that can contribute to ranking algorithms such as the clickstream.  This also includes things like the Facebook `like' button or the Google+ `+1' button which could be seen as direct user relevance feedback as well.

Ranking algorithms are the essence of any Web search engine and as such guarded with much secrecy. They decide which pages are listed highest in search results and if their ranking criteria were known publically, the potential for abuse (such as Google bombing\footnote{\url{http://www.searchenginepeople.com/blog/incredible-google-bombs.html}} for instance) would be much higher and search results would be less trustworthy. Despite the secrecy there are some algorithms like Google's PageRank algorithm that have been described and published in academic papers. Here is a survey of the most notable algorithms.

PageRank was developed in 1998 by Larry Page and Sergey Brin as part of their Google search engine and announced in their often cited paper \autocite{Brin1998b} and they further describe the algorithm here \autocite{Brin1998}. PageRank is a link analysis algorithm, meaning it looks at the incoming and outgoing links on pages. It assigns a numerical weight to each document, where each link counts as a vote of support in a sense. PageRank is executed at indexing time, so the ranks are stored with each page directly in the index. The following formula for calculating a PageRank PR is taken from \autocite[p.472]{Baeza-Yates2011}.

\begin{figure}[!htbp] % (here, top, bottom, page)
  \centering
  \includegraphics[width=.75\linewidth]{pagerank}
\caption[PageRank algorithm]{PageRank algorithm illustration from Wikipedia}
\label{fig:pagerank2}
\end{figure}

\begin{equation}
  PR(a) =
  \frac{q}{T} + (1 - q)
  \sum_{i=1}^{n} \frac{PR(p_i)}{L(p_i)}
  \label{eq:PRt}
\end{equation}
% \myequations{PR}

Where,\\
\itab{$L(p)$} \tab{is the number of outgoing links of page $p$,}\\
\itab{$a$} \tab{is the page we want to rank and is pointed to by pages $p_1$ to $p_n$,}\\
\itab{$T$} \tab{is the total number of pages on the Web graph, and}\\
\itab{$q$} \tab{is the is a parameter to be set by the system (typically 0.15)}\\
\itab{} \tab{needed to deal with dead ends in the graph.}

The HITS algorithm also works on the links between pages. It was first described by Kleinberg \autocite[p.472]{Kleinberg1999, Kleinberg} in 1999. HITS stands for Hyperlink Induced Topic Search and its basic features are the use of so called hubs and authority pages. It is executed at query time. Pages that have many incoming links are called authorities and page with many outgoing links are called hubs. Again, the following formula is taken from \autocite[p.471]{Baeza-Yates2011}. S is the set of pages.

\begin{equation}
  \begin{split}
  H(p) &= \sum_{u\in S \mid p\to u}A(u)\\
  A(p) &= \sum_{v\in S \mid v\to p}H(v)
  \end{split}
  \label{eq:HITS}
\end{equation}
% \myequations{HITS}

Hilltop is a similar algorithm with the difference that it operates on a specific set of expert pages as a starting point. It was defined by Bharat and Mihaila in 2000 in \autocite{Bharat2000}. The expert pages they refer to should have many outgoing links to non-affiliated pages on a specific topic. This set of expert pages needs to be pre-processed at the indexing stage. The authority pages they define must be linked to by one of their expert pages. The main difference to the HITS algorithm then is that their `hub' pages are predefined.

Another algorithm is the so called Fish search algorithm. It was first described by De Bra in 1994 \autocite{DeBra1994, DeBra1994a, DeBra}. The basic concept here is that the search starts with the search query and a seed URL as a starting point. A list of pages is then built dynamically in order of relevance following from link to link. Each node in this directed graph is given a priority depending on whether it is judged to be relevant or not. URLs with higher priority are inserted at the front of the list while others are inserted at the back. Special here is that the `ranking' is done dynamically at query time.

There are various algorithms that follow this approach. For example the shark search algorithm \autocite{Hersovici1998}. It improves the process of judging whether or not a given link is relevant or not. It uses a simple vector model with a fuzzy sort of relevance feedback. Another example is the improved fish search algorithm in \autocite{Luo2005} where the authors have simply added an extra parameter to allow more control over the search range and time. The Fish School Search algorithm is another approach based on the same fish inspiration \autocite{BastosFilho2008}. It uses principles from genetic algorithms and particle swarm optimization. Another genetic approach is Webnaut \autocite{Nick2001}.

Other variations include the incorporation of user behaviour \autocite{Agichtein2006}, social annotations \autocite{Bao2007}, trust \autocite{Garcia-Molina2004}, query modifications \autocite{Glover2001}, topic sensitive PageRank [59] (p430) \autocite{Haveliwala2003}, folksonomies \autocite{Hotho}, SimRank \autocite{Jeh}, neural-networks \autocite{Shu1999}, and semantic Web \autocite{Widyantoro2001,Du2007,Ding,Kamps,Taye2009}.


\subsection{Query Expansion and Relevance Feedback}
\label{s:qexpansion}

Relevance feedback is an idea of improving the search results by explicit or implicit methods. Explicit feedback asks users to rate results according to their relevance or collects that kind of information through analysis of mouse clicks, eye tracking etc. Implicit feedback occurs when external sources are consulted such as thesauri or by analysis the top results provided by the search engine. There are two ways of using this feedback. It can be displayed as a list of suggested search terms to the user and the user decided whether or not to take the advice, or the query is modified internally without the user's knowledge. This is then called automatic query expansion.


\subsubsection*{Challenges of Web Search}

Other issues that arise when trying to search the World Wide Web are as follows (\autocite[p.449]{Baeza-Yates2011}).

\begin{itemize}
  \item Data is distributed. Data is located on different computers all over the world and network traffic is not always reliable.
  \item Data is volatile. Data is deleted, changed or lost all the time so data is often out-of-date and links broken.
  \item The amount of data is massive and grows rapidly. Scaling of the search engine is an issue here.
  \item Data is often unstructured. There is no consistency of data structures.
  \item Data is of poor quality. There is no editor or censor on the Web. A lot of data is redundant too.
  \item Data is not heterogeneous. Different data types (text, images, sound, video) and different languages exist.
\end{itemize}

Since a single query for a popular word can results in  millions of retrieved documents from the index, search engine usually adopt a lazy strategy, meaning that they only actually retrieve the first few pages of results and only compute the rest when needed \autocite[p.459]{Baeza-Yates2011}. To handle the vast amounts of space needed to store the index, big search engines use a massive parallel and cluster-based architecture \autocite[p.459]{Baeza-Yates2011}. Google for example uses over 15,000 commodity-class PCs that are distributed over several data centres around the world \autocite{Dean2003}.


\subsubsection*{Summary}

\gls{ir} refers to the retrieval of information from a collection. In terms of the Internet it is often called Web search. A Web search engine is divided into different components, being the crawler to build an index of the collection and a ranking algorithm which stands between the index and the user.

Different retrieval models exist including the Boolean and the Vector model. Other methods exist to make search results more accurate, including relevance feedback and query expansion.

Search quality is generally measured using the metrics of precision and recall but for Web search precision is more important and usually a metric called `precision at n' is used for measurements.

Challenges are the size of the World Wide Web and ambiguous, unstructured nature of Web pages among others.

Ranking can be done at different stages of the search process. Depending on how the index is formatted and what information can be pre-computed at that stage, the ranking algorithm evaluates every page for relevance and returns them in order. There exist lots of different approaches on ranking, including PageRank and HITS (both analyse the link structure of the WWW), or more dynamic models like Fish search or genetic approaches.


\section{Natural Language Processing}

\todo{describe NLTK and the core functionality}
\gls{nltk} Python library\footnote{\url{http://www.nltk.org/}}.

\begin{description}
  \item [PlaintextCorpusReader] Reader for corpora that consist of plaintext documents. Paragraphs are assumed to be split using blank lines. Sentences and words can be tokenized using the default tokenizers, or by custom tokenizers specificed as parameters to the constructor.
  \item [Text] A wrapper around a sequence of simple (string) tokens, which is intended to support initial exploration of texts (via the interactive console). Its methods perform a variety of analyses on the text’s contexts (e.g., counting, concordancing, collocation discovery), and display the results.
  \item [index (word)] Find the index of the first occurrence of the word in the text.
  \item [count (word)] Count the number of times this word appears in the text.
\end{description}


\subsection{Damerau-Levensthein}

\begin{fcom}
  Damerau-Levensthein for clinamen!
  \url{https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance}
\end{fcom}

The Damerau–Levenshtein distance between two strings $a$ and $b$ is given by $d_{a,b}(|a|,|b|)$ where:

\begin{equation}
  d_{a,b}(i,j)=\left\{\begin{matrix*}[l]% chktex 21
  \max(i,j) & \textrm{if}\min(i,j)=0\\
  \min\left\{\begin{matrix*}[l]% chktex 21
  d_{a,b}(i-1,j)+1\\
  d_{a,b}(i,j-1)+1\\
  d_{a,b}(i-1,j-1)+1_{a_i\neq b_j}\\
  d_{a,b}(i-2,j-2)+1
  \end{matrix*}\right. & \textrm{if}\ i,j > 1 \ \textrm{and}\ a_i = b_{j-1}\ \textrm{and}\ a_{i-1} = b_j\\
  \min\left\{\begin{matrix*}[l]% chktex 21
  d_{a,b}(i-1,j)+1\\
  d_{a,b}(i,j-1)+1\\
  d_{a,b}(i-1,j-1)+1_{a_i\neq b_j}
\end{matrix*}\right. & \textrm{otherwise.}
  \end{matrix*}\right.
  \label{eq:DL}
\end{equation}
% \myequations{DL}

where $1_{(a_i \neq b_j)}$ is the indicator function equal to $0$ when $a_i = b_j$ and equal to $1$ otherwise.

Each recursive call matches one of the cases covered by the Dame\-rau-Leven\-shtein distance:

$d_{a,b}(i-1,j) + 1$ corresponds to a deletion (from a to b).\\
$d_{a,b}(i,j-1) + 1$ corresponds to an insertion (from a to b).\\
$d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}$  corresponds to a match or mismatch, depending on whether the respective symbols are the same.\\
$d_{a,b}(i-2,j-2) + 1$  corresponds to a transposition between two successive symbols.





\gls{nlp} blah blah blah\ldots

Bird, S., Klein, E. and Loper, E., 2009. \gls{nlp} with Python 1st ed., Sebasopol, CA:\@ O'Reilly Media.\autocite{Bird2009}

Manning, C., Raghavan, P. and Schuetze, H., 2008. Introduction to Information Retrieval 1st ed., Cambridge: Cambridge University Press.\autocite{Manning2009}

Taken from \autocite{Jurafsky2009}, also known as:
\begin{itemize}
  \item Speech and language processing
  \item Human language technology
  \item \gls{nlp}
  \item Computational linguistics
  \item Speech recognition and synthesis
\end{itemize}

Goals of \gls{nlp} are to get computers to perform useful tasks involving human language like:
\begin{itemize}
  \item Enabling human-machine communication
  \item Improving human-human communication
  \item Text and speech processing
\end{itemize}

e.g.\ machine translation, automatic speech recognition, natural language understanding, word sense disambiguation, spelling correction, grammar checking…


\hypertarget{stemming}{Techniques} that are useful for this are the following \autocite[Ch.2]{Manning2009}.

\begin{description}
  \item [Tokenisation] discarding white spaces and punctuation and making every term a token
  \item [Normalisation] making sets of words with same meanings, e.g.\ car and automobile
  \item [Case-folding] converting everything to lower case
  \item [Stemming] removing word endings, e.g.\ connection, connecting, connected $\to$ connect
  \item [Lemmatization] returning dictionary form of a word, e.g.\ went $\to$ go
\end{description}


\subsection*{Regular Expressions}

Used to specify text strings in text.

RE search requires a pattern that we want to search for and a corpus of texts to search through.

Errors can be false positives (FP) and false negatives (FN).

\begin{itemize}
  \item Increasing accuracy (minimizing FP)
  \item Increasing coverage (minimizing FN)
\end{itemize}

RE's can be expressed as Finite-State Automata (FSA).


\subsection*{Language Models (LM)}

Probabilities are based on counting things. Counting things in natural language is based on a corpus (pl corpora), a computer readable collection of text or speech.

Cats versus cat?

Same lemma but different wordforms.

\begin{itemize}
  \item A lemma is a set of lexical forms that have the same stem. (e.g.\ go)
  \item A wordform is the full inflected or derived form of the word. (e.g.\ goes)
  \item A word type is a distinct word in a corpus (repetitions are not counted but case sensitive).
  \item A word token is any word (repetitions are counted repeatedly)
\end{itemize}

The process of converting all words in a text to their lemma (e.g.\ goes $\to$ go) is called lemmatisation and the process of separating out all words in a text is called tokenisation or word segmentation.


\subsection*{$N$-Grams}

We can do word prediction with probabilistic models called $N$-Grams. They predict the probability of the next word from the previous $N-1$ words.

We want to compute the probability for $P(w|h)$ where $w$ is a word and $h$ is a history (the previous words). How many times occurred h followed by $w$ divided by how many times occurred $h$?

\begin{equation}
  P(w \mid h) = \frac{count(hw)}{count(h)}
  \label{eq:Probwh}
\end{equation}
% \myequations{Probwh}

Using the \textbf{chain rule of probability}:

\begin{equation}
  \begin{split}
  P(w_1^n) &= P(w_1)P(w_2 \mid w_1)P(w_3 \mid w_1^2 ) \ldots P(w_n \mid w_1^{n-1})\\
  &= \prod_{k=1}^{n}P(w_k \mid w_1^{k-1})
  \end{split}
  \label{eq:Probw1n}
\end{equation}
% \myequations{Probw1n}

Using the \textbf{Markov assumption} that probability of a word depends only on the previous word (or $n$ words).

\begin{equation}
  P(w_1^n) = \prod_{k=1}^{n}P(w_k \mid w_{k-1})
  \label{eq:Probw1n2}
\end{equation}
% \myequations{Probw1n2}

Using the \textbf{maximum likelihood estimation (MLE)} for $N$-Grams we can normalise counts to be between 0 and 1. $C$ stands for count.


\subsection*{Maximum likelihood estimation (MLE)}

\begin{equation}
  P(w_n \mid w_{n-N+1}^{n-1}) = \frac{C(w_{n-N+1}^{n-1} w_n)}{C(w_{n-N+1}^{n-1})}
  \label{eq:Probwnt}
\end{equation}
% \myequations{Probwn}

Usually instead of calculating the counts based on products we calculate them based on sums of logs.

So instead of  $p_1 \times p_2 \times p_3 \times p_4 = \log p_1 + \log p_2 + \log p_3 + \log p_4$

Google offers its $N$-Gram data for free on:

\begin{itemize}
  \item \url{http://bit.ly/1baDXAW} % http://googleresearch.blogspot.co.uk/2006/08/all-our-n-gram-are-belong-to-you.html
  \item \url{http://books.google.com/ngrams/}
  \item	\url{http://www.speech.sri.com/projects/srilm/}
  \item	\url{http://bit.ly/1G3ZJmX} % http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13
\end{itemize}


\subsection*{Evaluating N-Grams}

Extrinsic and intrinsic evaluation.

\begin{description}
  \item [Extrinsic]: evaluate performance of a language model by embedding it into an independent application.
  \item [Intrinsic]: evaluate independent on any application, e.g.\ perplexity.
\end{description}


\subsection*{Perplexity}

\begin{equation}
  PP(W) = \sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_i \mid w_{i-1})}}
  \label{eq:ppw}
\end{equation}
% \myequations{ppw}


\subsection*{Smoothing}


\subsection*{Add-One: Laplace smoothing for bigrams}

\begin{equation}
  P_{Add-1}(w_i \mid w_{i-1}) = \frac{c(w_{i-1}, w_i) + 1}{c(w_{i-1}) + V}
  \label{eq:padd1}
\end{equation}
% \myequations{padd1}


\subsection*{Adjusted count}

\begin{equation}
  c_i^* = (c_i+1)\frac{N}{N+V}
  \label{eq:ci}
\end{equation}
% \myequations{ci}

Add-1 smoothing is ok for text categorisation but not so much for language modelling.

Most commonly used is Kneser-Ney extended interpolated.

For very large N-grams like the Web “Stupid Backoff” is used.


\subsection*{Good Turing Discounting}

$N_c$ is the frequency of frequency $c$.

\begin{equation}
  c^* = (c+1)\frac{N_{c+1}}{N_c}
  \label{eq:cstar}
\end{equation}
% \myequations{cstar}


\subsection*{Naive Bayes}

[3] page 234…

\begin{quote}
  (Wikipedia): A naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions. A more descriptive term for the underlying probability model would be ``independent feature model''.
\end{quote}


\subsection*{Maximum Entropy Models (MaxEnt)}

Page 227 \ldots in [1]

MaxEnt models are also widely known as \textbf{multinomial logistic regression}. They are used for sequence classification, e.g.\ part-of-speech tagging. They belong to a family of classifiers known as \textbf{exponential or log-linear classifiers}.

The task of classification is to take a single observation, extract some useful features describing the observation, and then, based on these features, to classify the observation into one of a set of discrete classes. A probabilistic classifier also gives the probability of the observation being in that class; it gives a probability distribution over all classes.

MaxEnt works by extracting some set of features from the input, combining them linearly (meaning that each feature is multiplied by a weight and then added up), and then using this sum as an exponent.
Formula below shows how to calculate the probability of class $c$ given an observed datum (a given data point) $d$ and $\lambda$ is a weight that is assigned to feature $f$. Taking the exponent makes the result always positive. Dividing by the Sum of that for all classes makes it a probability.

\begin{equation}
  P(c \mid d, \lambda) = \frac{\exp\sum_i\lambda_i f_i (c,d)}
  {\sum_{c\prime}\exp\sum_{i}\lambda_i f_i (c\prime,d)}
  \label{eq:pcdlambda}
\end{equation}
% \myequations{pcdlambda}

To get the single best class with the highest probability we need to compute the following.

\begin{equation}
  \hat{c} = \underset{c\in C}{\text{argmax}} \ P(c \mid d,\lambda)
  \label{eq:hatc}
\end{equation}
% \myequations{hatc}

\begin{table}[htb]
  \centering
  \begin{tabu}{lll}
  \toprule
  PERSON    & LOCATION   & DRUG      \\ \midrule
  In Québec & In Québec  & In Québec \\
  0         & 1.8 + -0.6 & 0.3       \\
  \bottomrule
  \end{tabu}
\caption[MaxEnt Example table]{MaxEnt Example table}
\label{tab:maxent}
\end{table}

Features:

$f1(c,d) \equiv [ \ c = \text{LOCATION} \ \wedge \ w-1 = \text{``in''} \wedge \ \text{isCapitalized}(w)]$\\
$f2(c,d) \equiv [ \ c = \text{LOCATION} \ \wedge \ \text{hasAccentedLatinChar}(w)]$\\
$f3(c,d) \equiv [ \ c = \text{DRUG} \ \wedge \ \text{ends}(w,\text{``c''})]$

$P(\text{LOCATION} \mid \text{in Québec}) = \frac{e^{1.8} e^{–0.6}}{e^{1.8} e^{–0.6} + e^{0.3} + e^0} = 0.586$\\
$P(\text{DRUG} \mid \text{in Québec}) = \frac{e^{0.3}}{e^{1.8} e^{–0.6} + e^{0.3} + e^0} = 0.238$\\
$P(\text{PERSON} \mid \text{in Québec}) = \frac{e^0}{e^{1.8} e^{–0.6} + e^{0.3} + e^0} = 0.176$

The empirical expectation is the sum of all occurrences where a feature is true for one of our observed datums.

\begin{equation}
  empirical \ E(f_i)= \sum_{(c,d) \ \in \ observed(C,D)}f_i(c,d)
  \label{eq:epirical}
\end{equation}
% \myequations{epirical}


\subsection*{Evaluation}

\begin{equation}
  Precision = \frac{\text{number of correctly labeled}}{\text{total number of extracted}}
  \label{eq:preci}
\end{equation}
% \myequations{preci}

\begin{equation}
  Recall = \frac{\text{number of correctly labeled}}{\text{total number of gold}}
  \label{eq:reca}
\end{equation}
% \myequations{reca}

\begin{equation}
  F_1 = \frac{2PR}{P+R}
  \label{eq:f1mes}
\end{equation}
% \myequations{f1mes}


\subsection*{Information Extraction}

[1] Chapter 22, p 759…

``The process of information extraction (IE), also called text analytics, turns the unstructured information embedded in texts into structured data.''

IE involves named entity recognition (NER), relation detection and classification, event detection and classification and temporal analysis.


\subsection*{Named Entity Recognition}

A named entity can be anything that can be referred to by a proper name, such as person-, place- or organisation names and times and amounts.

Example (first sentence in Faustroll):

\begin{quote}
  In this year Eighteen Hundred and Ninety-eight, the Eighth day of February, Pursuant to article 819 of the Code of Civil Procedure and at the request of M. and Mme. Bonhomme (Jacques), proprietors of a house situate at Paris, 100 bis, rue Richer, the aforementioned having address for service at my residence and further at the Town Hall of Q borough.
\end{quote}

\begin{quote}
  In this [year Eighteen Hundred and Ninety-eight, the Eighth day of February]$^{\text{TIME}}$, Pursuant to article [819]$^{\text{NUMBER}}$ of the [Code of Civil Procedure]$^{\text{DOCUMENT}}$ and at the request of [M. and Mme. Bonhomme (Jacques)]$^{\text{PERSON}}$, proprietors of a house situate at [Paris, 100 bis, rue Richer]$^{\text{LOCATION}}$, the aforementioned having address for service at my residence and further at the [Town Hall]$^{\text{FACILITY}}$ of [Q borough]$^{\text{LOCATION}}$.
\end{quote}

Gazetteers (lists of place or person names for example) can help with the detection of these named entities.


\subsection*{Part of Speech Tagging}

Parts of speech (POS) are lexical tags for describing the different elements of a sentence. The eight main parts-of-speech (originating from ca. 100 B.C.) are noun, verb, pronoun, preposition, adverb, conjunction, participle and article.
Wikipedia:

\begin{description}
  \item [Noun]: any abstract or concrete entity; a person (police officer, Michael), place (coastline, London), thing (necktie, television), idea (happiness), or quality (bravery)
  \item [Pronoun]: any substitute for a noun or noun phrase
  \item [Adjective]: any qualifier of a noun
  \item [Verb]: any action (walk), occurrence (happen), or state of being (be)
  \item [Adverb]: any qualifier of an adjective, verb, or other adverb
  \item [Preposition]: any establisher of relation and syntactic context
  \item [Conjunction]: any syntactic connector
  \item [Interjection]: any emotional greeting (or `exclamation')
\end{description}

Building a Large Annotated Corpus of English \autocite{Marcus1993}

There exist other sets of tags, like the Penn Treebank with divides those 8 tags into a total of 45, for example $CC$ for coordinating conjunction, $CD$ for cardinal number, $NN$ for noun singular, $NNS$ for noun plural, $NNP$ for proper noun singular, $VB$ for verb base form, $VBG$ for verb gerund, etc.

The process of adding tags to the words of a text is called parts-of-speech tagging or just tagging. This usually is done together with the tokenisation of the text.

Example (first sentence in Faustroll):

\begin{quote}
  In\slash{}IN this\slash{}DT [year\slash{}NN Eighteen\slash{}CD Hundred\slash{}CD and\slash{}CC Ninety-eight\slash{}CD,\slash{}, the\slash{}DT Eighth\slash{}CD day\slash{}NN of\slash{}IN February\slash{}NNP]$^{\text{TIME}}$,\slash{}, Pursuant\slash{}JJ to\slash{}IN article\slash{}NN [819\slash{}CD]$^{\text{NUMBER}}$ of\slash{}IN the\slash{}DT [Code\slash{}NN of\slash{}IN Civil\slash{}NNP Procedure\slash{}NNP]$^{\text{DOCUMENT}}$ and\slash{}CC at\slash{}IN the\slash{}DT request\slash{}NN of\slash{}IN [M.\slash{}NN and\slash{}CC Mme.\slash{}NN Bonhomme\slash{}NNP (\slash{}(Jacques\slash{}NNP)\slash{})]$^{\text{PERSON}}$,\slash{}, proprietors\slash{}NNS of\slash{}IN a\slash{}DT house\slash{}NN situate\slash{}JJ at\slash{}IN [Paris\slash{}NNP,\slash{}, 100\slash{}CD bis\slash{}NN,\slash{}, rue\slash{}NN Richer\slash{}NNP]$^{\text{LOCATION}}$,\slash{}, the\slash{}DT aforementioned\slash{}JJ having\slash{}VBG address\slash{}NN for\slash{}IN service\slash{}NN at\slash{}IN my\slash{}PRP residence\slash{}NN and\slash{}CC further\slash{}JJ at\slash{}IN the\slash{}DT [Town\slash{}NNP Hall\slash{}NNP]$^{\text{FACILITY}}$ of\slash{}IN [Q\slash{}NNP borough\slash{}NN]$^{\text{LOCATION}}$.\slash{}.
\end{quote}

\begin{equation}
  t_1^n = \underset{t_1^n}{\text{argmax}} \ P(w_1^n \mid t_1^n) P(t_1^n)
  \label{eq:tn1}
\end{equation}
% \myequations{tn1}

\begin{equation}
  P(t_i \mid t_{i-1}) = \frac{C(t_{i-1},t_i)}{C(t_{i-1})}
  \label{eq:pti}
\end{equation}
% \myequations{pti}

For example: the probability of getting a common noun after a determiner is:

\begin{equation}
  P(\text{NN} \mid \text{DT}) = \frac{C(\text{DT},\text{NN})}{C(\text{DT})} = \frac{56,509}{116,454} = 0.49
  \label{eq:pnndtt}
\end{equation}
% \myequations{pnndt}

Given that there are $116,454$ occurrences of DT in the corpus and of these $56,509$ occurrences where a NN follows after the DT.% chktex 13

\begin{equation}
  P(\text{is} \mid \text{VBZ}) = \frac{C(\text{VBZ},\text{is})}{C(\text{VBZ})} = \frac{10,073}{21,627} = 0.47
  \label{eq:pisvbz}
\end{equation}
% \myequations{pisvbz}

Or the probability of a third person singular verb being `is' is 0.47.


\subsection*{Parsing}

Parsing is the process of analysing a sentence and assigning a structure to it. Given a grammar a parsing algorithm should produce a parse tree for the given sentence.


\subsection*{Grammar}

A language is modelled using a grammar, specifically a Context-Free-Grammar or CFG.\@ Such a grammar normally consists or rules and a lexicon. For example a rule could be NP $\to$ Det Noun, where NP stands for noun phrase, Det for determiner and Noun for a noun. The corresponding lexicon would then include facts like Det $\to$ a, Det $\to$ the, Noun $\to$ book. This grammar would let us form the noun phrases `the book' and `a book' only. The two parse trees would then look like this:

\begin{figure}[!htbp]
  \centering
  \begin{minipage}{.4\linewidth}
  \Tree[.NP [.Det \emph{a} ]
  [.Noun \emph{book} ]]
  \end{minipage}
  \hspace{.05\linewidth}
  \begin{minipage}{.4\linewidth}
  \Tree[.NP [.Det \emph{the} ]
  [.Noun \emph{book} ]]
  \end{minipage}
\caption[Grammars]{Grammars}
\label{Grammars}
\end{figure}

The parse tree for the previous example sentence from Faustroll is shown below, in horizontal for convenience.

\begin{alltt}
(ROOT
  (S
    (PP (IN In)
      (NP (DT this) (NN year) (NNPS Eighteen) (NNP Hundred)
        (CC and)
        (NNP Ninety-eight)))
    (, ,)% chktex 26
    (NP
      (NP (DT the) (JJ Eighth) (NN day))
      (PP (IN of)
        (NP (NNP February) (, ,) (NNP Pursuant)))% chktex 26
      (PP
        (PP (TO to)
          (NP
            (NP (NN article) (CD 819))
            (PP (IN of)
              (NP
                (NP (DT the) (NNP Code))
                (PP (IN of)
                  (NP (NNP Civil) (NNP Procedure)))))))
        (CC and)
        (PP (IN at)
          (NP
            (NP (DT the) (NN request))
            (PP (IN of)
              (NP (NNP M.)
                (CC and)
                (NNP Mme) (NNP Bonhomme))))))
      (PRN (-LRB- -LRB-)
        (NP (NNP Jacques))
        (-RRB- -RRB-))
      (, ,)% chktex 26
      (NP
        (NP (NNS proprietors))
        (PP (IN of)
          (NP
            (NP (DT a) (NN house) (NN situate))
            (PP (IN at)
              (NP (NNP Paris))))))
      (, ,)% chktex 26
      (NP (CD 100) (NN bis))
      (, ,))% chktex 26
    (VP (VBP rue)
      (NP
        (NP (NNP Richer))
        (, ,)% chktex 26
        (NP (DT the) (JJ aforementioned)
          (UCP
            (S
              (VP (VBG having)
                (NP
                  (NP (NN address))
                  (PP (IN for)
                    (NP (NN service))))
                (PP (IN at)
                  (NP (PRP$ my) (NN residence)))))
            (CC and)
            (PP
              (ADVP (RBR further))
              (IN at)
              (NP
                (NP (DT the) (NNP Town) (NNP Hall))
                (PP (IN of)
                  (NP (NNP Q))))))
          (NN borough))))
    (. .)))% chktex 26
\end{alltt}

This particular tree was generated using the Stanford Parser at \url{http://nlp.stanford.edu:8080/parser/index.jsp}.
Given the rather complicated nature of the words and sentence structure, some of the labels might be wrong.


\section{Linguistics / WordNet}

Here’s my \gls{hyper} term. \gls{holo} \gls{hyper}

I looked into linguistics for the purpose of patadata. This section definitely needs some expanding. Some concepts that might be relevant include (taken from Wikipedia):

\begin{description}
  \item [Hyponym] – subcategory of something
  \item [Hypernym] – top category of some things
  \item [Meronym] – member of something (e.g. finger is meronym to hand, wheel to car)
  \item [Holonym] – e.g. tree is holonym of bark, trunk, limb… opposite of meronym
  \item [Troponym] – presence of “manner” between things (e.g. to traipse and to mince = walk a certain way)
  \item [Homonym] – same spelling but different sound and meaning = heteronym – same sound but different spelling = heterography – same meaning = synonym
  \item [Antonym] – opposite
  \item [Metonym] – figure of speech (e.g. Hollywood for American movies) not quite metaphor but similar.
\end{description}

I need to find REFERENCES for this section.

% For a project of this kind of speciality, it is hard to find directly related work. There is no real  precedent to the idea of a pataphysical search tool.
% Looking at more broadly related work shows a few worthwhile mentions as follows. They are mostly examples of creative computing in various forms, some of which have pataphysical foundations, whether they are aware of it or not, and others don't. The list could be extended endlessly, so I will only include a selection of the most interesting ones. I have also included some examples of other kinds of projects that have pataphysical elements in them but are not related to computers or searching. Finally, I have added a few examples of current Web search engines and ranking algorithms, though they probably cannot be considered creative or pataphysical.






\section{Algorithm Formalisation}

Algorithm Classification

\begin{multicols}{2}
\begin{itemize}
  {\setlength\itemindent{-26pt} \item[] By implementation:}
  \item Recursive/iterative
  \item Logical
  \item Serial/parallel/distributed
  \item Deterministic/non-deterministic
  \item Exact/approximate
  \item Quantum
\end{itemize}

\begin{itemize}
  {\setlength\itemindent{-26pt} \item[] By design paradigm:}
  \item Brute-force/exhaustive search
  \item Divide and conquer
  \item Dynamic
  \item Greedy
  \item Linear
  \item Reduction
  \item Search and enumeration
\end{itemize}

\begin{itemize}
  {\setlength\itemindent{-26pt} \item[] By field of study:}
  \item Search
  \item Sorting
  \item Merge
  \item Numerical
  \item Graph
  \item String
  \item Computational geometrics
  \item Combinatorial
  \item Medical
  \item Machine learning
  \item Cryptography
  \item Data compression
  \item Parsing
\end{itemize}

\begin{itemize}
  {\setlength\itemindent{-26pt} \item[] By complexity:}
  \item Big-O-Notation
\end{itemize}
\end{multicols}

\begin{description}
  \item [High-Level Description] in prose, ignoring implementation details.
  \item [Implementation Description] in prose, describing implementation in detail.
  \item [Formal description] lowest level, most detailed.
\end{description}

\itab{$D = \{d_1, \ldots, d_n\}$} \tab{is the set of documents}\\
\itab{$Q = \{q_1, \ldots, q_n\}$} \tab{is the set of queries}\\
\itab{$q = \{t_1, \ldots, t_n\}$} \tab{is the set of query terms}\\
\itab{$V = \{v_1, \ldots, v_t\}$} \tab{is the set of all distinct index terms in a document collection (the Vocabulary)}\\
\itab{$R (q_i,d_j)$} \tab{is the ranking function, where $q_i \in{} Q$ and $d_j  \in{} D$}\\
\itab{$N$} \tab{is the total number of documents}\\
\itab{$w_{t,q}$} \tab{is the weight of the term in the query}\\
\itab{$tf_{t,d}$} \tab{is the term frequency of $t$ in $d$}\\
\itab{$wf_{t,d}$} \tab{is the tf-idf weight of $t$ in $d$}\\
\itab{$P_t$} \tab{is the postings list of all ($d$, $tf_{t,d}$) for a given $t$}
% chktex 16

\stopcontents[chapters]
