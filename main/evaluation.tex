% !TEX root = ../main.tex

\chapter{Evaluation}
\label{ch:evaluation}

\startcontents[chapters]

\vfill

\begin{alltt}\sffamily
Score,
quel grade avais,
of my cooler judgment,
and inquires after the evacuations of the toad on the horizon.

His judgment takes the winding way Of question distant,
if not always with judgment,
and showed him every mark of honour,
three score years before.

Designates him as above the grade of the common sailor,
but I was of a superior grade,
travellers of those dreary regions marking the site of degraded Babylon.

Mark the Quilt on which you lie,
und da Sie grade kein weißes Papier bei sich hatten,
and to draw a judgement from Heaven upon you for the Injustice.
\end{alltt}

\newpage
\minicontents
\spirals

\section{Evaluating Search}
\label{s:evalsearch}

Generally, computer systems are evaluated against functional requirements and performance specifications. Traditional \ac{IR} is evaluated using two metrics known as precision and recall. Precision is defined as the fraction of retrieved documents that are relevant, while recall is defined as the fraction of relevant documents that are retrieved.

\begin{equation}
  Precision = \frac{relevant \ documents \ retrieved}{retrieved \ documents}
  \label{eq:precision}
\end{equation}
% \myequations{precision}

\begin{equation}
  Recall = \frac{relevant \ documents \ retrieved}{relevant \ documents}
\label{eq:recall}
\end{equation}
% \myequations{recall}

Note the slight difference between the two. Precision tells us how many of all retrieved results were actually relevant (of course this should preferable be very high) and recall simply indicates how many of all possible relevant documents we managed to retrieve. This can be easily visualised\marginnote{\faicon{object-group}~\ref{fig:PR}} as follows.

% \begin{figure}[!htbp]
%   \centering
%   \input{images/precrec.pdf_tex}
%   \caption[Precision and Recall]{Precision and Recall}
% \label{fig:PR2}
% \end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{PRside}
  \caption[Precision and Recall]{Precision and Recall\footnotemark}
\label{fig:PR}
\end{figure}
\footnotetext{Image taken from Wikimedia Commons: {\url{https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg}}}
\todo{place footnotetext properly}

Precision is typically more important than recall in web search while it is the other way around in a database search system maybe. The mean average precision value (MAP) can be calculated following this formula \autocite[p.141]{Baeza-Yates2011}:

\begin{equation}
  MAP_i = \frac{1}{|R_i|} \sum_{k=1}^{|R_i|} P(R_i[k])
\label{eq:MAP}
\end{equation}
% \myequations{MAP}

Where $R_i$ is the set of relevant documents for query $q_i$.

But for many web searches is it not necessary to calculate the average of all results, since users don't inspect results after the first page very often and it is therefore desirable to have the highest level of precision in the first 5 to 30 results maybe. For this purpose it is common to measure the average precision of web search engines after only a few documents have been seen. This is called `Precision at n' or `P@n' \autocite[p.140]{Baeza-Yates2011}. So for example this could be P@5 or P@10 or P@20. For example, to compare two ranking algorithms, we would calculate P@10 for each of them over an average of 100 queries maybe and compare the results and therefore the performance of the algorithm.

The Text REtrieval Conference (TREC) is a conference that provides large test sets of data to participants and lets them compare results. They have specific test sets for web search comprised of crawls of $.gov$ web pages for example, but unfortunately they have to be paid for to get a copy.\footnote{\url{http://ir.dcs.gla.ac.uk/test_collections/}}% chktex 26

There are certain other factors that can be or need to be evaluated when looking at a complete search system, as shown below.

\begin{itemize}
  \item Speed of crawling.
  \item Speed of indexing data.
  \item Amount of storage needed for data.
  \item Speed of query response.
  \item Amount of queries per given time period.
\end{itemize}

Ranking is another issue that could be considered to pre-evaluate web pages at indexing time rather than query time. This is further discussed in chapter~\ref{ch:technology}\marginnote{§~\ref{ch:technology}}.


% \section{Creative Search}

% In this paper \autocite{Sawle2011} we have discussed an initial approach to measure the creativity of search results. Based on a definition of creativity by Boden, we attempt to define creativity in a way which could be applied to search results and provide a simple metric to measure it.

\todo{anything else about evaluating search/creative search?}


\section{Evaluating Creative Computers}
\label{s:creattributes}

\todo{bridge over from traditional search evaluation to general creative computing}

\begin{leftbar}
  Evaluating human creativity objectively seems problematic; evaluating computer creativity at all seems even harder. There are many debates across the disciplines involved. Taking theories on human creativity and directly applying them to machines seems logical but may be the wrong (anthropomorphic) approach. Adapting Mayer's five big questions \cite{Mayer1999} to machines does not seem to capture the real issues at play. Instead of asking if creativity is a property of people, products, or processes we might ask if it is a property of any or all of the following:

  \begin{itemize}
    \item programmers (and collaborators)
    \item users (audiences and participants)
    \item machines (this is problematic until the posited AI singularity \cite{Schmidhuber2006a})
    \item products (i.e. does a program output material that can be judged to be creative)
    \item processes (e.g. a Processing sketch, or in a self-modifying/learning program)
  \end{itemize}

  For instance, is the programmer the only creative agent, or are users (i.e. audiences or participants in interactive work) able to modify the system with their own creative input? Similarly for any instance of machine creativity, we might ask if it is:

  \begin{itemize}
    \item local (e.g. limited to a single machine or program?)
    \item networked (i.e. interacts with other predefined machines)
    \item web-based (e.g. is distributed and/or open to interactions, perhaps via an API)
  \end{itemize}

  For example, discussions from computational creativity often focus on very basic questions such as `whether an idea or artefact is valuable or not, and whether a system is acting creatively or not' \cite{Pease2011}.
\end{leftbar}

\todo{write better lit review for this section}
\todo{add francois stuff}
\todo{check ICCC conference 2014 and 2015}

Because creativity infused computing has only emerged in the last few decades or so, its evaluation is not well defined. Discussions from \ac{CompC} for example often focus on very basic questions such as `whether an idea or artefact is valuable or not, and whether a system is acting creatively or not' \autocite{Pease2011}.

Pease, Winterstein and Colton have argued that creativity may be seen as `output minus input.' \autocite[p.2]{Pease2001}. The output in this case is the creative product but the input is not the process. Rather, it is the `inspiring set' (comprised of explicit knowledge such as a database of information and implicit knowledge input by a programmer) of a piece of software.

\begin{quotation}
  The degree of creativity in a program is partly determined by the number of novel items of value it produces. Therefore we are interested in the set of valuable items produced by the program which exclude those in the inspiring set. \sourceatright{\autocite[p.3]{Colton2001}}
\end{quotation}

They also suggest that all creative products must be `novel and valuable' \citeyear[p.1]{Pease2001} and provide several measures that take into consideration the context, complexity, archetype, surprise, perceived novelty, emotional response and aim of a product. In terms of the creative process itself they only discuss `randomness' as a measurable approach. Elsewhere, Pease et al discuss using `serendipity' as an approach \citeyear{Pease2013}.

Graeme Ritchie supports the view that creativity in a computer system must be measured `relative to its initial state of knowledge' \autocite[p.72]{Ritchie2007}. He identifies three main criteria for creativity as `novelty, quality and typicality' \citeyear[p.72-73]{Ritchie2007}, although he argues that `novelty and typicality may well be related, since high novelty may raise questions about, or suggest a low value for, typicality' \citeyear[p.73]{Ritchie2007} \citeyear[see also][]{Ritchie2001}. He proposes several evaluation criteria which fall under the following categories: \autocite[p.91-92]{Ritchie2007} basic success, unrestrained quality, conventional skill, unconventional skill, avoiding replication and various combinations of those. Dan Ventura later suggested the addition of `variety and efficiency' to Ritchie's model \citeyear[p.7]{Ventura2008}.

It should be noted that `output minus input' might easily be misinterpreted as `product minus process', however, that is not the case. In fact, Pease, Winterstein and Colton argue that `the process by which an item has been generated and evaluated is intuitively relevant to attributions of creativity' \citeyear[p.6]{Pease2001}, and that `two kinds of evaluation are relevant; the evaluation of the item, and evaluation of the processes used to generate it.' \citeyear[p.7]{Pease2001}. If a machine simply copies an idea from its inspiring set then it just cannot be considered creative and needs to be disqualified so to speak.

Simon Colton came up with an evaluation framework called the `creative tripod'. The tripod consists of three behaviours a system or artefact should exhibit in order to be called creative. The three legs represent `skill, appreciation, and imagination' and three different entities can sit on it, namely the programmer, the computer and the consumer. Colton argues that the perception `that the software has been skillful, appreciative and imaginative, then, regardless of the behaviour of the consumer or programmer, the software should be considered creative.' \citeyear[p.5]{Colton2008a} + \citeyear[p.5]{Colton2008}. As such a product can be considered creative, if it appears to be creative. If not all three behaviours are exhibited, however, it should not be considered creative. \autocite[p.5]{Colton2008a} + \autocite[p.5]{Colton2008}

\begin{quotation}
  Imagine an artist missing one of skill, appreciation or imagination. Without skill, they would never produce anything. Without appreciation, they would produce things which looked awful. Without imagination, everything they produced would look the same. \sourceatright{\autocite{Colton2008a}}
\end{quotation}

Davide Piffer suggests that there are three dimensions of human creativity that can be measured, namely `novelty, usefulness/appropriateness and impact/influence' \citeyear[p.258-259]{Piffer2012}. As an example of how this applies to measuring a person's creativity he proposes `citation counts' \autocite[p.261]{Piffer2012}. While this idea works well for measuring scientific creativity maybe, he does not explain how this would apply to a visual artist for example\footnote{\url{http://www.artfacts.net} seems to provide just that though.}.

\spirals

Anna Jordanous proposed 14 key components of creativity (which she calls an  `ontology of creativity') \citeyear[p.104-120]{Jordanous2012}, from a linguistic analysis of creativity literature which identified words that appeared significantly more often in discussions of creativity compared to unrelated topics. \citeyear[p.120]{Jordanous2012}.

\begin{quotation}
  The themes identified in this linguistic analysis have collectively provided a clearer ``working'' understanding of creativity, in the form of components that collectively contribute to our understanding of what creativity is. Together these components act as building blocks for creativity, each contributing to the overall presence of creativity; individually they make creativity more tractable and easier to understand by breaking down this seemingly impenetrable concept into constituent parts. \sourceatright{\autocite[p.120]{Jordanous2012}}
\end{quotation}

The 14 components Jordanous collated are: \citeyear[p.118-120]{Jordanous2012}
\begin{quotation}
  \begin{enumerate}
    \item Active Involvement and Persistence
    \item Generation of Results
    \item Dealing with Uncertainty
    \item Domain Competence
    \item General Intellect
    \item Independence and Freedom
    \item Intention and Emotional Involvement
    \item Originality
    \item Progression and Development
    \item Social Interaction and Communication
    \item Spontaneity / Subconscious Processing
    \item Thinking and Evaluation
    \item Value
    \item Variety, Divergence and Experimentation
  \end{enumerate}
\end{quotation}

Anna Jordanous found that `evaluation of computational creativity is not being performed in a systematic or standard way' \autocite[p.2]{Jordanous2011} and proposed `\ac{SPECS}' \autocite[p.137-140]{Jordanous2012a}:
\label{s:specs}

\begin{quotation}
  \begin{enumerate}
    \item Identify a definition of creativity that your system should satisfy to be considered creative:
      \begin{enumerate}
        \item What does it mean to be creative in a general context, independent of any domain specifics?
        \begin{itemize}
          \item Research and identify a definition of creativity that you feel offers the most suitable definition of creativity.
          \item The 14 components of creativity identified in Chapter 4 are strongly suggested as a collective definition of creativity.
        \end{itemize}
        \item What aspects of creativity are particularly important in the domain your system works in (and what aspects of creativity are less important in that domain)?
        \begin{itemize}
          \item Adapt the general definition of creativity from Step 1a so that it accurately reflects how creativity is manifested in the domain your system works in.
        \end{itemize}
      \end{enumerate}
    \item Using Step 1, clearly state what standards you use to evaluate the creativity of your system.
      \begin{itemize}
        \item Identify the criteria for creativity included in the definition from Step 1 (a and b) and extract them from the definition, expressing each criterion as a separate standard to be tested.
        \item If using Chapter 4's components of creativity, as is strongly recommended, then each component becomes one standard to be tested on the system.
      \end{itemize}
    \item Test your creative system against the standards stated in Step 2 and report the results.
      \begin{itemize}
        \item For each standard stated in Step 2, devise test(s) to evaluate the system's performance against that standard.
        \item The choice of tests to be used is left up to the choice of the individual researcher or research team.
        \item Consider the test results in terms of how important the associated aspect of creativity is in that domain, with more important aspects of creativity being given greater consideration than less important aspects. It is not necessary, however, to combine all the test results into one aggregate score of creativity.
      \end{itemize}
  \end{enumerate}
\end{quotation}

The SPECS model essentially means that we cannot evaluate a creative computer system objectively, unless steps 1 and 2 are predefined and publically available for external assessors to execute step 3. Creative evaluation can therefore be seen as a move from subjectivity to objectivity, i.e. defining subjective criteria for objectively evaluating a product in terms of the initial criteria.

\begin{quotation}
  For transparent and repeatable evaluative practice, it is necessary to state clearly what standards are used for evaluation, both for appropriate evaluation of a single system and for comparison of multiple systems using common criteria. \cite{Jordanous2012a}
\end{quotation}

This is further strengthened by Richard Mayer stating that we need a `clearer definition of creativity' \cite{Mayer1999} and Linda Candy arguing for `criteria and measures [for evaluation] that are situated and domain specific.' \cite{Candy2012}



\todo{compare to CC research methodolody}
\begin{draft}
  Hugill and Yang suggest that existing research methodologies are unsuitable for transdisciplinary subjects such as Creative Computing (CC). The following is an example of a possible CC research methodology they propose as a starting point (Hugill and Yang 2013, p.17):
  1. Review literature across disciplines
  2. Identify key creative activities
  3. Analyse the processes of creation
  4. Propose approaches to support these activities and processes 5. Design and implement software following this approach
  6. Experiment with the resulting system and propose framework
  They go on to propose four standards for CC (Hugill and Yang 2013, p.17) namely, resist standardisation, perpetual novelty, continuous user interaction and combinational, exploratory and or transformational.
\end{draft}


\spirals

Linda Candy draws inspiration for the evaluation of (interactive) creative computer systems from \ac{HCI}. The focus of evaluation in \ac{HCI} has been on usabilty, she says \autocite[p.23]{Candy2012}, which may not be as useful in creativity research. She argues that in order to successfully evaluate an artefact, the practitioner needs to have `the necessary information including constraints on the options under consideration.' \autocite[p.7]{Candy2012}

Evaluation happens at every stage of the process (i.e. from design $\to$ implementation $\to$ operation). Some of the key aspects of evaluation Candy highlights are:

\begin{itemize}
  \item aesthetic appreciation
  \item audience engagement
  \item informed considerations
  \item reflective practice
\end{itemize}

Candy introduces the \ac{MMCE}\marginnote{\faicon{object-group}~\ref{fig:candy02}} with four main elements of people, process, product and context \autocite[p.11]{Candy2012} similar to some of the models of creativity\marginnote{§~\ref{ch:creativity}} we have seen in chapter~\ref{ch:creativity}.

\begin{figure}[!htbp] % (here, top, bottom, page)
  \centering
  \includegraphics[width=\linewidth]{candy02}
\caption[Multi-dimensional Model of Creativity and Evaluation]{Linda Candy's Multi-dimensional Model of Creativity and Evaluation}
\label{fig:candy02}
\end{figure}

Candy proposes the the following values or criterias for measurement \autocite{Candy2012}.

\begin{description}
  \item [People] capabilities, characteristics, track record, reputation, impact, influence (profile, demographic, motivation, skills, experience, curiosity, commitment)
  \item [Process] problem finding, solution oriented, exploratory, systematic, practice-based, empirical, reflective, opportunistic, rules, standards (opportunistic, adventurous, curious, cautions, expert, knowledgable, experienced)
  \item [Product] novel, original, appropriate, useful, surprising, flexible, fluent, engaging (immediate, engaging, enhancing, purposeful, exciting, disturbing)
  \item [Context] studio, living laboratory, public space, museum, constraints, opportunities, acceptability, leading edge (design quality, usable, convincing, adaptable, effective, innovative, transcendent)
\end{description}

Furthermore it is interesting to know the judging criteria for the Prix Ars Electronica, an international competition for Cyber Arts to be aesthetics, originality, excellence of execution, compelling conception and innovation in technique of the presentation \autocite[cited in][p.18]{Candy2012}.
\todo{do i see my product as cyber art?}

\spirals

\todo{rewrite}

Geraint Wiggins introduced a formal notation and set of rules for the description, analysis and comparison of creative systems \citeyear{Wiggins2006} which is largely based on Boden's theory of creativity \citeyear{Boden2003}. The framework uses three criteria for measuring creativity: `relevance, acceptability and quality'.

Geraint Wiggins previously described a formal notation and set of rules for the description, analysis and comparison of creative systems in the form of his \ac{CSF} \autocite{Wiggins2006} which was largely based on Margaret Boden's theory of creativity \autocite{Boden2003}. Graeme Ritchie then contributed to this framework \autocite{Ritchie2012} and we will base our work on his revised version of this framework.\todo{It's not an aesthetic framework, but rather a functionalist framework} The \ac{CSF} provides a formal description for Boden's concepts of exploratory and transformational creativity. Wiggins's R–transformation and T–transformation is akin to Boden's H-creativity and P-creativity respectively. To enable the transition from exploratory to transformational creativity in his framework, Wiggins introduced meta-rules which allow us to redefine our conceptual space in a new way.

It is important to note here that the exploratory search in an information retrieval sense should not be mistaken with what is discussed here. Exploratory search (for a creative solution to a problem) in the Wiggins/Ritchie/Boden sense happens one step before transformational search. This means that we want to end up with transformational tools from this framework (rather than exploratory ones) to use in our exploratory Web search system.

Ritchie described the \ac{CSF} as a set of initial concepts, which create `further concepts one after another, thus ``exploring the space''' but also argued that a search system would practically only go through a limited number of steps and therefore proposed some changes and additions to the framework. He summarised Wiggins' original \ac{CSF} as consisting of the following basic elements:

\begin{enumerate}
	\item the universal set of concepts $U$,
	\item the language for expressing the relevant mappings $L$,
	\item a symbolic representation of the acceptability map $R$,
	\item a symbolic representation of the quality mapping $E$,
  \item a symbolic representation of the search mechanism $T$,
	\item an interpreter for expressions like 3 and 4 $[ ]$, and
	\item an interpreter for expressions like 5 $\langle , , \rangle$.
\end{enumerate}

This set of elements is described as the object-level (enabling exploratory search). The meta-level (enabling transformational search) has the same seven elements with one exception; the universal set of concepts $U$ contains concepts described at the object-level. This allows transformations to happen; concepts from the object-level are searched using criteria and mechanisms (elements 2 to 5) from the meta-level, giving rise to a new and different subset of concepts to those which an object-level search would have produced.

A typical search process would go as follows. We start with an initial set of concepts C that represent our conceptual space and a query. We then explore C and find any elements that match the query with a certain quality (norm and value criteria) in a given amount of iterations. This produces the object-level set of exploratory concepts (in Boden's sense) which we would call the traditional search results. To get creative results we would need to apply the meta-level search (Boden's transformational search) with slightly different quality criteria, as suggested in the next section.

\paragraph{Uncreativity}
Wiggins explained various situations of creativity not taking place (uninspiration and aberration) in terms of his framework\marginnote{\faicon{table}~\ref{tab:wiggins}}. For example, a system not finding any valuable concepts would be expressed as $[E](U) = 0$ (in Wiggins' original notation). While this approach seems counter-intuitive and impractical, it actually provides an interesting inspiration on how to formulate some of our pataphysical concepts in terms of the \ac{CSF}.
\todo{explain more}

\begin{table}[!htbp]
  \centering
  \begin{tabu}{XXX}
  \toprule
  Hopeless Uninspiration & $V_\alpha (X) = \emptyset$ & valued set of concepts is empty \\
  Conceptual Uninspiration & $V_\alpha (N_\alpha (X)) = \emptyset$ & no accepted concepts are valuable \\
  Generative Uninspiration & $elements(A) = \emptyset$ & set of reachable concepts is empty \\
  Aberration	& $B$ is the set of reachable concepts not in $[N]_\alpha (X)$ and $B \neq \emptyset$ & search goes outside normal boundaries \\
  Perfect Aberration & $V_\alpha (B) = B$ & \\
  Productive Aberration &	$V_\alpha (B) \neq \emptyset$ and $V_\alpha (B) \neq B$ & \\
  Pointless Aberration & $V_\alpha (B) = \emptyset$ & \\
  \bottomrule
  \end{tabu}
\caption[Wiggins' uncreative concepts]{Wiggins' uncreative concepts in Ritchie's notation}
\label{tab:wiggins}
\end{table}

\spirals

\todo{finish section}
Researchers at IBM have fallen into the trap of over-simplifying creativity and computational creativity \autocite{Varshney2013}. First they define machine creativity to be a system that produces `novel, useful and quality' artefacts.


novelty = Bayesian surprise \autocite{Baldi2010}


In a diagram on the difference between `computational creativity' and `cognitive informatics and computing' they describe the former as consisting of:

\begin{itemize}
  \item Planning how to make
  \item Idea generation
  \item Defining creativity
  \item Curiosity
  \item Assessment of creative artefacts
\end{itemize}



% Pataphysical Search Framework
% We have explained earlier that evaluating creative search results largely depends on the user's understanding of relevance, usefulness and creativity. We can define pataphysical concepts and evaluation criteria in terms of the \ac{CSF} to provide a common ground for this understanding. The suggested definitions should provide more suitable criteria for the analysis of creative search results rather than traditional results.
% Norm	N_α (X)={c ∈X ┤|N(c)> α}
%  N∈ [0,1]^X
% Value	V_α (X)={c ∈X ┤|V(c)> α}
%  V∈ [0,1]^X
% Pata	P_α (X)={c ∈X ┤|  c∈(CLI(X)∪〖ANT〗_α (X)∪〖ANO〗_α (X)∪SYZ(X)∪ABS(X))}
%
% Clinamen	CLI(X)={c ∈X ┤| 〖 N〗_0.9 (N_0.1 (c))} given X ⊆ U
% (smallest aberration to make biggest difference)
% Antinomy	〖ANT〗_α (X)={c ∈X ┤|  V(N_0 (c))  > α}
% given X ⊆ U
% (reachable  abnormal concepts with value)
% Anomaly	〖ANO〗_α (X)={c ∈X ┤|N(c)< α}  given X ⊆ U
% (reachable concepts outside the norm)
% Syzygy (1)	SYZ(query)= ⋃_(n=0)^3▒〖elements(Q(N,V)^n (query))〗
% (concepts reachable within 3 steps)
% Syzygy (2)	SYZ(X)= S'(X)
% (the transformed set of concepts)
%  S_obj→ S^meta→ 〖S'〗_obj
% Absolute	V_1 (N_1 (X))  ≠ \emptyset
% (criteria for value and norm must be perfectly matched)
% Table 3 – Extension to the \ac{CSF} to provide a notation for pataphysical concepts. Norm and value are defined in a traditional sense, with the other pataphysical concepts being defined around those two.
% Table 3 shows various definitions for pataphysical principles that can be applied as criteria in search processes. There are of course more ways in which they could be expressed but as a demonstration this should suffice.
% 	Combined with a set of evaluative rules inspired by Ritchie (2001), this forms an essential part of creative exploratory search systems. Some example evaluative criteria are shown here:
% 	The norm and value ratings by themselves have lower importance than the pata rating,
% 	the average pata rating should be suitably high,
% 	results with good pata rating should form a significant proportion of the results, and
% 	there should be a good mixture of the different pataphysical concepts present in the results.
% Producing, interpreting and evaluating search results based on these ideas, together with a healthy knowledge and apprehension about the system's purpose, should be much more suitable and “fair” than trying to apply conventional measures such as precision and recall to something so subjective.


\stopcontents[chapters]
