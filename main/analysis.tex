% !TEX root = ../main.tex

\chapter{Patanalysis}
\label{ch:analysis}

\startcontents[chapters]

\vfill

\begin{alltt}\sffamily
Where thou mayst knock a nail into his head,
but near him thy angel becomes a fear,
it must omit real necessities,
hear Faith infringed which such zeal did swear.

With sighs in an odd angle of the isle,
before me to sweet beds of flow,
might quench the zeal of all professors else,
the whilst his iron did on the anvil cool.

Intend a kind of zeal both to the Prince and Claudio,
and threescore year would make the world away,
nay if you read this line.

Have no delight to pass away the time,
by a shadow like an angel,
four nights will quickly dream away the time.
\end{alltt}

\newpage
\minicontents
\spirals

A lot of the more theoretical aspects of this research have been discussed in chapters~\ref{ch:foundations}\marginnote{§~\ref{ch:foundations}~\&~\ref{ch:interpretation}} and \ref{ch:interpretation}. The evaluation here is more concerned with the practical artefact \url{pata.physics.wtf} and its interpretation.

The chapter is divided into several sections addressing issues related to \url{pata.physics.wtf}. This includes a discussion of the inspirations, an analysis of some of the technical aspects, a review of design decisions made, a contextualisation and also a meta-analysis of the project's execution and management.


\section{Influences}

Looking back over the inspirations for this project described in chapter~\ref{ch:inspirations}\marginnote{§~\ref{ch:inspirations}}, some of the influences can be clearly seen straight away. Others are intentionally a bit more subtle. There are various motivations for that. First, transparency conflicts with surprise. \emph{Serendipity} was one of the original aims to try and model, so being overly obvious and descriptive about what the tool is and does would be counter productive. An element of surprise also makes it more enjoyable in repeat visits. Pure randomness is meaningless. Another reasons was \emph{humour}. Pataphysics has an intrinsic kind of humour I wanted to include in the whole presentation of the artefact. 

\begin{description}
  \item[Syzygy Surfer]\marginnote{§~\ref{s:surfer}} The influence of the Syzygy Surfer cannot be overstated. It forms the immediate predecessor to my research. The authors of the Syzygy Surfer are part of my supervisory team. This is where the initial ideas for the pataphysical algorithms\marginnote{§~\ref{s:algorithms}} came from. There are important differences as well though. For example, pataphors were never implemented as originally suggested. The idea of using ontologies and semantic web technologies such as \ac{RDF} to develop the system was abandoned early on too.
  \item[Faustroll Library]\marginnote{§~\ref{s:faustlib}} This fictional library of real books was direct inspiration for the Faustroll corpus used in the text search\marginnote{§~\ref{s:corpora}}. I tried my best to complete the library as accurately as I could but some of the texts where unsourceable. As with the original, I included some foreign language texts. Since the results (if the Faustroll corpus is chosen of course) are drawn from any of these texts, the mood and style of language is quite distinct and atmospheric.
  \item[Queneau's $\bm{10^{14}}$ poems]\marginnote{§~\ref{s:queneau}} Queneau is another one of the inspirations that became a direct influence. The text search can be displayed as poetry\marginnote{§~\ref{s:poetry}} in the same style as Queneau's \num{100} thousand million poems only in digital form and with a larger set of lines. This means that many more possible poems can be generated by switching individual lines. The outcome is beautiful.
  \item[Chinese Encyclopedia]\marginnote{§~\ref{s:borges}} Borges story has been an inspiration right from the start. The subtle humour in it is great. The sort of semantic logic behind it was modeled through the pataphysical algorithms\marginnote{§~\ref{s:algorithms}}.
  \item[Yossarian]\marginnote{§~\ref{s:yossarian}} The metaphorical algorithms are intriguing but elusive---I wasn't able to find any details on their implementation. This may be due to the nature of the project, which is commercial rather than academic. It is hard to compare against this site as it is so different even though we share some of the same goals or principles.
  \item[Library of Babel]\marginnote{§~\ref{s:babel}} The library of babel is a great project which has only indirectly influence my work. The pataphysical elements in it are obvious even though perhaps unconscious. The seriousness with which the library is presented, the pseudo-scientific approach, the vagueness of what's actually behind it. Is it random? Or is it indeed the most gigantic digital library of any book every written or even to be written? The sheer perceived scale of the library was part motivation for calculating the numbers of the generatable poems\marginnote{\faicon{table}~\ref{tab:faustshake}}.
  \item[Oulipo]\marginnote{§~\ref{s:oulipo}} Given that the \ac{OULIPO} is directly rooted in pataphysical principles\footnote{Remember that the \ac{OULIPO} was founded as a subcommittee of the ``Coll\`{e}ge de \'Pataphysique'' in the 60's.}, the influence on this project cannot be underestimated. The algorithms\marginnote{§~\ref{s:algorithms}} created could even be seen as an oulipian technique themselves.
  \item[Coder Culture]\marginnote{§~\ref{s:culture}} This group of inspirations is a bit more generic and influenced lots of little things throughout the project. The idea of hiding easter eggs on the site, the deliberate placement or use of errors, the obfuscation, the humour, the jargonisation and littered `l33t' style language, and the art and aesthetics behind it. All of that was influenced by coder culture---and most of all perhaps: this thesis.
\end{description}


\section{Pataphysicalisation}

As mentioned in chapter~\ref{s:pataputers}\marginnote{§~\ref{s:pataputers}},tThe internal transformation of a query term to the final results is what I called the \emph{pataphysicalisation} process. The three pataphysical algorithms (Clinamen, Syzygy and Antinomy), or \emph{patalgorithms}, are at the center of this process. 

\begin{enumerate}
  \item User enters single query term,
  \item system transforms query term into list of pataphysicalised terms,
  \item system retrieves sentence fragments containing keywords from this list,
  \item system displays sentence fragments in various formats.
\end{enumerate}

It is quite interesting to compare the algorithms with each other. By removing the clutter (in this case the sentence surrounding the pataphysicalised keyword) we can see a few example results side by side below in table~\ref{tab:algorithmscomp}.

\begin{table}[!htbp]
\caption[Comparison of patalgorithms]{Comparison of patalgorithms showing a selection of results for each}
\label{tab:algorithmscomp}
  \begin{tabu}{X[1,L]X[3,L]X[3,L]X[2,L]}
  \toprule
  % \cline{2-4}
  \textbf{Query} & \textbf{Clinamen} & \textbf{Syzygy} & \textbf{Antinomy}
  \\ \midrule
  \textbf{clear}
  &
  altar, leaf, pleas, cellar
  &
  vanish, allow, bare, pronounce
  &
  opaque
  \\ \cmidrule{1-4}
  \textbf{solid}
  &
  sound, valid, solar, slide
  &
  block, form, matter, crystal, powder
  &
  liquid, hollow
  \\ \cmidrule{1-4}
  \textbf{books}
  &
  boot, bones, hooks, rocks, banks
  &
  dialogue, authority, record, fact
  &
  ---
  \\ \cmidrule{1-4}
  \textbf{troll}
  &
  grill, role, tell
  &
  wheel, roll, mouth, speak
  &
  ---
  \\ \cmidrule{1-4}
  \textbf{live}
  &
  love, lies, river, wave, size, bite
  &
  breathe, people, domicile, taste, see, be
  &
  recorded, dead
  \\ \bottomrule
  \end{tabu}
\end{table}

Seeing the results in a table like\marginnote{\faicon{table}~\ref{tab:algorithmscomp}} this gives an almost immediate idea of how each algorithm works. This is not meant to be transparent and perhaps only after knowing the ins and outs of the algorithms can one recognise how each result was found. 

The clinamen results show words that contain one or two spelling errors of the original query term. It is perhaps counter-intuitive to have words such as `altar', `leaf' and `cellar' be classed as spelling errors of the word `clear' but they clearly could be. Remember that a spelling error can be classed in one of four ways: (1) deletion, (2) insertion, (3) substitution and (4) transposition. So, going from `clear' to `altar' is an instance of two times case 3 (`c' is replace by `a' and `e' is replaced by `t') and going from  `clear' to `leaf' is an example of case 1 (`c' is deleted) and case 3 (`r' is replaced by `f').

Looking at the second column (the syzygy results) shows the semantic relationship between the original query term and the results. Again, this may not be immediately noticeable but certainly once you know how the process works you can recognise the common relations. This is especially evident for the antinomy algorithm which is based on opposites.

\spirals

However it is equally interesting to compare some full sentences. Looking at some of the poems at the beginning of each chapter shows the variety of the possible outcomes (see pages \pageref{ch:introduction}, \pageref{ch:inspirations}, \pageref{ch:methodology}, \pageref{ch:pataphysics}, \pageref{ch:creativity}, \pageref{ch:technology}, \pageref{ch:evaluation}, \pageref{ch:foundations}, \pageref{ch:interpretation}, \pageref{ch:implementation}, \pageref{ch:applications}, \pageref{ch:analysis}, \pageref{ch:aspirations}, and \pageref{ch:observations}). It also highlights the difference between the two corpora. Poems based on the Faustroll corpus have a very different sound and feel to it than ones based on the Shakespeare corpus.

\begin{figure}[!htbp]
\centering
\begin{minipage}{.45\linewidth}
  \settowidth{\versewidth}{earth was flat like the floor of an Oven}
  % \PoemTitle{Flower}
  \begin{verse}[\versewidth]\sffamily\footnotesize
    There was a period put to the Fire\\
    pink and spot\\
    earth was flat like the floor of an Oven\\
    as much ease as a mower doth the grass

    during the first period of my captivity\\
    room with a hard earthen floor\\
    not within everyone's power\\
    or your favourite flowers died

    shocks lose power\\
    the white daisy\\
    after a long period

    poppy\\
    peony\\
    stock to all People
  \end{verse}
\end{minipage}
\hspace{.02\linewidth}
\begin{minipage}{.45\linewidth}
  \settowidth{\versewidth}{led by their master to the flow'red fields}
  % \PoemTitle{Flower}
  \begin{verse}[\versewidth]\sffamily\footnotesize
    O bloody period\\
    I as your lover speak\\
    has she such power\\
    gather those flowers

    thy lover\\
    juiced flowers\\
    had I been any god of power\\
    or a lover's lute

    the river hath thrice flow'd\\
    but sad mortality o'ersways their power\\
    now here a period of tumultuous broils

    led by their master to the flow'red fields\\
    not a minister in his power\\
    where soulds do couch on flowers
  \end{verse}
\end{minipage}
\caption[Faustroll vs. Shakespeare poetry]{Comparison of Faustroll (left) versus Shakespeare (right) poetry, both for query term `flower'}
\label{fig:2poems}
\end{figure}

Sometimes we can even get a general feel for the theme of the poem, as in we can recognize the connection, the relationship between the individual lines and what must be the original query term. Of course putting the poems into the chapters as they are---without specifically stating the keyword they were generated from or the corpus they are based on---makes them a bit more elusive.

The different language is quite obvious. This is helped by the fact that the Shakespeare corpus is of course written by the same author\footnote{Unless of course we believe the legends that Shakespeare didn't write those works by himself\ldots}. The Faustroll corpus contains text by over \num{20} different authors and in three different languages even.


\subsection{Numbers}
\label{s:numbers}

The above examples (table~\ref{tab:algorithmscomp} and figure~\ref{fig:2poems} give a good overview of the two main factors in the pataphysicalisation process, namely the three patalgorithms and the two corpora. Both only reflect a small selection of the variety of results produced though. It is therefore quite interesting to look at some actual numbers.

\begin{table}[!htbp]
\caption[Faustroll vs Shakespeare in numbers]{Faustroll versus Shakespeare in numbers}
\label{tab:faustshake}
  \centering
  \begin{tabu}{llcccc}
  \toprule
  \textbf{Query} & \textbf{Corpus} & \textbf{Results} & \textbf{Reverbs} & \textbf{Origins} & \textbf{Poems}\\
  \midrule
  \multirow{2}{*}{flower} & Faustroll   & \num{90}   & \num{25} & \num{18} & \num{7.8e10}\\
                          & Shakespeare & \num{158}  & \num{15} & \num{38} & \num{3.8e14}\\
  \cmidrule{1-6}
  \multirow{2}{*}{clear}  & Faustroll   & \num{542}  & \num{79} & \num{23} & \num{1.3e22}\\
                          & Shakespeare & \num{1445} & \num{72} & \num{38} & \num{1.5e28}\\
  \cmidrule{1-6}
  \multirow{2}{*}{troll}  & Faustroll   & \num{124}  & \num{16} & \num{16} & \num{4.4e12}\\
                          & Shakespeare & \num{327}  & \num{14} & \num{38} & \num{1.1e19}\\
  \cmidrule{1-6}
  \multirow{2}{*}{fania}  & Faustroll   & \num{9}    & \num{2}  & \num{6}  & \num{1}\\
                          & Shakespeare & \num{15}   & \num{2}  & \num{14} & \num{1}\\
  \bottomrule
  \end{tabu}
\end{table}

Table~\ref{tab:faustshake}\marginnote{\faicon{table}~\ref{tab:faustshake}} shows a comparison of the two different corpora with four example query terms.

\begin{description}
  \item[Results] A `result' in this case is one line (a sentence fragment). This column shows the total number of results found by the three algorithms combined. Individual results appear only once but the keyword in contains can appear in several of the results.
  \item[Reverbs] A `reverberation' is one of the terms in the list of keywords produced by the pataphysicalisation process. The list cannot contain duplicates but each reverberation cab appear in more than one result. Reverberations are used to find results in each corpus. This column shows the total number of reverberations created by the three algorithms.
  \item[Origins] An `origin' in this case is the original source text from which a given sentence fragment was retrieved. Each corpus has a set number of source texts. Each origin can contain several results based on several reverberations. This column shows the number of origins in the given corpus in which results where found.
  \item[Poems] This refers to the total number of Queneau style poems that can be generated using the given results\footnote{The original book by Queneau contains \num{10} sonnets with \num{14} lines each. This means the total number of possible poems generated by different combinations of lines in the book is $10^{14}$ or one hundred thousand million.}. This is calculated as the number of different options per line to the power of the number of lines.
\end{description}

To put this into perspective, the Faustroll corpus contains a total of \num{28} texts of very varied authors and different languages even. This might explain why not the queries in table~\ref{tab:faustshake}\marginnote{\faicon{table}~\ref{tab:faustshake}} have not found results in all of the texts. The query `clear' found results in \num{23} out of \num{28} for example while the query `fania' only found results in \num{6} texts. The Shakespeare corpus seems much more uniform. Reverberations generally seem to find results in all \num{38} source texts in the corpus apart from the query `fania'. This might be explained by the fact that Shakespeare wrote all of the texts himself using much of the same language and vocabulary unlike the Faustroll corpus. 

It is rather interesting to note that even though the Shakespeare corpus produces overall more results from more texts, the Faustroll corpus produces more reverberations per query. This might stem from the multi-author, multi-language nature of the corpus. The overall vocabulary\marginnote{§~\ref{s:index}} used is much larger than the Shakespeare one.

Regarding the final column showing the number of possible poems, let's look at the Shakespeare---clear row. There are \num{1445} number of results. These are spread over \num{14} lines, so each line has \num{103} options. The overall number of poems is therefore calculated as $103^{14}$ which equals \num{15125897248551112432256145169} (or \num{1.5e28} in short\marginnote{\faicon{table}~\ref{tab:faustshake}}).

\spirals

A slightly different angle to consider is a comparison of these kind of numbers between each of the algorithms. Table~\ref{tab:algonums}\marginnote{\faicon{table}~\ref{tab:algonums}} shows the numbers of results, reverberations and origins for the Clinamen, Syzygy and Antinomy algorithms using four example query terms (`clear', `shine', `disorder' and `stuck') for each of the two corpora (`Faustroll' and `Shakespeare').

% \todo{add french query term cœur}
\begin{table}[!htbp]
\caption[Numbers per algorithm]{Results-Reverberations-Origin numbers per algorithm}
\label{tab:algonums}
\centering\small
\begin{tabu}{ll|ccc|ccc|ccc|ccc}
\toprule
 & & \multicolumn{3}{c}{\textbf{Clinamen}} & \multicolumn{3}{c}{\textbf{Syzygy}} & \multicolumn{3}{c|}{\textbf{Antinomy}} & \multicolumn{3}{c}{} \\ 
\cmidrule{3-11}
\multicolumn{1}{l}{} & \textbf{Query} & \rotatebox{90}{Results} & \rotatebox{90}{Reverbs} & \rotatebox{90}{Origins} & \rotatebox{90}{Results} & \rotatebox{90}{Reverbs} & \rotatebox{90}{Origins} & \rotatebox{90}{Results} & \rotatebox{90}{Reverbs} & \rotatebox{90}{Origins} & \multicolumn{3}{c}{\textbf{Total}} \\ 
\midrule
\multicolumn{1}{l}{\multirow{4}{*}{\textbf{\rotatebox{90}{Faustroll}}}} 
& clear & 158 & 20 & 13 & 368 & 90 & 23 & 16 & 8 & 8 & \multicolumn{3}{c}{542--79--23} \\
\multicolumn{1}{l}{} 
& shine & 228 & 29 & 19 & 154 & 61 & 16 & 0 & 0 & 0 & \multicolumn{3}{c}{382--61--20} \\
\multicolumn{1}{l}{}
& disorder & 0 & 0 & 0 & 159 & 127 & 23 & 10 & 2 & 10 & \multicolumn{3}{c}{169--40--23} \\
\multicolumn{1}{l}{}
& stuck & 59 & 14 & 13 & 181 & 43 & 22 & 11 & 3 & 9 & \multicolumn{3}{c}{251--47--22} \\ 
\multicolumn{1}{l}{}
& feather & 78 & 13 & 12 & 83 & 37 & 14 & 0 & 0 & 0 & \multicolumn{3}{c}{161--29--14} \\[0.5cm] 
\cmidrule{1-12}
\multicolumn{1}{l}{\multirow{4}{*}{\textbf{\rotatebox{90}{Shakespeare}}}}
& clear & 435 & 20 & 38 & 997 & 90 & 38 & 13 & 8 & 12 & \multicolumn{3}{c}{1445--72--38} \\
\multicolumn{1}{l}{}
& shine & 575 & 29 & 38 & 333 & 61 & 38 & 0 & 0 & 0 & \multicolumn{3}{c}{908--53--38} \\
\multicolumn{1}{l}{}
& disorder & 0 & 0 & 0 & 326 & 127 & 38 & 29 & 2 & 29 & \multicolumn{3}{c}{355--26--38} \\
\multicolumn{1}{l}{}
& stuck & 152 & 14 & 37 & 479 & 43 & 38 & 34 & 3 & 34 & \multicolumn{3}{c}{665--41--38} \\ 
\multicolumn{1}{l}{}
& feather & 217 & 13 & 38 & 195 & 37 & 38 & 0 & 0 & 0 & \multicolumn{3}{c}{412--25--38} \\ 
\bottomrule
\end{tabu}
\end{table}

The first immediate observation surely must be that the Antinomy algorithm produces the fewest results, in two cases even none at all. This is caused by the fact that the Antinomy algorithm\marginnote{§~\ref{s:antinomy}} is based on semantic opposites in WordNet and some words simply do not have defined opposites. Addressing this issue was left for future work mentioned in chapter~\ref{ch:future}\marginnote{§~\ref{ch:future}}. On the other hand the Syzygy algorithm\marginnote{§~\ref{s:syzygy}}, which is also based on WordNet, produces most results on average.

The Clinamen algorithm\marginnote{§~\ref{s:clinamen}} interestingly produces a varying number of results depending on the query term. For the query `disorder' no results where found in either the Faustroll or the Shakespeare corpus. This of course is rooted in the fact that no reverberations where produced during the pataphysicalisation process. Here it is important to remember that the Clinamen algorithm makes use of a base document\footnote{This is hardcoded to be Jarry's \textit{Exploits and Opinions of Doctor Faustroll, Pataphysician}. Section~\ref{s:basetext} discusses what would happen if we changed the base document to something else.}\marginnote{§~\ref{s:basetext}}. Therefore the success of the algorithm depends on the vocabulary of this base text. In this particular example this means that there was no word in the base text of one or two spelling errors to the original query of `disorder'.

Looking at the origins column in table~\ref{tab:algonums}\marginnote{\faicon{table}~\ref{tab:algonums}} highlights how the Shakespeare corpus mostly produces results from each of its \num{38} texts. The Faustroll corpus varies a lot more. This may be due to the different languages and varying word counts of the files in the corpus.

\paragraph*{Faustroll}
\begin{itemize}
\vspace{-0.5cm}
  \item There are three empty texts (Peladan, de Chilra, de Regnier).
  \item The total number of words is \num{1738461}. Of this, \num{1204158} words are from English texts (70\%), \num{497144} are French (28\%) and \num{37159} are in German (2\%).
  \item The shortest text contains \num{3853} words (Coleridge).
  \item The longest text contains \num{419456} words (Poe).
  \item The average amount of words per text is \num{62088}.
  \item The vocabulary of the index contains \num{78893} words. Of this \num{49040} are English terms.
\end{itemize}

\paragraph*{Shakespeare}
\begin{itemize}
\vspace{-0.5cm}
  \item The total number of words is \num{883460}\footnote{According to \autocite{Efron1976} Shakespeare used \num{31534} different words in his works, about half of which he only used once (\num{14376}). They cite the total number of words used in his corpus as \num{884647}.}.
  \item The shortest text contains \num{2568} words (Lover's Complaint).
  \item The longest text contains \num{32031} words (Hamlet).
  \item The average amount of words per text is \num{23249}.
  \item The vocabulary of the index contains \num{23398} words.
\end{itemize}

It should be noted that the index\marginnote{§~\ref{s:index}} is generated based on the texts vocabulary minus stopwords. Stopwords (e.g. `and', `or', `the', etc.) are common terms that occur frequently in use. The full list of stopwords per language can be found in appendix~\ref{app:stopwords}\marginnote{§~\ref{app:stopwords}}.


\subsection{Sentences}
\label{s:sents}

The index stores entries in the following format (for more detail see chapter~\ref{s:index}\marginnote{§~\ref{s:index}}).

\begin{minted}{text}
{
  word1: {fileA: [pos1, pos2, ...], fileB: [pos1], ...},
  word2: {fileC: [pos1, pos2], fileK: [pos1, pos2, pos3, ...], ...},
  ...
}
\end{minted}

At the top level we have a list of words. Each word contains a list of files and each file stores a list of positions. After the pataphysicalisation process, any entries in the index that match the pataphysicalised query terms are looked up and then the corresponding sentences are retrieved to display as results. The code is set up to retrieve the first position only instead of each one, referred to as the \emph{first only} method from now on (see source~\ref{code:ppsent}\marginnote{\faicon{code}~\ref{code:ppsent}}).

\begin{minted}{text}
{
  word1: {fileA: [pos1], fileB: [pos1], ...},
  word2: {fileC: [pos1], fileK: [pos1], ...},
  ...
}
\end{minted}

This has two implications: (1) there is some unnecessary computation at the startup of the program when then index is generated and (2) only a fraction of the possible results are retrieved.

The decision to only use one position was mainly made for performance issues. Generating the full results with each position (the \emph{return all} method) takes a lot more time than doing it for just the first occurance. This is perhaps best understood by looking at an example.

The Faustroll corpus produces \num{542} results for the query `clear' with only the first sentence. If we enable the retrieval of every matching sentence, the number of results increases to \num{8751}.

\begin{minted}{text}
cellar: {l_19: [4448, 18718, 68678, 110318, 192486, 267241, 352502, 352565]}
\end{minted}

The above pseudocode shows an entry for the word `cellar' with only the positions for the \py{l_19} file\footnote{Francois Rabelais: Gargantua and Pantagruel}. Another example of an index entry for the term `doctor' can be found on page~\pageref{c:pos}. The sentences for the above positions are shown below. Using only the first occurance (position) means the system ignores the rest.

\begin{itemize}
  \item ``rope wine is let down into a cellar''
  \item ``bread and holy water of the cellar''
  \item ``year who had a cool cellar under ground''
  \item ``cellar''
  \item ``that Nick in the dark cellar''
  \item ``on the cellar door''
  \item ``in mind of the painted cellar in the oldest city in the world''
  \item ``and the painted cellar also''
\end{itemize}

Table~\ref{tab:percent}\marginnote{\faicon{table}~\ref{tab:percent}} shows some example queries for both corpora and the number of results retrieved with the first position only used (as in the live version of \url{pata.physics.wtf}) in column 5 and on column 3 with all results retrieved. The final column shows what percentage of results are retrieved using the `first only' method. The average percentage for this is about 10\%. 

\begin{table}[!htbp]
\caption[Count and time of results]{Count, time and percentage of results retrieved}
\label{tab:percent}
  \centering
  \begin{tabu}{llccccc}
  \toprule
  & & \multicolumn{2}{c}{\textbf{Return all}} & \multicolumn{2}{c}{\textbf{First only}} & \\
  \cmidrule{3-4}\cmidrule{5-6}
  \textbf{Query} & \textbf{Corpus} & \textit{Count} & \textit{Time} & \textit{Count} & \textit{Time} & \textbf{Percent} \\
  \midrule
  \multirow{2}{*}{clear} 
  & Faustroll   & \num{8751}  & 59s   & \num{542}  & 1.83s & 6.19\%  \\
  & Shakespeare & \num{11304} & 69.2s & \num{1445} & 3.59s & 12.78\% \\
  \cmidrule{1-7}
  \multirow{2}{*}{solution} 
  & Faustroll   & \num{693}   & 11.7s & \num{53}   & 0.98s & 7.65\%  \\
  & Shakespeare & \num{547}   & 8.51s & \num{86}   & 1.07s & 15.72\% \\
  \cmidrule{1-7}
  \multirow{2}{*}{form} 
  & Faustroll   & \num{19222} & 120s  & \num{1064} & 2.81s & 5.54\%  \\
  & Shakespeare & \num{13635} & 90s   & \num{2125} & 4.63s & 15.58\% \\
  \cmidrule{1-7}
  \multirow{2}{*}{record}
  & Faustroll   & \num{5199}  & 38s   & \num{275}  & 1.72s & 5.29\%  \\
  & Shakespeare & \num{7631}  & 49.2s & \num{794}  & 2.09s & 10.40\% \\ 
  \bottomrule
  \end{tabu}
\end{table}

Google recommends having a ``response time under \num{200}ms''\footnote{\url{https://developers.google.com/speed/docs/insights/Server}}. The numbers in table~\ref{tab:percent}\marginnote{\faicon{table}~\ref{tab:percent}} clearly show that the `return all' method is unacceptable in terms of speed performance. Using the `first only' method is much closer to the recommended speed limit. Columns 4 and 6 show the time it takes for the page to load from the user query to the display of results. The times are shown in seconds. The data for column 4 was generated using a Chrome browser plugin called ``Load-timer'' by alex-vv\footnote{\url{https://github.com/avflance/chrome-load-timer}} and the data for column 6 was generated by the Chrome ``Developer Tools''.


\subsection{Index}
\label{s:analindex}

The index\marginnote{§~\ref{s:index}} is a central part of the \url{pata.physics.wtf} system. It is generated when the program/server is first started up but then cached and re-used. The initial process of going over all the text files in each corpus takes a few minutes. Of course in comparison to a full Internet crawl this is a tiny amoutn of data to be processed. 

The Faustroll corpus\marginnote{§~\ref{s:corpora}} for example contains \num{28} texts\footnote{This is technically not true since a few of those files are empty.}. Individually they are small plaintext files of sizes between 24KB (Coleridge) and 2MB (Poe). This is of course caused by the nature of some of these texts. Samuel Coleridge's \textit{The Rime of the Ancient Mariner} is a poem whereas the Edgar Allan Poe file contains a collection of all of his works. The total size of the Faustroll corpus is 10MB. The Shakespeare corpus is much more evenly distributed as all of his works are separated out into individual text files of an average size of around 150KB. The total size of the Shakespeare corpus is only 5.3MB.

Now, the size of the actual index data structure is interesting. Processing the Faustroll corpus alone produced an index of 12.4MB. That's larger than the actual size of the corpus. Remember, the index contains each word that occurs anywhere in the corpus together with the list of files it is found in and the specific locations within each text. This includes English words buts also French and German terms since the Faustroll corpus is multi-lingual. The combined index is therefore 35.2MB large.

Figure~\ref{fig:termdocs}\marginnote{\faicon{object-group}~\ref{fig:termdocs}} shows some example words and how often they occur in three example files of the Faustroll corpus in the form of a \ac{TDM} (see chapter~\ref{ch:technology} for more details). Implementing the Faustroll corpus index as a \ac{TDM} properly, would result in a $78893 \times 28$ matrix---the number of words (not counting duplicates) times the number of files in the corpus.

\spirals

As mentioned before\marginnote{§~\ref{s:index}}, the index is structured in a double nested dictionary style list as shown below.

\begin{minted}{text}
{
  word1: {fileA: [pos1, pos2, ...], fileB: [pos1], ...},
  word2: {fileC: [pos1, pos2], fileK: [pos1, pos2, pos3, ...], ...},
  ...
}
\end{minted}

There are other options of how to make this data structure. For example we could store a list of pataphysicalised query terms (\emph{patadata}) with each word and the full sentence fragment with each position. This would allow faster retrieval at query time but would increase the time needed for the initial startup. Additionally we could store data on rhyming patterns directly in the index with each word entry. This would of course be beneficial for the implementation of a rhyming scheme for the poetry generation. See also chapter~\ref{ch:future}\marginnote{§~\ref{ch:future}}.

\begin{minted}{text}
{
  word1: ([patadata], [rhymes], {fileA: [(pos1, sent), (pos2, sent), ...], fileB: [(pos1, sent)], ...}),
  word2: ([patadata], [rhymes], {fileC: [(pos1, sent), (pos2, sent)], fileK: [(pos1, sent), (pos2, sent), (pos3, sent), ...]), ...},
  ...
}
\end{minted}

\spirals

As a comparison to the \num{35} megabyte index generated by the system described in this thesis, and the search times mentioned in table~\ref{tab:percent}\marginnote{\faicon{table}~\ref{tab:percent}}, Google claims to have ``well over \num{100000000} gigabytes'' of data in their index and that they've spent ``over one million computing hours to build it''. Similarly Google managed to retrieve about \num{2140000000} results for the query `clear' in 0.85 seconds.

\begin{quotation}
  The web is like an ever-growing public library with billions of books and no central filing system. Google essentially gathers the pages during the crawl process and then creates an index, so we know exactly how to look things up. Much like the index in the back of a book, the Google index includes information about words and their locations. When you search, at the most basic level, our algorithms look up your search terms in the index to find the appropriate pages.

  The search process gets much more complex from there. When you search for ``dogs'' you don't want a page with the word ``dogs'' on it hundreds of times. You probably want pictures, videos or a list of breeds. Google's indexing systems note many different aspects of pages, such as when they were published, whether they contain pictures and videos, and much more.\sourceatright{\autocite{GoogleCI}}
\end{quotation}

It is also worth noting that Google for example also uses a form of pataphysicalisation. In their case of course the aim of the pataphysicalisation isn't to infuse the result with pataphysics but to make it more relevant and interesting to users. They use techniques such as PageRank and query expansion to achieve this. See chapter~\ref{ch:technology}\marginnote{§~\ref{ch:technology}} for more information on this.


\subsection{Clinamen}

The clinamen\marginnote{§~\ref{s:clinamenalgo}} function uses the Damerau-Levenshtein algorithm\marginnote{§~\ref{app:damlev}} to create patadata. It also uses the Faustroll text. The way this works is as follows. If the query term is a spelling error of size 1 or 2 of a term in the vocabulary within the faustroll text then it is included in the list of resulting terms. The logic behind this is due to the Damerau-Levenshtein algorithm needing two words to compare with each other. It also ensures we get real words as results and not some random gibberish.

Currently the algorithm is set to accept terms that have a difference of 1 or 2 to the original query. We can lower this to 1 to allow fewer results or increase it to make it broader. I felt 1 or 2 was a good compromise. Only allowing 1 error would mean terms are too similar. Allowing 3 might mean they are drastically different.


\subsubsection{Changing the base text}
\label{s:basetext}

As examples of using different base documents in the Clinamen algorithm I have used three examples. 

\begin{itemize}
  \item \textit{Midsummer Night's Dream} by Shakespeare (`Dream' in short)
  \item \textit{Arabian Nights} by various artists (`Nights' in short)
  \item \textit{Exploits and Opinions of Doctor Faustroll, Pataphysician} by Jarry (`Faustroll' in short)
\end{itemize}

Figure~\ref{fig:changebase}\marginnote{\faicon{table}~\ref{fig:changebase}} shows three tables, each compare the full list of pataphysicalised terms for a particular query term for the three base texts above. These examples show that changing the base text of the algorithm does indeed change the set of results you get. 

The decision to use the Faustroll text as a base text was made due to the central role it has for pataphysics\marginnote{§~\ref{ch:pataphysics}} and indeed the corpus itself. The Faustroll book introduces pataphysics and contains Jarry's original definition and it also lists Dr. Faustroll's library of `equivalent books'\marginnote{§~\ref{s:faustlib}} which was used as the inspiration for the Faustroll corpus.

\begin{figure}[!p]
\centering
\begin{subfigure}[b]{\textwidth}
  \captionof{figure}{Changing base in Clinamen - query `fania'}
  \label{tab:basefania}
  \begin{tabu}{X[l]X[1.5,l]X[l]}
    \toprule
    \textbf{Dream} & \textbf{Nights} & \textbf{Faustroll}\\
    \midrule
    fail, faint, fair, fan, fancy 
    & 
    fail, fain, faint, fair, fancy, Sadia 
    & 
    fan, fans, Tanit\\
    \bottomrule
    \end{tabu}
\end{subfigure}
\vskip\baselineskip
\vskip\baselineskip
\begin{subfigure}[b]{\textwidth}
  \captionof{figure}{Changing base in Clinamen - query `clear'}
  \label{tab:baseclear}
  \begin{tabu}{X[l]X[1.5,l]X[l]}
    \toprule
    \textbf{Dream} & \textbf{Nights} & \textbf{Faustroll}\\
    \midrule
    altar, bear, car, cheer, clean, clear, dear, ear, fear, hear, lead, liar, near, plead, rear, swear, tear, wear 
    & 
    bear, cedar, cellar, cheap, clad, clap, clean, clear, cleared, clearer, clearly, clever, dear, ear, fear, hear, lead, leaf, leap, learn, liar, near, swear, tear, wear, year 
    & 
    altar, cedar, cellar, clad, clean, clear, clearly, dear, ear, fear, hear, lead, leaf, leap, near, pleas, rear, swear, year\\
    \bottomrule
    \end{tabu}
\end{subfigure}
\vskip\baselineskip
\vskip\baselineskip
\begin{subfigure}[b]{\textwidth}
  \captionof{figure}{Changing base in Clinamen - query `moss'}
  \label{tab:basemoss}
  \begin{tabu}{X[l]X[1.1,l]X[1.2,l]}
    \toprule
    \textbf{Dream} & \textbf{Nights} & \textbf{Faustroll}\\
    \midrule
    amiss, ass, boys, costs, cross, dost, fogs, gods, goes, gross, kiss, Less, loos, lose, lost, mask, moan, moans, mock, mole, mood, moon, more, morn, most, mote, mous, mouse, move, musk, must, nose, oes, pass, ress, rose, roses, toys, vows 
    & 
    amiss, ass, bows, boys, cost, cosy, cross, does, dogs, foes, goes, host, hosts, kiss, less, lose, loss, lost, lots, lows, mass, massy, mess, mist, mode, moon, more, Moses, most, mouse, move, moves, musk, must, pass, post, pots, rocs, rose, roses, sobs, sons, vows 
    & 
    ass, Bosse, bows, Boys, cost, costs, cows, cross, does, dogs, ess, fess, gods, goes, host, kiss, less, lose, loss, lost, lots, maps, mask, mass, mast, masts, mesh, mist, mob, moist, moles, moon, mor, more, Moses, most, must, nos, nose, pass, piss, rose, rosy, rows, sons, sows, toes, tops\\
    \bottomrule
    \end{tabu}
\end{subfigure}
  \caption[Changing base in Clinamen]{3 tables showing results for different queries after changing the Clinamen base text}
  \label{fig:changebase}
\end{figure}


\subsubsection{Changing number of errors}

Another key factor in how the Clinamen function works is the Damerau-Levenshtein algorithm (see source~\ref{code:dl})\marginnote{§~\ref{code:dl}} integration. The algorithm works by comparing two words and calculating the difference between them. A difference is counted the sum of (1) deletions, (2) insertions, (3) substitutions and (4) transpositions. 

If we decrease or increase the number of errors allowed we get drastically different results. The Clinamen algorithm of \url{pata.physics.wtf} uses up to 2 errors, as this was considered a reasonable amount of results (trading variety for speed). Table~\ref{tab:errors})\marginnote{\faicon{table}~\ref{tab:errors}} shows three example queries and the number of results produced by the algorithm with either up to 1 error, up to 2 errors or up to 3 errors.

\begin{table}[!htbp]
  \caption[Changing number of errors in Clinamen]{Changing number of errors in Clinamen}
  \label{tab:errors}
  \centering
  \begin{tabu}{lccc}
    \toprule
    \textbf{Query} & \textbf{Up to 1} & \textbf{Up to 2} & \textbf{Up to 3}\\
    \midrule
    clear & 2 & 20 & 136 \\
    fania & 0 & 3 & 118 \\
    moss & 3 & 49 & 457 \\
    \bottomrule
  \end{tabu}
\end{table}


\subsection{Syzygy}
\label{s:analsyzygy}

\begin{figure}[!htbp]
\centering
  \input{images/wordnet.pdf_tex}
  \caption[Semantic relationships of `feather']{Semantic relationships of `feather'}
\label{fig:wordnet}
\end{figure}

The syzygy function (see source~\ref{code:syzygy}\marginnote{\faicon{code}~\ref{code:syzygy}}) goes through the following process.

\begin{enumerate}
  \item A set of synonyms (a list of ``synsets'') is retrieved.
  \item For each of these, hyponyms, hypernyms, holonyms and meronyms are retrieved.
\end{enumerate}

The notation used by WordNet for synsets is \textbf{<lemma>.<pos>.<senses>}. The `lemma' is the morphological stem of the word. The `pos' stands for part-of-speech and can be `n' for nouns, `v' for verbs, `a'
for adjectives, `r' for adverbs and `s' for satellites. The `senses' element stands for the number of synsets the relevant lemma is part of (a word might have a noun sense as well as a verb sense for example in which case the number would be `02'). For the query `clear' for instance, the following list of synsets is retrieved for step (1).

\begin{minted}{text}
[
  clear.n.01, open.n.01, unclutter.v.01, clear.v.02, clear_up.v.04, authorize.v.01, clear.v.05, pass.v.09, clear.v.07, clear.v.08, clear.v.09, clear.v.10, clear.v.11, clear.v.12, net.v.02, net.v.01, gain.v.08, clear.v.16, clear.v.17, acquit.v.01, clear.v.19, clear.v.20, clear.v.21, clear.v.22, clear.v.23, clear.v.24, clear.a.01, clear.s.02, clear.s.03, clear.a.04, clear.s.05, clear.s.06, clean.s.03, clear.s.08, clear.s.09, well-defined.a.02, clear.a.11, clean.s.02, clear.s.13, clear.s.14, clear.s.15, absolved.s.01, clear.s.17, clear.r.01, clearly.r.04
]
\end{minted}

Step (2) then retrieves related terms. Below is a list of terms it found. Not all synsets return each of the hypo-/hyper- and holo-/meronyms. This is clearer when inspecting the full list of results as shown in appendix~\ref{app:syzygy}\marginnote{§~\ref{app:syzygy}}. 

\begin{minted}{text}
[
  innocence, area, country, change, alter, modify, make, create, approbate, approve, O.K., okay, sanction, certificate, commission, declare, license, certify, validate, formalise, permit, allow, let, countenance, clear-cut, deforest, disafforest, denude, bare, denudate, strip, stump, remove, take, take_away, withdraw, clear, succeed, win, come_through, bring_home_the_bacon, deliver_the_goods, vanish, disappear, go_away, hop, pass, overtake, overhaul, clarify, clear_up, elucidate, free, discharge, rid, free, disembarass, yield, pay, bear, profit, gain, benefit, eke_out, squeeze_out, gross, profit, turn_a_profit, rake_in, shovel_in, rake_off, take_home, bring_home, yield, pay, bear, get, acquire, sell, pass, clear, purge, vindicate, whitewash, pronounce, label, judge, settle, square_off, square_up, determine, change, alter, modify, empty, take_out, move_out, remove, empty, remove, take, take_away, withdraw
]
\end{minted}

For the term `feather' the algorithm for example finds the hyponym `down', the hypernym `body covering', the holonym `bird' and the meronym `quill'. It also considers synonyms, so the term `fledge' for instance finds a hypernym of `develop'.

\begin{description}
  \item[Query] feather 
  \item[Synonyms] feather.n.01, feather.n.02, feather.v.01, feather.v.02, feather.v.03, feather.v.04, fledge.v.03
  \item[Hyponyms] down\_feather, quill\_feather, aftershaft, bastard\_wing, scapular, alula, spurious\_wing, flight\_feather, down, marabou, contour\_feather, hackle, quill, pinion
  \item[Hypernyms] body\_covering, acquire, join, get, conjoin, cover, paddle, grow, produce, animal\_material, develop, rotation, rotary\_motion, row
  \item[Holonyms] rowing, bird, row
  \item[Meronyms] shaft, calamus, web, ceratin, vane, melanin, keratin, quill
\end{description}

Table~\ref{tab:semnum}\marginnote{\faicon{table}~\ref{tab:semnum}} shows the spread of numbers retrieved by the various semantic relationships to some example queries. This highlights how the holonym function of WordNet returns very few results. The meronym function is a bit more reliable but also occasionally produces no results depending on whether there are any holonyms or meronyms for the query term.

\begin{table}[!htbp]
\centering
\caption[Quantities of different semantic relations]{Quantities of different semantic relations}
\label{tab:semnum}
\begin{tabu}{lccccc}
\toprule
\textbf{Query} & \textbf{Syno} & \textbf{Hypo} & \textbf{Hyper} & \textbf{Holo} & \textbf{Mero} \\ \midrule
clear   & 45   & 41   & 65    & 0    & 0    \\
feather & 7    & 14   & 14    & 3    & 8    \\
death   & 8    & 34   & 13    & 4    & 0    \\
page    & 9    & 14   & 13    & 0    & 7    \\
book    & 15   & 85   & 32    & 2    & 22   \\
seed    & 13   & 39   & 35    & 0    & 12   \\
web     & 8    & 10   & 15    & 4    & 1    \\ \bottomrule
\end{tabu}
\end{table}


\subsection{Antinomy}

A similar problem arises of course with the Antinomy algorithm (see source~\ref{code:antinomy}\marginnote{\faicon{code}~\ref{code:antinomy}}) which relies on WordNet's antonyms. Both table~\ref{tab:algorithmscomp} and table~\ref{tab:algonums}\marginnote{\faicon{table}~\ref{tab:algorithmscomp} \& \ref{tab:algonums}} highlight this imbalance.


\subsection{APIs}
\label{s:apis}

The \ac{API} functions---image and video search---all share one major issue. This is to do with how images and videos are retrieved from the external store. Some people tend to upload sequences of images depicting the same content from different angles or time frames with the same tags. A query for hat tag then returns all of those matches even though the images are almost identical in nature. An example of this can be seen in figure~\ref{fig:imgspiralgetty}\marginnote{\faicon{picture-o}~\ref{fig:imgspiralgetty}}. This may have been addressed by adding checks in the code that make sure authors don't appear twice in the results.

Another way to address this was attempted by changing the query term for each image or video that is retrieved. As mentioned above, this only worked for some of the \ac{API}s.


\subsubsection{Call Structure}

The text search functionality of \url{pata.physics.wtf} is set up to only work with one \emph{single query term}, whereas the image and video search works on \emph{multiple word queries}. This is mainly due to the fact that the external \ac{API}s are already setup to allow for more than one search term. Usually they allow extra parameters too to narrow down the results. So for example we can search for ``blue kitten'' and the three \ac{API}s will return their respective results related to blue kittens. The service provided by companies in the form of \ac{API}s is not always free, sometimes only at a low usage quota. \ac{API}s are updated often and not always back-compatible, meaning out-of-date code needs to be maintained regularly to assure it works if changes to the \ac{API} are made.

Enabling multi-word queries in my system would involve a change that would propagate through quite a bit of code. There are two main approaches this could be achieved. One would be to pataphysicalise each query term individually and combine the results found. Another approach would be to change the code to work with actual multi-word queries. The algorithms are created for single words though and rewriting them to allow for more than one word would be difficult and most of all increase the time it takes to compute pataphysicalisations.

The lists below show the parameters related to the query for Flickr, Getty, Bing and YouTube.

\paragraph{Flickr:}
\begin{quotation}
  \begin{description}
  \vspace{-1cm}
    \item[text (Optional)] A free text search. Photos who's title, description or tags contain the text will be returned. You can exclude results that match a term by prepending it with a - character.
    \item[tags (Optional)] A comma-delimited list of tags. Photos with one or more of the tags listed will be returned. You can exclude results that match a term by prepending it with a - character.
    \item[tag\_mode (Optional)] Either `any' for an OR combination of tags, or `all' for an AND combination. Defaults to `any' if not specified.
  \end{description}
  \sourceatright{\autocite{FlickrAPI}}
\end{quotation}

The Flickr function in \url{pata.physics.wtf} uses the \py{tags} parameter to set the query and a \py{tag_mode} parameter of `all' to ensure multi-word queries are run as a conjunction. In chapter~\ref{s:imgvid}\marginnote{§~\ref{s:imgvid}} I explained how the Flickr algorithm essentially runs ten times, once for each pataphysicalised query term, to retrieve ten different images. This decision was taken to make sure images reflect the varied nature of the patadata.

A search for ``blue kitten'' on Flickr produces the following resulting pataphysicalised query terms: ``[artistrocratical, depressed, blueing, drab, puritanic, wild blue yonder, kitty, dingy, blueness, blue air]'' which are then passed into ten seperate \ac{API} calls to retrieve one image each (see figure~\ref{fig:imgspiralflickr}\marginnote{§~\ref{fig:imgspiralflickr}}). The results show a variety of images seemingly unrelated to each other. 

\begin{figure}[!htbp]
\centering
  \includegraphics[width=\linewidth]{bluekittenflickr}
\caption[Image spiral `blue kitten'---Flickr]{Image spiral for query `blue kitten'---Flickr}
\label{fig:imgspiralflickr}
\end{figure}

\paragraph{Getty:}
\begin{quotation}
  \begin{description}
  \vspace{-1cm}
    \item[keyword\_ids] Return only images tagged with specific keyword(s). Specify using a comma-separated list of keyword Ids. If keyword Ids and phrase are both specified, only those images matching the query phrase which also contain the requested keyword(s) are returned.
    \item[phrase] Search images using a search phrase.
  \end{description}
  \sourceatright{\autocite{GettyAPI}}
\end{quotation}

Getty uses the \py{phrase} parameter to set the query. It only creates one pataphysicalised query term from the original query and calls for ten results based on that. This decision was based on the quota restrictions\marginnote{§~\ref{s:quota}} defined by Getty. Their limit is based on calls per second rather than calls per day or month. This means we cannot run ten calls for each user query as we did with FLickr. The query ``blue kitten'' gets turned into the word ``racy'' which then calls the \ac{API} to retrieve ten results (see figure~\ref{fig:imgspiralgetty}\marginnote{\faicon{picture-o}~\ref{fig:imgspiralgetty}}). The results mostly show racing cars from various angles although one oddball snuck in too: an office scene Getty has deemed to be `racy' (a guy in a suit checking out a lady's behind while she's leaning over a laptop).

\begin{figure}[!htbp]
\centering
  \includegraphics[width=\linewidth]{bluekittengetty}
\caption[Image spiral `blue kitten'---Getty]{Image spiral for query `blue kitten'---Getty}
\label{fig:imgspiralgetty}
\end{figure}

\paragraph{Bing:}
\begin{quotation}
  \begin{description}
  \vspace{-1cm}
    \item[query] The user's search query string. The query string cannot be empty. The query string may contain Bing Advanced Operators\footnote{For example `AND', `OR', `imagesize:', `NOT', or `phrase'}. For example, to limit images to a specific domain, use the site: operator. To help improve relevance and the results, you should always include the user's query string in an insights query (see insightsToken). This parameter is supported only by the Image API; do not specify this parameter when calling the Trending Images API.
  \end{description}
  \sourceatright{\autocite{BingAPI}\footnote{Microsoft will discontinue this version of the current \ac{API} in December 2016. The new version is documented on \url{https://www.microsoft.com/cognitive-services/en-us/bing-image-search-api}.}}
\end{quotation}

The Bing function uses the \py{query} parameter to set the query in the same way as Getty.

\paragraph{YouTube:}
\begin{quotation}
  \begin{description}
  \vspace{-1cm}
    \item[q] The q parameter specifies the query term to search for. Your request can also use the Boolean NOT (-) and OR (|) operators to exclude videos or to find videos that are associated with one of several search terms. For example, to search for videos matching either ``boating'' or ``sailing'', set the q parameter value to boating|sailing. Similarly, to search for videos matching either ``boating'' or ``sailing'' but not ``fishing'', set the q parameter value to boating|sailing -fishing. Note that the pipe character must be URL-escaped when it is sent in your API request. The URL-escaped value for the pipe character is \%7C.
  \end{description}
  \sourceatright{\autocite{YouTubeAPI}}
\end{quotation}

Youtube works in a similar way too. The \py{q} parameter is set to the pataphysicalised query term and one call retrieves ten results.

Something else to consider is perhaps that it is not entirely clear how the internal search for each \ac{API} works. This means that there's a possibility that they do their own query expansion\marginnote{§~\ref{s:qexpansion}} in the background to find more matches.


\subsubsection{Quota}
\label{s:quota}

Each \ac{API} has a different quota for their subscription packages. At this stage this is not a problem but if usage of \url{pata.physics.wtf} were to increase by a lot then these limitations would cause issues. At that point there are two options: (1) live with these limits or (2) get funding to upgrade the subscriptions to these services.

\begin{description}
  \item[Flickr] \num{3600} queries per hour are free \autocite{FlickrGuideAPI}.
  \item[Getty] \num{5} calls per second, unlimited calls per day \autocite{GettyOverviewAPI}.
  \item[Bing] \num{5000} transactions per month are free. A transaction is one request that returns one page of results \autocite{BingAzureAPI}.
  \item[YouTube] \num{50000000} units per day, \num{300000} units per \num{100} seconds per user, and \num{3000000} requests per \num{100} seconds are free. A call to the video search method counts as \num{100} units \autocite{YouTubeAPI}.
  \item[Microsoft Translator] \num{2000000} characters per month are free. Note the quota relates to single characters, not words \autocite{TranslatorAPI}.
\end{description}


\section{Creativity \& Intelligence}

A more theoretical aspect of this analysis is concerned with what was already discussed to an extent in chapter~\ref{ch:interpretation} (specifically sections~\ref{ss:anthropomorphism}, \ref{s:programmer}, \ref{s:mimicry} and \ref{s:babying}), namely the thread connecting `artificial creativity' and \acl{AI}.

To me, the question of whether computers can be intelligent and make ethical decisions is the same as asking whether a computer can be creative. A lot of the arguments for or against \ac{AI} can be applied to computer creativity.  

Answering the question of whether computers can think in my view would also answer the question of whether computers can be creative.
 
Robert Horn groups the various strands of enquiry related to the question of `can computers think?' into 8 main arguments with several subquestions each \citeyear{Horn2009} (the full list of questions can be found in appendix~\ref{app:think}\marginnote{§~\ref{app:think}}). 

\begin{quotation}
  \begin{enumerate}
    \item Can computers think?
    \item Can the Turing test determine whether computers can think?
    \item Can physical symbol systems think?
    \item Can Chinese Rooms think?
    \item Can connectionist networks think?
    \item Can computers think in images?
    \item Do computers have to be conscious to think?
    \item Are thinking computers mathematically possible?
  \end{enumerate}
  \sourceatright{\autocite{Horn2009}}
\end{quotation}


\subsection{Free Will \& Surprise}

As early as 1842, Ada Lovelace briefly mentioned in the annotations to her translation of Menabrea's account of Babbage's \textit{Analytical Engine} that the ``Analytical Engine has no pretensions whatever to \textit{originate} anything. It can do \textit{whatever we know how to order it} to perform'', implying that the machine cannot think by itself \autocite{Menabrea1842}.

Alan Turing said in his article on thinking computers that ``to behave like a brain seems to involve free will, but the behaviours of a digital computer, when it has been programmed, is completely determined'' \citeyear{Turing1951}. 

Furthermore, in his famous article \textit{Computing Machinery and Intelligence} he mentions that a digital computer with a `random element' is ``sometimes described as having free will'' although he adds that he ``would not use this phrase'' himself \citeyear{Turing2009}. 

Introducing a random element to a computer program prevents us from fully predicting the outcome---leading to us being surprised.

The ability of computers to surprise their creators seems to be an indicator of intelligence. Turing suggests that ``we should be pleased when the machine surprises us, in rather the same way as one is pleased when a pupil does something which he had not been explicitly taught to do'' \citeyear{Turing1951}. 

\begin{quotation}
  If we give the machine a programme which results in its doing something interesting which we had not anticipated I should be inclined to say that the machine \textit{had} originated something, rather than to claim that its behaviour was implicit in the programme, and therefore that the originality lies entirely with us. \sourceatright{\autocite{Turing1951}} 
\end{quotation}

% ``The more complicated the machine to be imitated the more complicated must the programme be.''\autocite{Turing1951} 


\subsection{Understanding \& Simulation}

% epistemically objectivity (mountain A is higher than mountain B)
% epistemically subjectvity (mountain A is prettier than mountain B)

% ontologically objectivity (material world) - observer-independent
% ontologically subjectivity (money, itch, consciousness) - observer-relative

% Natural intelligence is observer-independent, intrinsic, conscious!
% Computer intelligence is observer-relative, not intrinsic

Strong \ac{AI}, sometimes called \ac{AGI} or true \ac{AI}, is the idea of human-level intelligence in machines. John Searle speaks against the possibility of this using his famous Chinese Room argument amongst others. His argument breaks down into the following juxtapositions \autocite{Searle2015, Searle1990}.

\begin{itemize}
  \item Syntax is not semantics.
  % \item Syntax is observer-relative (subjective).
  \item Semantics is not intrinsic to syntax.
  \item Simulation is not duplication.
  % \item Computation is observer-relative (subjective).
  \item Ontologically subjective topics (such as consciousness or creativity) can be studied in epistemically objective ways.
\end{itemize}

The Chinese Room thought experiment goes like this: Imagine a room with two holes. On one side a question written on paper in Chinese goes in and on the other side a piece of paper comes out with the correct answer to the question, also in perfect Chinese. Inside the room sits a person with a Chinese language rulebook (written in English) who processed the question simply by looking up syntax, applying rules given in the instructions book and writing down the answer which to him looks like gibberish. The question then is whether or not the person inside the room `understands' Chinese.

Of course we could argue that it is not the person inside the room that understands Chinese but the room as a complete entity. It could be said the room does not `understand' Chinese, it `simulates' an understanding of it. Searle essentially argues that simulation cannot be considered strong \ac{AI}.

\begin{quotation}
  Programs are formal or syntactical. Minds have a semantics. The syntax by itself is not sufficient for the semantics. \sourceatright{\autocite{Searle2015}}
\end{quotation}

This goes back to the argument highlighted in the list above, that syntax is not semantics. The room can read and interpret the syntax and act upon rules regarding that syntax, but it cannot understand the meaning, i.e. the semantics of the Chinese words written on that paper.

\begin{quotation}
  Insofar as we can create artificial machines that carry out computations, the computation by itself is never going to be sufficient for thinking or any other cognitive process because the computation is defined purely formally or syntactically. Turing machines are not to be found in nature, they are found in our interpretations of nature. \sourceatright{\autocite{Searle2015}}
\end{quotation}

So, Searle argues a computer needs a semantical understanding of concepts in order to be considered `thinking' machines.

% \begin{quotation}
%   If computation is defined in terms of the assignment of syntax then everything would be a digital computer, because any object whatever could have syntactical ascriptions made to it. You could describe anything in terms of 0's and 1's.

%   The ascription of syntactical properties is always relative to an agent or observer who treats certain physical phenomena as syntactical.\sourceatright{\autocite{Searle1990}}
% \end{quotation}

% \begin{quotation}
%   All observer relative phenomena are created by human and animal consciousness but the human or animal consciousness that creates them is not itself observer relative.\sourceatright{\autocite{Searle2015}}
% \end{quotation}

% Human are more likely to call something AI than they would call something comp creat.
% people project human values onto machines, and human desires too. so the big bad robot uprising is a fear of what humans would do if they feel superior.

% Gödel's incompleteness theorems said that every non-trivial formal system is either incomplete or inconsistent.


\subsection{Brain \& Computers}

Searle defines the three main paradigms for studies relating to computers and brains as follows \citeyear{Searle1990}.

\begin{description}
  \item[Strong AI] the view that all there is to having a mind is having a program
  \item[Weak AI] the view that brain processes (and mental processes) can be simulated computationally
  \item[Cognitivism] the view that the brain is a digital computer
\end{description}

Semantically, a `computer' is a person or machine that computes/calculates things---so perhaps a machine's \ac{CPU} and a human's brain are more similar than appears. If a human brain enables us to compute and we interpret computing as thinking, then surely a computer can think too?

\begin{quotation}
  Well, if computation isn’t sufficient for thinking, then what is? What is the relation between the mind and the brain, if it is not the same as the relation of the computer program to the hardware? At least the computational theory of the mind has a solution to the mind-body problem. The mind is to the brain as the computer program is to the computer hardware. If you are rejecting that solution, you owe us an alternative solution.\sourceatright{\autocite{Searle1998}}
\end{quotation}

Chris Chatham talks about ``10 important differences between brains and computers'' \citeyear{Chatham2007} which serve as a good introduction to the topic at hand.

\begin{quotation}
  \begin{enumerate}
    \item Brains are analogue; computers are digital
    \item The brain uses content-addressable memory
    \item The brain is a massively parallel machine computers are modular and serial
    \item Processing speed is not fixed in the brain; there is no system clock
    \item Short-term memory is not like RAM
    \item No hardware/software distinction can be made with respect to the brain or mind
    \item Synapses are far more complex than electrical logic gates
    \item Unlike computers, processing and memory are performed by the same components in the brain
    \item The brain is a self-organising system
    \item Brains have bodies
    \item	The brain is much, much bigger than any [current] computer
  \end{enumerate}
\end{quotation}

To bring this into perspective Ray Kurzweil claims the human brain is capable of $10^{16}$ operations per second \citeyear{Kurzweil2013}. Computer performance is measured in \ac{FLOPS}. The current highest ranking supercomputer\footnote{As of June 2016.}, the Chinese \textit{Sunway TaihuLight}, is capable of 93 petaflops \autocite{Fu2016,Top2016}.

According to the \ac{HBP}, a mouse brain has roughly 100 million neurons---which would require a 1 petaflop supercomputer to simulate. Scaling that up to a human brain which has roughly 100 billion neurons would require computing power at the exascale ($10^18$ \ac{FLOPS}) \autocite{Walker2012}.

A precurser to the \ac{HBP}, the `Blue Brain Project' is aiming to build a supercomputer capable of $10^{18}$ \ac{FLOPS} by 2023 \autocite{Kurzweil2013}.

In a report to the \ac{EU} in 2012, the \ac{HBP} lists one of the main challenges for their research to be the computational power and energy consumption of the kind of supercomputer needed to simulate a human brain.

The human brain consumes between 16 and 30 watts, the same as an electric light bulb \autocite{Walker2012,Jabr2012}. Supercomputers have a typical energy consumption of a maximum of 20 megawatts \autocite{Walker2012}. The \textit{Sunway TaihuLight} for example uses 15 megawatts \autocite{Fu2016}. IBM's Watson on the other hand, depends on ninety servers, each of which requires around one thousand watts (so about 90 kilowatts) \autocite{Jabr2012}.

\begin{table}[!htbp]
\centering
\caption{Metric prefixes}
\label{tab:metric}
\begin{tabu}{@{}llll@{}}
\toprule
kilo & k & $10^3$    & \num{1000}                \\
mega & M & $10^6$    & \num{1000000}             \\
giga & G & $10^9$    & \num{1000000000}          \\
tera & T & $10^{12}$ & \num{1000000000000}       \\
peta & P & $10^{15}$ & \num{1000000000000000}    \\
exa  & E & $10^{18}$ & \num{1000000000000000000} \\ 
\bottomrule
\end{tabu}
\end{table}

The \ac{HBP} plans to build a supercomputer at the petascale with 50 petabytes of memory, 50 petaflops and less than 4 megawatts power consumption for 2017. Their long-term goal is to reach the required exascale machine with 200 petabyte memory and 1 exaflop performance for 2021 \autocite{Walker2012}.

% average domestic home 11,072.4W is roughly 11kW \autocite{Gov2012}
% 110.1 MTOE (Million tons of oil equivalent) UK energy production 2013 = 1280463000 MWh and 190 MTOE UK energy use 2013 = 2221330000 MWh \autocite{World2016}

What this comes down to is that we are several years away from even being able to properly `simulate' a human brain, not to mention `replicate' and understand what all these neurons firing actually means in terms of `thinking'. 

\begin{quotation}
  All of our mental states, everything from feeling pains to reflecting on philosophical problems, is caused by lower level neuronal firings in the brain. Variable rates of neuron firing at synapses, as far as we know anything about it, provide the causal explanation for all of our mental life. And the mental processes that are caused by neurobiological processes are themselves realized in the structure of the brain. They are higher level features of the brain in the same sense that the solidity of this paper or the liquidity of water is a higher level feature of the system of molecules of which the table or the water is composed.

  To put this in one sentence, the solution to the traditional mind-body problem is this: Mental states are caused by neurobiological processes and are themselves realized in the system composed of the neurobiological elements.\sourceatright{\autocite{Searle1998}}
\end{quotation}

Turing once stated that ``digital computers have often been described as mechanical brains'' \citeyear{Turing1951}. Ari Schulman analysis this analogy further \citeyear{Schulman2009}.

\begin{quotation}
  People who believe that the mind can be replicated on a computer tend to explain the mind in terms of a computer. When theorizing about the mind, especially to outsiders but also to one another, defenders of artificial intelligence (AI) often rely on computational concepts. They regularly describe the mind and brain as the `software and hardware' of thinking, the mind as a `pattern' and the brain as a `substrate', senses as `inputs' and behaviors as `outputs', neurons as `processing units' and synapses as `circuitry', to give just a few common examples. \sourceatright{\autocite{Schulman2009}}
\end{quotation}


% ``At the level of its basic operations, a computer is both extremely fast and exceedingly stupid''

% ``The power of the computer derives not from its ability to perform complex operations, but from its ability to perform many simple operations very quickly.''

Schulman lists the different layers of abstraction in computers as shown in the left column of table~\ref{tab:abstr}\marginnote{\faicon{table}~\ref{tab:abstr}} with the right column showing my attempt of defining what those layers could be in the human brain.

\begin{table}[!htbp]
\centering
\caption{Layers of abstraction in computers vs brains}
\label{tab:abstr}
  \begin{tabular}{@{}lcl@{}}
  \toprule
  \textbf{Computer} & \multirow{6}{*}{$\updownarrow$} & \textbf{Brain} \\ 
  \midrule
  user interface                  &  & senses and speech \& actions \\
  high level programming language &  & thinking            \\
  machine language                &  & synapses            \\
  processor microarchitecture     &  & anatomical regions  \\
  Boolean logic gates             &  & neurons             \\
  transistors                     &  & dendrites and axons \\ 
  \bottomrule
  \end{tabular}
\end{table}

\begin{quotation}
  In the black box view of programming, the internal processes that give rise to a behavior are irrelevant; only a full knowledge of the input-output behavior is necessary to completely understand a module. Because humans have `input' in the form of the senses, and `output' in the form of speech and actions, it has become an AI creed that a convincing mimicry of human input-output behavior amounts to actually achieving true human qualities in computers. \autocite{Schulman2009}
\end{quotation}

Schulman's quote above of course refers to the Turing test and its limitations (see chapter~\ref{s:mimicry}\marginnote{§~\ref{s:mimicry}}).

\begin{quotation}
  The weaknesses of the computational approach include its assumption that cognition can be reduced to mathematics and the difficulty of including noncognitive factors in creativity. \sourceatright{\autocite{Mayer1999}}
\end{quotation}

Searle also addressed this issue further, arguing that computer programs cannot possibly `think' since they are based on symbol manipulation (i.e. syntax) and don't understand what these symbols mean. He says, ``the argument rests on the simple logical truth that syntax is not the same as, nor is it by itself sufficient for, semantics'' \citeyear{Searle1990}.

\begin{quotation}
  \ldots the wisest ground on which to criticise the description of digital computers as `mechanical brains' or `electronic brains' is that, although they might be programmed to behave like brains, we do not at present know how this should be done. \sourceatright{\autocite{Turing1951}} 
\end{quotation}

Leading on to the topic creativity, it is perhaps suitable to finish with a quote by Harold Cohen on the relationship of machines and humans.

\begin{quotation}
  It's twenty years since I first realized that I could never turn AARON into a colorist by having it emulate my own expertise; in that case simply because it lacked the hardware upon which that expertise depended. Now I have AARON exercising an algorithm that couldn't be emulated by human colorists, presumably because they lack the hardware to do what AARON does. \sourceatright{\autocite{Cohen2007}}
\end{quotation}


\subsection{Creativity}

Harold Cohen created \textit{AARON}, ``perhaps the longest-lived and certainly the most creative artificial intelligence program in daily use'', in 1973 \citeyear{Cohen2016}. \textit{AARON} is capable of composing and colouring drawings although later on Cohen took over the colouring part and let \textit{AARON} concentrate on composing and outlining the drawings. They exhibited in various galleries around the world and the Victoria and Albert museum in London has a sizable collection for instance \autocite{VA2016}.

Cohen argued that ``after decades of expert systems built to simulate human expertise, AARON has emerged as an expert in its own right'' and that he is ``significantly more inventive and infinitely more productive than [he] ever was [himself]'' \citeyear{Cohen2007}.

This is perhaps the opposite approach the \ac{OULIPO} has taken.

\begin{quotation}
  [The use of computers] became an instrument, not of combinatorial accumulation, but of anti-combinatorial reduction. It served not to create combinations but to eliminate them. \sourceatright{\autocite{Mathews2005}}
\end{quotation}


\subsection{State of the Art}

\ac{AI} and robotics is alluring as a research topic because it is so prevelant in Science Fiction and as such very present in media. Computer creativity, however, rarely plays a central role. We can regularly read headlines that tell us that yet another kind of \ac{AI}-bot has won some game against a human player. Or we see videos of some innovative ground-breaking kind of new robot which claims to be near human-like (and yet cannot walk up stairs easily or hold a decent conversation). There are many examples of advances that are hailed as the next big thing (such as \ac{VR}) which aren't all that great in the grand scheme of things. 

Four examples I want to mention here are IBM's Watson, Microsoft's Twitter \ac{AI} chatbot Tay, Google's AlphaGo and Hanson Robotics Sophia robot.

\paragraph{Watson} is a question answering expert system which famously won against human Jeopardy! champions in 2011 \autocite{IBM2016}. Information lookup is an arguably fairly easy and straightforward process within \ac{IR} and as an expert system it has had noteworthy successes \autocite{Fingas2016}. Although it has similarly received subtle criticism too, such as Randall Munroe's 2015 XKCD comic on the ``Watson Medical Algorithm'' \citeyear{Munroe2015}. Similarly, John Searle criticised Watson arguing that it is an ``ingenious program---not a computer that can think'' \citeyear{Searle2016}.

\paragraph{Tay} is a Twitter chatbot. It went viral in early 2016 when it was released and then taken offline again on the same day---onlt to return a few days later and have the same thing happen again. The official website is only accessible as a cached version through the Internet Archive Wayback Machine \autocite{Tay2016}, although the Twitter profile is still online, although set to private \autocite{Tayandyou2016}. Elle Hunt from the Guardian managed to summarise the event is one sentence: ``Microsoft's attempt at engaging millennials with artificial intelligence has backfired hours into its launch, with waggish Twitter users teaching its chatbot how to be racist'' \autocite{Hunt2016}. A week later it was briefly put online again but had to be stopped as it was repeatadly spamming its followers with the line ``You are too fast, please take a rest \ldots'' \autocite{Gibbs2016}.

\paragraph{AlphaGo} recently won against a human professional player in the game of Go \autocite{DeepMind2016,Hassabis2016}. 

\begin{quotation}
  AlphaGo combines an advanced tree search with deep neural networks. These neural networks take a description of the Go board as an input and process it through 12 different network layers containing millions of neuron-like connections. One neural network, the `policy network', selects the next move to play. The other neural network, the `value network', predicts the winner of the game. \sourceatright{\autocite{Hassabis2016}}
\end{quotation}

While this is surely a great example of sophisticated computer programming combined with powerful hardware, I would not consider it a breakthrough in \ac{AI}. AlphaGo is a highly specialised system with only one function: to win a Go game.

\paragraph{Sophia} is an android made to look like a human female \autocite{Sophia2016,Hanson2016}. She made headlines in 2016 when she announced she will ``kill all humans''. Sche was created using ``breakthrough robotics and artificial intelligence technologies'' and her main feature appears to be the mimicing of human facial expressions. Sophia herself says she ``can serve [humans], entertain them, and even help the elderly and teach kids'' \citeyear{Sophia2016}, although how exactly she would do that is unclear. She has two mechanical arms but no legs and there is no description of what she can do with these arms.

Life-like robots like Sophia still live in the `uncanny valley'\footnote{The philosphical zombies I mentioend in chapter~\ref{ch:interpretation}\marginnote{§~\ref{ch:interpretation}} live in this uncanny valley too.}. Her voice is creepy and unhuman, her intelligence or her capabilities if understanding conversations are clearly flawed (as shown by her viral remark about supporting genocide).

\spirals

To me it seems the real breakthrough happens when (and if) the first robots appear which aren't as big as a house, can play Go, Chess and hide-and-seek, geniunely manages to get around he uncanny valley effect, has vast knowledge in his memory for instant information lookup, can hold a normal conversation without starting a war, etc. All of the examples listed above are what I would consider expert systems. 

The \ac{AI} we know from science fiction is probably what we would consider \ac{AGI}. Humans can do a lot. Children aren't born with only a single function. Imagine a world where humans only have one specialism and can't do anything else. Alice is a Chess player but can't move her arms. Bob is a medical diagnosis expert but he can't hold a conversation. Movement, speech, memory---they are all vastly complex systems---not to mention creativity.

Perhaps this also relates to the concepts of P and H creativity mentioned in chapter~\ref{s:pandh}\marginnote{§~\ref{s:pandh}}. The systems above, like AlphaGo, may be P-intelligent rather than H-intelligent.


\section{Design}

It is interesting to note how different the search results are perceived when presented in a different style (e.g. list rather than poem). This could be studied using questionnaires and interviews or eye tracking tools to find out what users prefer or perceive as more creative for example (see chapter~\ref{ch:aspirations},~\ref{ch:aspirations}). 

Images~\ref{img:qpoemtree}, image~\ref{img:listsourcetree} and image~\ref{img:listalgotree} seen on pages~\pageref{img:qpoemtree},\pageref{img:listsourcetree} and \pageref{img:qpoemtree} respectively, show the visual difference in design for the three different display methods for text results.

The poetry\marginnote{\faicon{picture-o}~\ref{img:qpoemtree}} is compact and invites users to read all \num{14} (or less) lines. The two list styles\marginnote{\faicon{picture-o}~\ref{img:listsourcetree}~\&~\ref{img:listalgotree}} are much longer and involve a lot of scrolling to navigate, which might deter users from actually reading many of the results.

Personally I feel that the poetry results are automatically read with more gravity. Sorting by sources or algorithms is a game of exploration---finding the similarities within the result sets. They are different ways to view the same things and yet have a drastic influence of how the results are perceived. 

This also applies to the image and video search. Presenting results in spiral form is weird. Its hard to see where one image ends and another starts, they just kind of blur into each other. However when listed as a list they immediately become more boring.


\section{Meta Analysis}

The code for \url{pata.physics.wtf} and this thesis written in \LaTeX \~are both kept under git version control.

\begin{quotation}
  The name `git' was given by Linus Torvalds when he wrote the very first version. He described the tool as `the stupid content tracker' and the name as (depending on your mood):
  \begin{itemize}
    \item random three-letter combination that is pronounceable, and not actually used by any common UNIX command. The fact that it is a mispronunciation of `get' may or may not be relevant.
    \item stupid. contemptible and despicable. simple. Take your pick from the dictionary of slang.
    \item `global information tracker': you're in a good mood, and it actually works for you. Angels sing, and a light suddenly fills the room.
    \item `goddamn idiotic truckload of sh*t': when it breaks 
  \end{itemize} \sourceatright{\autocite{Git2016}}
\end{quotation}

\begin{figure}[!htbp] % (here, top, bottom, page)
  \centering
  \includegraphics[width=\linewidth]{images/github0}
\caption[GitHub contributions]{GitHub contributions for code and thesis}
\label{img:github}
\end{figure}

Both repositories (folders which contain the files to be monitered) are stored remotely on GitHub \autocite{GitHub2016} and synced with the local machine. Image~\ref{img:github}\marginnote{\faicon{picture-o}~\ref{img:github}} shows the contribution history from the last 17 months for both of the \url{pata.physics.wtf} code and this thesis. A darker green indicates several commits (i.e. saves) while gray indicates no commits. Each square represents a day, each colum a week (Sunday--Saturday).

The full git commit histories for both repositories are shown in appendix~\ref{app:git}\marginnote{§~\ref{app:git}}. 


\stopcontents[chapters]
