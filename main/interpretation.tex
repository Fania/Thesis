% !TEX root = ../main.tex

\chapter{Interpretation}
\label{ch:interpretation}

\startcontents[chapters]

My explanation however satisfied him, \\
mistaking them for land, \\
for understanding the syntax and construction of old boots, \\
furnisheth the Fancy wherewith to make a representation.

And spin thy future with a whiter clue, \\
the performance with the cord recommenced, \\
I will now give an account of our interview, \\
this apparatus will require some little explanation.

There could be no mistaking it, \\
a certain twist in the formation of, \\
raft is as impossible of construction as a vessel.

Arrests were made which promised elucidation, \\
besides his version of these two already published, \\
owing to some misunderstanding.

\minicontents


\section{Problems with Evaluation}

Evaluating human creativity objectively seems problematic; evaluating computer creativity at all seems even harder. There are many debates across the disciplines involved. Taking theories on human creativity\marginnote{§~\ref{ch:creativity}} and directly applying them to machines seems logical but may be the wrong approach. Adapting Mayer's five big questions\marginnote{§~\ref{s:Mayer5questions}} to machines does not seem to capture the real issues at play\marginnote{\faicon{table}~\ref{tab:HCCC}}. Instead of asking if creativity is a property of people, products, or processes we might ask if it is a property of programmers, users, machines, products, or processes. Similarly we might ask if creativity is a local, a network or an Internet phenomenon.

\begin{table}[htbp]
  \centering
  \begin{tabu}{XX}
  \toprule
  \textbf{Human Creativity} & \textbf{Computer Creativity} \\
  \midrule
  people, products, or processes & programmers, users, machines, products, or processes \\
  personal or social & local, network or Internet \\
  common or rare & common or rare \\
  domain-general or domain-specific & domain-general or domain-specific \\
  quantitative or qualitative & quantitative or qualitative \\
  \bottomrule
  \end{tabu}
\caption[Human Creativity vs Computer Creativity]{Human Creativity vs Computer Creativity}
\label{tab:HCCC}
\end{table}

\todo{this table is unnecessary really}

Current evaluation methodologies in creative computing disciplines have concentrated on only a handful of the points raised in chapter~\ref{ch:evaluation}\marginnote{§~\ref{ch:evaluation}}, for example studying only the creative end-product itself (out of context), only judging it by its novelty objectively, assigning an arbitrary thresholds, etc. This also includes the assumption that machines `mimic' humans and are therefore not judged at their full potential\footnote{Human Brain --- Computer Brain stuff here?}.

\todo{add human brain stuff?}

\spirals

Table~\ref{tab:HCCC} also brings up several questions.
\todo{revise questions here}

\begin{itemize}
  \item Can a machine judge whether a human is creative?
  \item Is creativity a property of machines (hardware or software?)
  \item Is mimicking human creativity really enough and appropriate?
  \item Compare against ``human creativity''? Or define machine creativity from scratch?
  \item Who is creative? The programmer or the program?
  \item Can creativity be objectively measured?
  \item Quantitative or qualitative?
  \item In respect to P or H creativity?
  \item Output minus input? (we don’t have the same strict judgement on humans)
  \item Is it the product or the process or both?
  \item Does context matter? (Blind deaf dumb person = computer?)
  \item Does time matter?
  \item Does purpose or intention matter?
  \item AGI vs AI? Artificial general creativity vs artificial creativity?
  \item What is the definition of creativity?
  \item Who is being creative? WHO
  \item What was the aim/intention, if any?
  \item What was the process, approach? HOW
  \item What factors influenced the person/process? WHERE
  \item What is the result/product, if any? Is it original, relevant? WHAT
  \item What is the impact, if any?
  \item What is the maintenance plan, if any?
\end{itemize}

\todo{where are the last few items from??}

\spirals

Anna Jordanous found that ``evaluation of computational creativity is not being performed in a systematic or standard way'' \autocite[p.2]{Jordanous2011}, which further confuses the problem of objective evaluation. To remedy this she proposes ``\gls{specs}''\marginnote{§~\ref{s:specs}} (see chapter~\ref{ch:evaluation} for more details) \autocite[p.137-140]{Jordanous2012a}:

\begin{quote}
  \begin{enumerate}
    \item Identify a definition of creativity that your system should satisfy to be considered creative.
    \item Using Step 1, clearly state what standards you use to evaluate the creativity of your system.
    \item Test your creative system against the standards stated in Step 2 and report the results.
  \end{enumerate}
\end{quote}

The \gls{specs} model essentially means that we cannot evaluate a creative computer system objectively, unless steps 1 and 2 are predefined and publically available for external assessors to execute step 3. Creative evaluation can therefore be seen as a move from subjectivity to objectivity, i.e. defining subjective criteria for objectively evaluating a product in terms of the initial criteria.

\begin{quote}
  ``For transparent and repeatable evaluative practice, it is necessary to state clearly what standards are used for evaluation, both for appropriate evaluation of a single system and for comparison of multiple systems using common criteria.'' \autocite[p.67]{Jordanous2012a}
\end{quote}

We need a ``clearer definition of creativity'' \autocite[p.459]{Mayer1999}, with ``criteria and measures [for evaluation] that are situated and domain specific.'' \autocite[p.7]{Candy2012}

\begin{quote}
  ``[A] person's creativity can only be assessed indirectly (for example with self report questionnaires or official external recognition) but it cannot be measured.'' \autocite[p.258]{Piffer2012}
\end{quote}

Since many problems with evaluating creativity in computers (and humans alike) seem to stem from a lack of a clear relevant definition it seems logical to try and remedy this first and foremost.


\subsection{Defining Creativity}

Following Jordanous' \gls{specs} model\marginnote{§~\ref{s:specs}}, we need to state our own definition of creativity in regards to the computer system being evaluated.

Summarising section~\ref{s:creattributes}\marginnote{§~\ref{s:creattributes}}, chapter~\ref{ch:evaluation} and chapter~\ref{ch:creativity} we get the following list of terms used to describe or define creativity.

\begin{description}
  \item [Margaret Boden] new, surprising, valuable \autocite{Boden2003}
  \item [Davide Pfiffer] novelty, appropriatness, impact \autocite{Piffer2012}
  \item [Pease et al] novel, valuable \autocite{Pease2001} and randomness, serendipity \autocite{Pease2013}
  \item [Graeme Ritchie] novelty, quality, typicality \autocite{Ritchie2007}
  \item [Dan Ventura] variety, efficiency \autocite{Ventura2008}
  \item [Simon Colton] skill, appreciation, imagination \autocite{Colton2008a, Colton2008}
  \item [Geraint Wiggins] relevance, acceptability, quality \autocite{Wiggins2006}
  \item [Anna Jordanous] active involvement and persistence, generation of results, uncertainty, domain competence, general intellect, independence and freedom, intention and emotional involvement, originality, progression and development, social interaction and communication, spontaneity / subconscious processing, thinking and evaluation, value, variety, divergence and experimentation \autocite{Jordanous2012}
\end{description}
\todo{complete summary}

From this we can derive the following 7 key attributes of creativity.

\begin{description}
  \item [Novelty] originality, newness, variety, typicality, imagination
  \item [Value] usefulness, appropriateness, appreciation, relevance, impact, influence
  \item [Quality] skill, efficiency, competence, intellect, acceptability
  \item [Ephemeral/Uncontrolled] serendipity, randomness, uncertainty, experimentation
  \item [Temporal/Controlled] persistence, results, development, progression, spontaneity
  \item [Purpose] intention, communication, evaluation, aim, independence
  \item [Spatial] context, environment, press
\end{description}




\section{5 P’s: product, process, people, place and purpose}


\begin{figure}[htb] % (here, top, bottom, page)
  \centering
  \tikzset{every fit/.append style=text badly centered}
  \tikzset{class/.style={draw,rectangle},
           label/.style={align=center,inner ysep=2pt,outer ysep=2pt,node distance=4pt}}
  \begin{tikzpicture}
  \node [class] (prod) {Product};
  \node [label, below=of prod] (proc) {Process};
  \node [label, below=of proc] (pers) {Person};
  \node [label, below=of pers] (env) {Environment};
  \begin{pgfonlayer}{background}
  \node [class, inner xsep=1em, fit=(prod) (proc)] {};
  \node [class, inner xsep=2em, fit=(prod) (proc) (pers)] {};
  \node [class, inner xsep=2em, fit=(prod) (proc) (pers) (env)] {};
  \end{pgfonlayer}
  \end{tikzpicture}
\caption[4 Aspects of Creativity]{4 Aspects of Creativity}
\label{fig:4CreaI}
\end{figure}

\begin{draft}
  Figure~\ref{fig:4CreaI} shows how these aspects relate to each other. The environment influences all others and the person creates the product in a process.
\end{draft}


\begin{quote}
  One way of characterizing these processes is to use an alliteration that allows us to keep track of some of the core features of RRI in ICT, namely the four P's, which are: product, process, purpose and people. The purpose of using the four P's is to draw attention to the fact that, in addition to the widely recog- nized importance of both product and process of technical development, the purpose of the development needs to be considered and people involved in the innovation need to be incorporated in RRI.\@ \autocite[p.203, my emphasis]{Stahl2013}
\end{quote}

\begin{fcom}
  combine the 4 P’s with purpose//
  5 P’s: product, process, people, place and purpose//
  Why is the purpose important?//
  Interpreting or Measuring?//
  Maybe we should not be looking for metrics but rather guidelines for interpretations of creativity.
\end{fcom}

\begin{draft}
  In the end I believe it is impossible to measure creativity objectively. I don’t just think it is impossible, I think it is unwise to try and do so. It would be silly to put a percentage on how creative something is just like it would be silly to say a certain product is 50percent ethical. In fact there are lots of parallels between (computer) ethics and (computer) creativity. Both are subjective, both are highly dependent on context.

  What is important is to study and consider the factors that influence our perception of whether something is creative (or ethical) and what the implications are.

  Creativity in a process or product will mean different things to different people, in different environments and contexts.
  Common sense.

  Just as there are people who just cannot see any creativity in in modern art, there will always be people who wont accept anything produced by a computer as creative.
\end{draft}% chktex 17



We would need to investigate each individual search result in terms of its value and creativity. This could be done by user ratings or satisfaction questionnaires. Rather than measuring the success of individual results we could look at evaluation them as one set instead, similar to the blind side-by-side comparisons by Bing or MillionShort.

The search results produced by our tool can be quite surprising sometimes and it not always clear how they connect to the initial query (especially if the inner workings of the algorithms are unknown), even if we identify through which function a result has been obtained. Obviously these keywords might not be helpful to users unfamiliar with the concept of pataphysics and might therefore appear rather nonsensical. Whilst there is a clear logic to each search result, they might appear anomalous to the user’s expectations if he received these results without knowing the philosophy of the search tool. The results could possibly appear random then, and would therefore likely to be detrimental to the user.

To prevent that, the level of interaction the user has with the system and the feedback the system gives to the user on decisions it is making will have a large influence on the overall effectiveness and appreciation of the tool. A quick and simple solution to this problem would be to add an icon to the side of each search result, which displays exactly how the original query was pataphysicalised.


\section{Creative Search Evaluation}

\todo{reformat, add references etc}

\begin{leftbar}
The word useless is defined as ``not fulfilling or not expected to achieve the intended purpose or desired outcome'' in the Oxford dictionary (2010). Given this definition most of the search results we have in mind would be classed as useless. That is at least if we considered every result individually, outside of context and with an information-lookup query in mind. If we have an exploratory search in mind however things get more interesting and we will explain why in the remainder of the paper.
\end{leftbar}

Relevant versus Creative

\begin{leftbar}
When we say relevant results we mean the kind of search results that any mainstream search engine would produce, the kind of results you would immediately understand their connection to the query for, the kind of results that just makes sense. Consider the example results in table 1.
\end{leftbar}

\begin{leftbar}
Results which might seem useless at first can be much more creative or even poetic. And creative results support exploratory search. Surprise and user expectations play a big role in creativity according to Boden (2003).
\end{leftbar}

\begin{leftbar}
Fewer expectations (an open mind) allow creativity to happen more easily. Empirical experiences form expectations, which hinder our ability to accept creative ideas when they happen. In order to be able to recognise creative ideas we need to be able to see what they all have in common and in what way they differ and not reject unusual, unexpected ones.
\end{leftbar}

\begin{leftbar}
We can link this very nicely to the idea of exploratory search. Lowering expectations or opening the mind implies extending the task domain or problem space. Creativity and exploratory search seem predestined to work with each other.
\end{leftbar}

Biases

\begin{leftbar}
Wikipedia defines bias as ``an inclination to present or hold a partial perspective at the expense of (possibly equally valid) alternatives. Anything biased generally is one-sided, and therefore lacks a neutral point of view.'' (2012)
\end{leftbar}

\begin{leftbar}
However, biases can be good and bad. It is important to consider the implications of their existence though, especially when trying to measure the success of something objectively. An example of when biases can be advantageous is location signals that the search tool takes into account when producing results. An Englishmen would probably not have much use of a Chinese website and vice-versa, even if the actual content matches the original query (unless of course the user happens to understand both languages perfectly). Another example of this is location queries such as `Chinese restaurants in Cambridge', which should return web pages about restaurants based in Cambridge, UK or Massachusetts, USA, depending on the user’s I.P. address.  This might seem logical, but in the truest sense it is a bias employed by the search engine to help provide more relevant results to individuals. Truly unbiased search results are probably impossible to come by nowadays.
\end{leftbar}

\begin{leftbar}
There is a general move from objectivity to subjectivity in the sense that users become the subject of search results as much as the query they pose. Instead of neutrally providing results for a query alone, the results are tailored around the information known about the user (e.g.\ language, location, clickstream, social media likes, bookmarks, etc.) to make up the missing context. The user becomes the subject and context of a query, while the results become an objective list of matches for all those values rather than just the query term (s).
\end{leftbar}

Standard Web search:  Subject/User  Object/Results

Constraints

\begin{leftbar}
There are certain factors and constraints that influence the perception and success of the results. Some can be taken into account when building a search system but others cannot be avoided. User education is one way to deal with those issues. Earlier we briefly mentioned some external constraints such as the setting in which the search takes place. Is the user operating from a handheld device or a desktop computer? Is he or she in a hurry to find answers or just leisurely browsing for them? Is the search system web-based or is the user querying a database?
\end{leftbar}

\begin{leftbar}
User Expectations  It is important to note that ``search systems are not used in isolation from their surrounding context, i.e., they are used by real people who are influenced by environmental and situational constraints such as their current task'' (White and Marchionini 2004). User expectations should be taken into consideration during the evaluation of search results. Users who are hoping to find precise answers to a specific question might not be satisfied by exploratory search results. Someone browsing for inspiration on a broad topic on the other hand could benefit from them. Users should therefore be informed about the nature of the search tool in some way.
\end{leftbar}

\begin{leftbar}
User Skill   The searching skills of the user matter. Specifically his or her ability to articulate an information need and any knowledge of special search techniques (use of Boolean modifiers, quotation marks, wildcards, etc.) are two important factors that influence the results obtained greatly. This is very much based on the old idea of garbage-in, garbage-out (Lidwell et al. 2010).
Visual Representation   The way that results are presented affects how the user perceives them. A diversity of different document types, for example text, images, sound, or video results could improve how well the results are rated (Sawle et al. 2011). Johanna Drucker had already pointed out that ``many information structures have graphical analogies and can be understood as diagrams that organise the relations of elements within the whole'' (2009). An alphabetical list is a typical model for representing text data sets for example. But is a ranked list really the best way to represent search results? Other models could be a differently ranked or ordered list, a tree structure, a matrix, a one-to-many relationship, etc.
\end{leftbar}

\begin{leftbar}
Structure of Results  As suggested by Sawle et al (2011) we need to consider different ways to structure and measure search results. A single, perfectly good result might be deemed irrelevant and useless if it is surrounded by several unsuitable results. Therefore there might be certain advantages to measuring and evaluating the value or relevance of individual results over a whole set of results.
\end{leftbar}

\begin{leftbar}
Direct User Relevance Feedback   Relevance feedback lets users rate individual results or sets of results either directly (through manual ratings) or indirectly (through click-stream data). This data is then congregated and used for webpage rankings or other purposes such as suggesting other query terms. It can improve results for similar queries in the future but also lets the user stir the direction his search is taking in real-time. Users can adjust their query to manipulate the results; this basically means they adjust some of their own constraints.
\end{leftbar}

\begin{quote}
``Relevance feedback—asking information seekers to make relevance judgments about returned objects and then executing a revised query based on those judgments—is a powerful way to improve retrieval.'' (Marchionini 2006)
\end{quote}

\begin{leftbar}
Automatic Query Expansion   As opposed to integrating and involving the user actively in the refinement of a query, in automatic query expansion the improvements are done passively, often completely without the user’s knowledge. Information gathering methods include, for example, the analysis of mouse clicks, so called like buttons (e.g. Facebook, Google+) or eye tracking, etc. How the collected data is then used varies. Simple examples of automatic query expansion are the correction of spelling errors or the hidden inclusion of synonyms when evaluating a query.
\end{leftbar}

\begin{leftbar}
Depending on these factors and constraints, search results can be viewed as useful or useless. In a way the usefulness or correctness of an idea or result cannot always be judged fairly – there are always conditions that will affect how the outcome is interpreted. In the scenario of a creative search tool, results could be very useful, while they might be completely useless in another.
\end{leftbar}


\stopcontents[chapters]
