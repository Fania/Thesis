<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Evaluation</title>
  <link href="../htmlstyle.css" rel="stylesheet">
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]},
											TeX:     {extensions: ["color.js"],
																Macros:     {RR:    '{\\bf R}',
												                     bold:  ['{\\bf #1}', 1]}
															 }
										 });
	</script>
</head>
<body>
<nav class="mainnav">
  <a class="left" href="technology.html">Prev</a>
  <a class="centre" href="contents.html">Contents</a>
  <a class="right" href="part3.html">Next</a>
</nav>

<section id="ch:evaluation" class="level1">
<h1><span class="header-section-number">7</span> Evaluation</h1>

<nav id="TOC">
<ul>
<li><a href="#ch:evaluation"><span class="toc-section-number">7</span> Evaluation</a><ul>
<li><a href="#s:evalsearch"><span class="toc-section-number">7.1</span> Evaluating Search</a></li>
<li><a href="#s:creattributes"><span class="toc-section-number">7.2</span> Evaluating Creative Computers</a><ul>
<li><a href="#s:s:creattributes"><span class="toc-section-number">7.2.1</span> Output minus Input</a></li>
<li><a href="#creative-tripod"><span class="toc-section-number">7.2.2</span> Creative Tripod</a></li>
<li><a href="#s:specs"><span class="toc-section-number">7.2.3</span> SPECS</a></li>
<li><a href="#s:mmce"><span class="toc-section-number">7.2.4</span> MMCE</a></li>
<li><a href="#s:csf"><span class="toc-section-number">7.2.5</span> CSF</a></li>
<li><a href="#s:indcrit"><span class="toc-section-number">7.2.6</span> Individual Criteria</a></li>
</ul></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>

<section class="poem">
<code>Score,</code><br />
<code>quel grade avais,</code><br />
<code>of my cooler judgment,</code><br />
<code>and inquires after the evacuations of the toad on the horizon.</code><br />
<br />
<code>His judgment takes the winding way Of question distant,</code><br />
<code>if not always with judgment,</code><br />
<code>and showed him every mark of honour,</code><br />
<code>three score years before.</code><br />
<br />
<code>Designates him as above the grade of the common sailor,</code><br />
<code>but I was of a superior grade,</code><br />
<code>travellers of those dreary regions marking the site of degraded Babylon.</code><br />
<br />
<code>Mark the Quilt on which you lie,</code><br />
<code>und da Sie grade kein weißes Papier bei sich hatten,</code><br />
<code>and to draw a judgement from Heaven upon you for the Injustice.</code>
</section>

<section id="s:evalsearch" class="level2">
<h2><span class="header-section-number">7.1</span> Evaluating Search</h2>
<p>Generally, computer systems are evaluated against functional requirements and performance specifications. Traditional Information Retrieval (IR) however is usually evaluated using two metrics known as precision and recall <span class="citation" data-cites="Baeza-Yates2011">(Baeza-Yates and Ribeiro-Neto <a href="#ref-Baeza-Yates2011">2011</a>)</span>. Precision is defined as the fraction of retrieved documents that are relevant, while recall is defined as the fraction of relevant documents that are retrieved.</p>

<figure id="eq:7.1" class="equation scrollbar">
$$Precision = \frac{\text{relevant documents retrieved}}{\text{retrieved documents}}
  \label{eq:precision}$$
<figcaption>(7.1)</figcaption>
</figure>
<figure id="eq:7.2" class="equation scrollbar">
$$Recall = \frac{\text{relevant documents retrieved}}{\text{relevant documents}}
\label{eq:recall}$$
<figcaption>(7.2)</figcaption>
</figure>

<p>Note the slight difference between the two. Precision tells us how many of all retrieved results were actually relevant (of course this should preferable be very high) and recall simply indicates how many of all possible relevant documents we managed to retrieve. This can be easily visualised as as shown in figure <a href="#fig:7.1">7.1</a>.</p>

<figure id="fig:7.1">
<img src="../images/PRside.png" alt="Precision and recall (Walber 2014)" />
<figcaption>Figure 7.1 - Precision and recall <span class="citation" data-cites="Wikimedia2014">(Walber <a href="#ref-Wikimedia2014">2014</a>)</span></figcaption>
</figure>

<p>Precision is typically more important than recall in web search, so often evaluation is reduced to measuring the Mean Average Precision (MAP) value, which can be calculated using the formula in equation <a href="#eq:7.3">7.3</a> <span class="citation" data-cites="Baeza-Yates2011">(Baeza-Yates and Ribeiro-Neto <a href="#ref-Baeza-Yates2011">2011</a>)</span>, where $R_i$ is the set of results for query $i$, $P(R_i[k])$ is the precision value for result $k$ for query $i$ and $|R_i|$ is the total number of results.</p>

<figure id="eq:7.3" class="equation scrollbar">
$$MAP_i = \frac{1}{|R_i|} \sum_{k=1}^{|R_i|} P(R_i[k])
\label{eq:MAP}$$
<figcaption>(7.3)</figcaption>
</figure>

<p>But for many web searches it is not necessary to calculate the average of all results, since users don’t inspect results after the first page very often and it is therefore desirable to have the highest level of precision in the first page of results maybe. For this purpose it is common to measure the average precision of web search engines after only a few documents have been seen. This is called ‘Precision at n’ or ‘P@n’ <span class="citation" data-cites="Baeza-Yates2011">(Baeza-Yates and Ribeiro-Neto <a href="#ref-Baeza-Yates2011">2011</a>)</span>. So for example this could be P@5, P@10, or P@20. To compare two ranking algorithms, we would calculate P@10 for each of them over an average of 100 queries maybe and compare the results and therefore the performance of the algorithm.</p>
<p>The Text REtrieval Conference (TREC) <span class="citation" data-cites="Nist2016">(Trec <a href="#ref-Nist2016">2016</a>)</span> provides large test sets of data <span class="citation" data-cites="Trec2011">(Trec <a href="#ref-Trec2011">2011</a>)</span> to participants and let’s them compare results. They have specific test sets for web search comprised of crawls of <span class="math inline">.<em>g</em><em>o</em><em>v</em></span> web pages.</p>
<p>There are certain other factors that can be or should be evaluated when looking at a complete search system, as shown below <span class="citation" data-cites="Baeza-Yates2011">(Baeza-Yates and Ribeiro-Neto <a href="#ref-Baeza-Yates2011">2011</a>)</span>.</p>
<ul>
<li><p>Speed of crawling.</p></li>
<li><p>Speed of indexing data.</p></li>
<li><p>Amount of storage needed for data.</p></li>
<li><p>Speed of query response.</p></li>
<li><p>Amount of queries per given time period.</p></li>
</ul>
<p>Ranking is another issue that could be considered to pre-evaluate web pages at indexing time rather than query time. This was previously discussed in chapter <a href="technology.html#s:ranking">6.1.3</a>.</p>

<img class="triplespiral" src="../images/triplespiral.png">

<p>Evaluating creative search is more complex, as the notion of ‘relevance’ is very different and this will be addressed in chapter <a href="interpretation.html">9</a>.</p>
<p>Sawle, Raczinski and Yang <span class="citation" data-cites="Sawle2011">(<a href="#ref-Sawle2011">2011</a>)</span> discussed an initial approach to measure the creativity of search results in 2011. Based on a definition of creativity by Boden (as explained in chapter <a href="creativity.html#s:boden">5.1.6</a>), we attempted to define creativity in a way which could be applied to search results and provide a simple metric to measure it. A copy of this paper can be found in appendix [app:pub].</p>
</section>

<section id="s:creattributes" class="level2">
<h2><span class="header-section-number">7.2</span> Evaluating Creative Computers</h2>
<p>This section moves on from evaluating search and focuses on evaluating creativity in computers.</p>
<blockquote>
<p>The evaluation of artificial creative systems in the direct form currently practiced is not in itself empirically well-grounded, hindering the potential for incremental development in the field.</p><span class="blockcitation" data-cites="Bown2014">(Bown <a href="#ref-Bown2014">2014</a>)</span>
</blockquote>
<p>Evaluating human creativity objectively seems problematic; evaluating computer creativity seems even harder. There are many debates across the disciplines involved. Taking theories on human creativity (see section <a href="creativity.html#s:humancreativity">5.1</a>) and directly applying them to machines (see section <a href="creativity.html#s:compcreativity">5.2</a>) seems logical but may be the wrong (anthropomorphic) approach. Adapting Mayer’s five big questions <span class="citation" data-cites="Mayer1999">(<a href="#ref-Mayer1999">1999</a>)</span> to machines does not seem to capture the real issues at play. Instead of asking if creativity is a property of people, products, or processes we might ask if it is a property of any or all of the following:</p>
<ul>
<li><p>programmers</p></li>
<li><p>users</p></li>
<li><p>machines<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p></li>
<li><p>products</p></li>
<li><p>processes</p></li>
</ul>
<p>For instance, is the programmer the only creative agent, or are users (i.e. audiences or participants in interactive work) able to modify the system with their own creative input? Similarly for any instance of machine creativity, we might ask if it is:</p>
<ul>
<li><p>local (e.g. limited to a single machine, program or agent)</p></li>
<li><p>networked (i.e. interacts with other predefined machines or programs)</p></li>
<li><p>web-based (e.g. is distributed and/or open to interactions, perhaps via an API)</p></li>
</ul>
<p>Norton, Heath and Ventura highlight the importance of dealing with ‘evaluator bias’ when using human judges for evaluating any form of creativity. They identified 5 main problems as follows <span class="citation" data-cites="Norton2015">(<a href="#ref-Norton2015">2015</a>)</span>.</p>
<dl class="constheight">
<dt>1<sup>st</sup> problem</dt>
<dd>Do we assess products or processes?</dd>
<dt>2<sup>nd</sup> problem</dt>
<dd>What are the measurable assessment criteria?</dd>
<dt>3<sup>rd</sup> problem</dt>
<dd>How do we un-ambiguate ambigous terminology?</dd>
<dt>4<sup>th</sup> problem</dt>
<dd>Which methodology to use for the assessment?</dd>
<dt>5<sup>th</sup> problem</dt>
<dd>How do we compensate for biases?</dd>
</dl>
<p>This point is also strengthend by Lamb, Brown and Clarke, saying that “non-expert judges are very poor at using metrics to evaluate creativity” and that the criteria they tested were not “objective enough to produce trustworthy judgments” <span class="citation" data-cites="Lamb2015">(<a href="#ref-Lamb2015">2015</a>)</span>.</p>

<section id="s:o-i" class="level3">
<h3><span class="header-section-number">7.2.1</span> Output minus Input</h3>
<p>Discussions from computational creativity often focus on very basic questions such as “whether an idea or artefact is valuable or not, and whether a system is acting creatively or not” <span class="citation" data-cites="Pease2011">(Pease and Colton <a href="#ref-Pease2011">2011</a>)</span>. Certain defining aspects of creativity, such as novelty and value (as discussed in chapter <a href="creativity.html">5</a>), are often used to measure the outcome of a creative process. These are highlighted throughout the following pages and further addressed in chapter <a href="interpretation.html">9</a>.</p>
<p>One recurring theme is the clear separation of training data input and creative output in computers. Pease, Winterstein and Colton called this principle “output minus input” <span class="citation" data-cites="Pease2001">(<a href="#ref-Pease2001">2001</a>)</span>. The output in this case is the creative product but the input is not the process. Rather, it is the ‘inspiring set’ (comprised of explicit knowledge such as a database of information and implicit knowledge input by a programmer) or training data of a piece of software.</p>
<blockquote>
<p>The degree of creativity in a program is partly determined by the number of novel items of value it produces. Therefore we are interested in the set of valuable items produced by the program which exclude those in the inspiring set.</p><span class="blockcitation" data-cites="Colton2001">(Colton, Pease, and Ritchie <a href="#ref-Colton2001">2001</a>)</span>
</blockquote>
<p>They also suggest that all creative products must be “novel and valuable” <span class="citation" data-cites="Pease2001">(Pease, Winterstein, and Colton <a href="#ref-Pease2001">2001</a>)</span> and provide several measures that take into consideration the context, complexity, archetype, surprise, perceived novelty, emotional response and aim of a product. In terms of the creative process itself they only discuss randomness as a measurable approach. Elsewhere, Pease et al discuss using serendipity as an approach <span class="citation" data-cites="Pease2013">(<a href="#ref-Pease2013">2013</a>)</span>.</p>
<p>Graeme Ritchie supports the view that creativity in a computer system must be measured “relative to its initial state of knowledge” <span class="citation" data-cites="Ritchie2007">(<a href="#ref-Ritchie2007">2007</a>)</span>. He identifies three main criteria for creativity as “novelty, quality and typicality” <span class="citation" data-cites="Ritchie2007">(<a href="#ref-Ritchie2007">2007</a>)</span>, although he argues that “novelty and typicality may well be related, since high novelty may raise questions about, or suggest a low value for, typicality” <span class="citation" data-cites="Ritchie2001 Ritchie2007">(<a href="#ref-Ritchie2001">2001</a>; <a href="#ref-Ritchie2007">2007</a>)</span>. He proposes several evaluation criteria which fall under the following categories <span class="citation" data-cites="Ritchie2007">(<a href="#ref-Ritchie2007">2007</a>)</span>: basic success, unrestrained quality, conventional skill, unconventional skill, avoiding replication and various combinations of those. Dan Ventura later suggested the addition of “variety and efficiency” to Ritchie’s model <span class="citation" data-cites="Ventura2008">(<a href="#ref-Ventura2008">2008</a>)</span>.</p>
<p>It should be noted that ‘output minus input’ might easily be misinterpreted as ‘product minus process’, however, that is not the case. In fact, Pease, Winterstein and Colton argue that “the process by which an item has been generated and evaluated is intuitively relevant to attributions of creativity” <span class="citation" data-cites="Pease2001">(<a href="#ref-Pease2001">2001</a>)</span>, and that “two kinds of evaluation are relevant; the evaluation of the item, and evaluation of the processes used to generate it” <span class="citation" data-cites="Pease2001">(<a href="#ref-Pease2001">2001</a>)</span>. If a machine simply copies an idea from its inspiring set then it just cannot be considered creative and needs to be disqualified so to speak.</p>
</section>

<section id="creative-tripod" class="level3">
<h3><span class="header-section-number">7.2.2</span> Creative Tripod</h3>
<p>Simon Colton came up with an evaluation framework called the <em>creative tripod</em>. The tripod consists of three behaviours a system or artefact should exhibit in order to be called creative. The three legs represent “skill, appreciation, and imagination” and three different entities can sit on it, namely the programmer, the computer and the consumer. Colton argues that the perception “that the software has been skillful, appreciative and imaginative, then, regardless of the behaviour of the consumer or programmer, the software should be considered creative” <span class="citation" data-cites="Colton2008 Colton2008a">(<a href="#ref-Colton2008">2008</a><a href="#ref-Colton2008">a</a>; <a href="#ref-Colton2008a">2008</a><a href="#ref-Colton2008a">b</a>)</span>. As such a product can be considered creative, if it appears to be creative. If not all three behaviours are exhibited, however, it should not be considered creative <span class="citation" data-cites="Colton2008 Colton2008a">(Colton <a href="#ref-Colton2008">2008</a><a href="#ref-Colton2008">a</a>; Colton <a href="#ref-Colton2008a">2008</a><a href="#ref-Colton2008a">b</a>)</span>.</p>
<blockquote>
<p>Imagine an artist missing one of skill, appreciation or imagination. Without skill, they would never produce anything. Without appreciation, they would produce things which looked awful. Without imagination, everything they produced would look the same.</p><span class="blockcitation" data-cites="Colton2008a">(Colton <a href="#ref-Colton2008a">2008</a>)</span>
</blockquote>

<img class="triplespiral" src="../images/triplespiral.png">

<p>Davide Piffer suggests that there are three dimensions of human creativity that can be measured, namely “novelty, usefulness/appropriateness and impact/influence” <span class="citation" data-cites="Piffer2012">(<a href="#ref-Piffer2012">2012</a>)</span>. As an example of how this applies to measuring a person’s creativity he proposes ‘citation counts’ <span class="citation" data-cites="Piffer2012">(Piffer <a href="#ref-Piffer2012">2012</a>)</span>. While this idea works well for measuring scientific creativity maybe, he does not explain how this would apply to a visual artist for example.</p>
</section>

<section id="s:specs" class="level3">
<h3><span class="header-section-number">7.2.3</span> SPECS</h3>
<p>Anna Jordanous proposed 14 key components of creativity (which she calls an “ontology of creativity”) <span class="citation" data-cites="Jordanous2012">(<a href="#ref-Jordanous2012">2012</a>)</span>, from a linguistic analysis of creativity literature which identified words that appeared significantly more often in discussions of creativity compared to unrelated topics <span class="citation" data-cites="Jordanous2012">(<a href="#ref-Jordanous2012">2012</a>)</span>.</p>
<blockquote>
<p>The themes identified in this linguistic analysis have collectively provided a clearer “working” understanding of creativity, in the form of components that collectively contribute to our understanding of what creativity is. Together these components act as building blocks for creativity, each contributing to the overall presence of creativity; individually they make creativity more tractable and easier to understand by breaking down this seemingly impenetrable concept into constituent parts.</p><span class="blockcitation" data-cites="Jordanous2012">(Jordanous and Keller <a href="#ref-Jordanous2012">2012</a>)</span>
</blockquote>
<p>The 14 components Jordanous collated are: <span class="citation" data-cites="Jordanous2012">(<a href="#ref-Jordanous2012">2012</a>)</span></p>
<blockquote>
<ol>
<li><p>Active Involvement and Persistence</p></li>
<li><p>Generation of Results</p></li>
<li><p>Dealing with Uncertainty</p></li>
<li><p>Domain Competence</p></li>
<li><p>General Intellect</p></li>
<li><p>Independence and Freedom</p></li>
<li><p>Intention and Emotional Involvement</p></li>
<li><p>Originality</p></li>
<li><p>Progression and Development</p></li>
<li><p>Social Interaction and Communication</p></li>
<li><p>Spontaneity / Subconscious Processing</p></li>
<li><p>Thinking and Evaluation</p></li>
<li><p>Value</p></li>
<li><p>Variety, Divergence and Experimentation</p></li>
</ol>
</blockquote>
<p>Jordanous also found that “evaluation of computational creativity is not being performed in a systematic or standard way” <span class="citation" data-cites="Jordanous2011">(<a href="#ref-Jordanous2011">2011</a>)</span> and proposed ‘Standardised
Procedure for Evaluating Creative Systems (SPECS)’ <span class="citation" data-cites="Jordanous2012a">(<a href="#ref-Jordanous2012a">2012</a>)</span>:</p>
<blockquote>
<ol>
<li><p>Identify a definition of creativity that your system should satisfy to be considered creative:</p>
<ol>
<li><p>What does it mean to be creative in a general context, independent of any domain specifics?</p>
<ul>
<li><p>Research and identify a definition of creativity that you feel offers the most suitable definition of creativity.</p></li>
<li><p>The 14 components of creativity identified in Chapter 4 are strongly suggested as a collective definition of creativity.</p></li>
</ul></li>
<li><p>What aspects of creativity are particularly important in the domain your system works in (and what aspects of creativity are less important in that domain)?</p>
<ul>
<li><p>Adapt the general definition of creativity from Step 1a so that it accurately reflects how creativity is manifested in the domain your system works in.</p></li>
</ul></li>
</ol></li>
<li><p>Using Step 1, clearly state what standards you use to evaluate the creativity of your system.</p>
<ul>
<li><p>Identify the criteria for creativity included in the definition from Step 1 (a and b) and extract them from the definition, expressing each criterion as a separate standard to be tested.</p></li>
<li><p>If using Chapter 4’s components of creativity, as is strongly recommended, then each component becomes one standard to be tested on the system.</p></li>
</ul></li>
<li><p>Test your creative system against the standards stated in Step 2 and report the results.</p>
<ul>
<li><p>For each standard stated in Step 2, devise test(s) to evaluate the system’s performance against that standard.</p></li>
<li><p>The choice of tests to be used is left up to the choice of the individual researcher or research team.</p></li>
<li><p>Consider the test results in terms of how important the associated aspect of creativity is in that domain, with more important aspects of creativity being given greater consideration than less important aspects. It is not necessary, however, to combine all the test results into one aggregate score of creativity.</p></li>
</ul></li>
</ol>
</blockquote>
<p>The SPECS model essentially means that we cannot evaluate a creative computer system objectively, unless steps 1 and 2 are predefined and publically available for external assessors to execute step 3. Creative evaluation can therefore be seen as a move from subjectivity to objectivity, i.e. defining subjective criteria for objectively evaluating a product in terms of the initial criteria.
<blockquote>
<p>For transparent and repeatable evaluative practice, it is necessary to state clearly what standards are used for evaluation, both for appropriate evaluation of a single system and for comparison of multiple systems using common criteria.</p></p><span class="blockcitation" data-cites="Jordanous2012a">(Jordanous <a href="#ref-Jordanous2012a">2012</a>)</span>
</blockquote>
<p>This is further strengthened by Richard Mayer stating that we need a “clearer definition of creativity” <span class="citation" data-cites="Mayer1999">(<a href="#ref-Mayer1999">1999</a>)</span> and Linda Candy arguing for “criteria and measures [for evaluation] that are situated and domain specific” <span class="citation" data-cites="Candy2012">(<a href="#ref-Candy2012">2012</a>)</span>.</p>
<p>Jordanous also defined 5 ‘meta-evaluation criteria’ of correctness, usefulness, faithfulness as a model of creativity, usability of the methodology, and generality <span class="citation" data-cites="Jordanous2014">(<a href="#ref-Jordanous2014">2014</a>)</span>.</p>
</section>

<section id="s:mmce" class="level3">
<h3><span class="header-section-number">7.2.4</span> MMCE</h3>
<p>Linda Candy draws inspiration for the evaluation of (interactive) creative computer systems from Human Computer Interaction (HCI). The focus of HCI evaluation in has been on usability, she says <span class="citation" data-cites="Candy2012">(<a href="#ref-Candy2012">2012</a>)</span>. She argues that in order to successfully evaluate an artefact, the practitioner needs to have “the necessary information including constraints on the options under consideration” <span class="citation" data-cites="Candy2012">(<a href="#ref-Candy2012">2012</a>)</span>.</p>
<p>Evaluation happens at every stage of the process (i.e. from design <span class="math inline">→</span> implementation <span class="math inline">→</span> operation). Some of the key aspects of evaluation Candy highlights are:</p>
<ul>
<li><p>aesthetic appreciation</p></li>
<li><p>audience engagement</p></li>
<li><p>informed considerations</p></li>
<li><p>reflective practice</p></li>
</ul>
<p>She goes on to introduce the Multi-dimensional Model of Creativity and Evaluation (MMCE) (shown in figure <a href="#fig:7.2">7.2</a>) with four main elements of people, process, product and context <span class="citation" data-cites="Candy2012">(<a href="#ref-Candy2012">2012</a>)</span> similar to some of the models of creativity we have seen in chapter <a href="creativity.html">5</a>.</p>

<figure id="fig:7.2">
<img src="../images/mmce.png" alt="Candy’s Multi-dimensional Model of Creativity and Evaluation">
<figcaption>Figure 7.2 Candy’s Multi-dimensional Model of Creativity and Evaluation</figcaption>
</figure>

<p>She proposes the following values or criteria for measurement <span class="citation" data-cites="Candy2012">(<a href="#ref-Candy2012">2012</a>)</span>.</p>
<dl>
<dt>People</dt>
<dd><p>capabilities, characteristics, track record, reputation, impact, influence (profile, demographic, motivation, skills, experience, curiosity, commitment)</p>
</dd>
<dt>Process</dt>
<dd><p>problem finding, solution oriented, exploratory, systematic, practice-based, empirical, reflective, opportunistic, rules, standards (opportunistic, adventurous, curious, cautions, expert, knowledgeable, experienced)</p>
</dd>
<dt>Product</dt>
<dd><p>novel, original, appropriate, useful, surprising, flexible, fluent, engaging (immediate, engaging, enhancing, purposeful, exciting, disturbing)</p>
</dd>
<dt>Context</dt>
<dd><p>studio, living laboratory, public space, museum, constraints, opportunities, acceptability, leading edge (design quality, usable, convincing, adaptable, effective, innovative, transcendent)</p>
</dd>
</dl>
</section>

<section id="s:csf" class="level3">
<h3><span class="header-section-number">7.2.5</span> CSF</h3>
<p>Geraint Wiggins introduced a formal notation and set of rules for the description, analysis and comparison of creative systems called Creative Search Framework (CSF)<span class="citation" data-cites="Wiggins2006">(<a href="#ref-Wiggins2006">2006</a>)</span> which is largely based on Boden’s theory of creativity <span class="citation" data-cites="Boden2003">(<a href="#ref-Boden2003">2003</a>)</span>. The framework uses three criteria for measuring creativity: “relevance, acceptability and quality”. Graeme Ritchie then contributed to this framework with several revisions <span class="citation" data-cites="Ritchie2012">(<a href="#ref-Ritchie2012">2012</a>)</span>.</p>
<p>The CSF provides a formal description for Boden’s concepts of exploratory and transformational creativity. Wiggins’s ‘R–transformation’ and ‘T–transformation’ is akin to Boden’s ‘H-creativity’ and ‘P-creativity’ respectively. To enable the transition from exploratory to transformational creativity in his framework, Wiggins introduced meta-rules which allow us to redefine our conceptual space in a new way.</p>
<p>It is important to note here that the exploratory search in an IR sense (as discussed in section <a href="technology.html#s:browsing">6.1.2</a>) should not be mistaken with the topic at hand. Exploratory search (for a creative solution to a problem) in the Wiggins/Ritchie/Boden sense happens one step before transformational search. This means that we want to end up with transformational tools from this framework (rather than exploratory ones) to use in our exploratory web search system.</p>
<p>Ritchie described the CSF as a set of initial concepts, which create ‘further concepts one after another, thus “exploring the space”’ but also argued that a search system would practically only go through a limited number of steps and therefore proposed some changes and additions to the framework <span class="citation" data-cites="Ritchie2012">(<a href="#ref-Ritchie2012">2012</a>)</span>. He summarised Wiggins’ original CSF as consisting of the following basic elements:</p>
<ol>
<li><p>the universal set of concepts $U$,</p></li>
<li><p>the language for expressing the relevant mappings $L$,</p></li>
<li><p>a symbolic representation of the acceptability map $R$,</p></li>
<li><p>a symbolic representation of the quality mapping $E$,</p></li>
<li><p>a symbolic representation of the search mechanism $T$,</p></li>
<li><p>an interpreter for expressions like 3 and 4 $[ \ ]$, and</p></li>
<li><p>an interpreter for expressions like 5 $⟨ \ ,  \ , \  ⟩$.</p></li>
</ol>
<p>This set of elements is described as the ‘object-level’ (enabling exploratory search). The ‘meta-level’ (enabling transformational search) has the same seven elements with one exception; the universal set of concepts $U$ contains concepts described at the object-level. This allows transformations to happen; concepts from the object-level are searched using criteria and mechanisms (elements 2 to 5) from the meta-level, giving rise to a new and different subset of concepts to those which an object-level search would have produced.</p>
<p>A typical search process would go as follows. We start with an initial set of concepts $C$ that represent our conceptual space and a query. We then explore $C$ and find any elements that match the query with a certain quality (norm and value criteria) in a given amount of iterations. This produces the object-level set of exploratory concepts (in Boden’s sense) which we would call the traditional search results. To get creative results we would need to apply the meta-level search (Boden’s transformational search) with slightly different quality criteria.</p>
<p>Wiggins explained various situations of creativity not taking place (uninspiration and aberration) in terms of his framework as shown below. For example, a system not finding any valuable concepts would be expressed as $[E](U)=0$ (in Wiggins’ original notation). While this approach seems counter-intuitive and impractical, it actually provides an interesting inspiration on how to formulate some of our pataphysical concepts in terms of the CSF (see chapter <a href="aspirations.html#s:pataasp">13.4</a>).</p>
<dl class="defaultDL">
<dt>Hopeless Uninspiration</dt>
<dd>$V_α(X)=∅$ (valued set of concepts is empty)</dd>
<dt>Conceptual Uninspiration</dt>
<dd>$V_α(N_α(X)) = ∅$ (no accepted concepts are valuable)</dd>
<dt>Generative Uninspiration</dt>
<dd>$elements(A)=∅$ (set of reachable concepts is empty)</dd>
<dt>Aberration</dt>
<dd>$B$ is the set of reachable concepts not in $[N]_α(X)$ and $B ≠ ∅$ (search goes outside normal boundaries)</dd>
<dt>Perfect Aberration</dt>
<dd>$V_α(B)=B$</dd>
<dt>Productive Aberration</dt>
<dd>$V_α(B)≠∅$ and $V_α(B)≠B$</dd>
<dt>Pointless Aberration</dt>
<dd>$V_α(B)=∅$</dd>
</dl>
<p>The potential of these definitions of ‘uncreativity’ is further explored in chapter <a href="aspirations.html">13</a>.</p>
</section>

<section id="s:indcrit" class="level3">
<h3><span class="header-section-number">7.2.6</span> Individual Criteria</h3>
<p>Many separate attempts exist at defining an evaluation model that focuses on a single criterion for creativity.</p>
<p>One such example is a model for evaluating the ‘interestingness’ of computer generated plots <span class="citation" data-cites="Perez2013">(Pérez y Pérez and Ortiz <a href="#ref-Perez2013">2013</a>)</span>.</p>
<p>Another approach looks at “quantifying surprise by projecting into the future” <span class="citation" data-cites="Maher2013">(Maher, Brady, and Fisher <a href="#ref-Maher2013">2013</a>)</span>.</p>
<p>Bown looks at “evaluation that is grounded in thinking about interaction design, and inspired by an anthropological understanding of human creative behaviour” <span class="citation" data-cites="Bown2014">(<a href="#ref-Bown2014">2014</a>)</span>. He argues that “systems may only be understood as creative by looking at their interaction with humans using appropriate methodological tools” <span class="citation" data-cites="Bown2014">(<a href="#ref-Bown2014">2014</a>)</span>. He proposed the following methodology.</p>
<ol>
<li><p>The recognition and rigorous application of ‘soft science’ methods wherever vague unoperationalised terms and interpretative language is used.</p></li>
<li><p>An appropriate model of creativity in culture and art that includes the recognition of humans as ‘porous subjects’, and the significant role played by generative creativity in the dynamics of artistic behaviour.</p></li>
</ol>
<p>Others argues that creativity can be measured by looking at the overall ‘unexpectedness’ of an artefact <span class="citation" data-cites="Kazjon2014">(Kazjon and Maher <a href="#ref-Kazjon2014">2013</a>)</span>.</p>
<p>McGregor, Wiggins and Purver introduce the idea of creativity as an “intimation of dualism, with its inherent mental representations, is a thing that typical observers seek when evaluating creativity” <span class="citation" data-cites="Mcgregor2014">(<a href="#ref-Mcgregor2014">2014</a>)</span>.</p>
<p>Another attempt to evaluate computational creativity suggests that systems must go through a sequence of 4 phases “in order to reach a level of creativity acceptable to a set of human judges” <span class="citation" data-cites="Negrete2014">(Negrete-Yankelevich and Morales-Zaragoza <a href="#ref-Negrete2014">2014</a>)</span>. The phases are as follows.</p>
<ol>
<li><p><strong>Structure</strong> is the basic architecture of a piece; it is what allows spectators to make out different parts of it, to analyze it to understand its main organization.</p></li>
<li><p><strong>Plot</strong> is the specialization scaffold of the structure to one purpose; it is the basis for narrative and the most detailed part of planned structure. It is upon plots that pieces are rendered.</p></li>
<li><p><strong>Rendering</strong> is a particular way in which the plot was developed and filled with detail in order to be delivered to the audience.</p></li>
<li><p><strong>Remediation</strong> is the transformation of a creative piece already rendered into another one, re-rendered, possibly into another media.</p></li>
</ol>
<p>França et al. propose a system called <em>Regent-Dependent Creativity</em> (RDC) to address the “lack of domain independent metrics” and which combines “the Bayesian Surprise and Synergy to measure novelty and value, respectively ” <span class="citation" data-cites="Franca2016">(<a href="#ref-Franca2016">2016</a>)</span>.</p>
<p>This dependency relationship is defined by a pair $P(r;d)$ associated with a numeric value $v$, where $r$ is the regent (a feature that contributes to describing an artifact), $d$ is the dependent (it can change the state of an attribute), and $v$ is a value that represents the intensity of a specific pair in different contexts.</p>
<blockquote>
<p>For example, an artifact car can be described by a pair $p_i(color;blue)$, where blue changes the state of the attribute color. The same artifact could also be described by another pair $p_i(drive; home)$, where the dependent home connects a target to the action drive.</p><span class="blockcitation" data-cites="Franca2016">(França et al. <a href="#ref-Franca2016">2016</a>)</span>
</blockquote>
<p>Velde et al. have broken down creativity into 5 main clusters <span class="citation" data-cites="Velde2015">(<a href="#ref-Velde2015">2015</a>)</span>:</p>
<ul>
<li><p>Original (originality)</p></li>
<li><p>Emotion (emotional value)</p></li>
<li><p>Novelty / innovation (innovative)</p></li>
<li><p>Intelligence</p></li>
<li><p>Skill (ability)</p></li>
</ul>
</section>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-Baeza-Yates2011">
<p>Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 2011. <em>Modern Information Retrieval: The Concepts and Technology Behind Search</em>. Harlow, UK: Pearson Education Limited. <a href="http://www.mir2ed.org/" class="uri">http://www.mir2ed.org/</a>.</p>
</div>
<div id="ref-Boden2003">
<p>Boden, Margaret. 2003. <em>The Creative Mind: Myths and Mechanisms</em>. London, UK: Routledge.</p>
</div>
<div id="ref-Bown2014">
<p>Bown, Oliver. 2014. “Empirically Grounding the Evaluation of Creative Systems: Incorporating Interaction Design.” In <em>Proceedings of the Fifth International Conference on Computational Creativity</em>, 112–19. Ljubljana, Slovenia.</p>
</div>
<div id="ref-Candy2012">
<p>Candy, Linda. 2012. “Evaluating Creativity.” In <em>Creativity and Rationale: Enhancing Human Experience by Design</em>, edited by J.M. Carroll. London, UK: Springer.</p>
</div>
<div id="ref-Colton2008">
<p>Colton, Simon. 2008a. “Computational Creativity.” <em>AISB Quarterly</em>, 6–7.</p>
</div>
<div id="ref-Colton2008a">
<p>———. 2008b. “Creativity Versus the Perception of Creativity in Computational Systems.” In <em>Proceedings of the 23rd Aaai Spring Symposium on Creative Intelligent Systems</em>. Chicago, Illinois, USA.</p>
</div>
<div id="ref-Franca2016">
<p>França, Celso, Luís Fabrício Góes, Álvaro Amorim, Rodrigo Rocha, and Alysson Ribeiro de Silva. 2016. “Regent-Dependent Creativity: A Domain Independent Metric for the Assessment of Creative Artifacts.” In <em>Proceedings of the Seventh International Conference on Computational Creativity</em>, 68–75.</p>
</div>
<div id="ref-Jordanous2011">
<p>Jordanous, Anna Katerina. 2011. “Evaluating Evaluation : Assessing Progress in Computational Creativity Research.” In <em>Proceedings of the Second International Conference on Computational Creativity</em>.</p>
</div>
<div id="ref-Jordanous2012a">
<p>———. 2012. “Evaluating Computational Creativity: A Standardised Procedure for Evaluating Creative Systems and its Application.” PhD thesis, University of Sussex.</p>
</div>
<div id="ref-Jordanous2014">
<p>———. 2014. “Stepping Back to Progress Forwards: Setting Standards for Meta-Evaluation of Computational Creativity.” In <em>Proceedings of the Fifth International Conference on Computational Creativity</em>, 129–36.</p>
</div>
<div id="ref-Jordanous2012">
<p>Jordanous, Anna Katerina, and Bill Keller. 2012. “Weaving creativity into the Semantic Web: a language-processing approach.” In <em>Proceedings of the 3rd International Conference on Computational Creativity</em>, 216–20.</p>
</div>
<div id="ref-Kazjon2014">
<p>Kazjon, Grace, and Mary Lou Maher. 2013. “What to expect when you’re expecting: The role of unexpectedness in computationally evaluating creativity.” In <em>Proceedings of the Fifth International Conference on Computational Creativity</em>, 120–28.</p>
</div>
<div id="ref-Lamb2015">
<p>Lamb, Carolyn, Daniel Brown, and Charles Clarke. 2015. “Human Competence in Creativity Evaluation.” In <em>Proceedings of the Sixth International Conference on Computational Creativity</em>, 102–9.</p>
</div>
<div id="ref-Maher2013">
<p>Maher, Mary Lou, Katherine Brady, and Douglas Fisher. 2013. “Computational Models of Surprise in Evaluating Creative Design.” In <em>Proceedings of the Fourth International Conference on Computational Creativity</em>, 147–51.</p>
</div>
<div id="ref-Mayer1999">
<p>Mayer, Richard E. 1999. “Fifty Years of Creativity Research.” In <em>Handbook of Creativity</em>, edited by Robert J Sternberg, 449–60. New York: Cambridge University Press.</p>
</div>
<div id="ref-Mcgregor2014">
<p>McGregor, Stephen, Geraint Wiggins, and Matthew Purver. 2014. “Computational Creativity: A Philosophical Approach, and an Approach to Philosophy.” In <em>Proceedings of the Fifth International Conference on Computational Creativity</em>, 254–62.</p>
</div>
<div id="ref-Negrete2014">
<p>Negrete-Yankelevich, Santiago, and Nora Morales-Zaragoza. 2014. “The apprentice framework: planning and assessing creativity.” In <em>Proceedings of the Fifth International Conference on Computational Creativity</em>, 280–83.</p>
</div>
<div id="ref-Norton2015">
<p>Norton, David, Derrall Heath, and Dan Ventura. 2015. “Accounting for Bias in the Evaluation of Creative Computational Systems: An Assessment of Darci.” In <em>Proceedings of the Sixth International Conference on Computational Creativity</em>, 31–38.</p>
</div>
<div id="ref-Pease2011">
<p>Pease, Alison, and Simon Colton. 2011. “On impact and evaluation in Computational Creativity: A discussion of the Turing Test and an alternative proposal.” In <em>Proceedings of the Artificial Intelligence and Simulation of Behaviour Conference</em>.</p>
</div>
<div id="ref-Pease2013">
<p>Pease, Alison, Simon Colton, Ramin Ramezani, John Charnley, and Kate Reed. 2013. “A Discussion on Serendipity in Creative Systems.” In <em>Proceedings of the 4th International Conference on Computational Creativity</em>, 64–71. Sydney, Australia: University of Sydney.</p>
</div>
<div id="ref-Pease2001">
<p>Pease, Alison, Daniel Winterstein, and Simon Colton. 2001. “Evaluating Machine Creativity.” In <em>Proceedings of Iccbr Workshop on Approaches to Creativity</em>, 129–37.</p>
</div>
<div id="ref-Perez2013">
<p>Pérez y Pérez, Rafael, and Otoniel Ortiz. 2013. “A model for evaluating interestingness in a computer-generated plot.” In <em>Proceedings of the Fourth International Conference on Computational Creativity</em>, 131–38.</p>
</div>
<div id="ref-Piffer2012">
<p>Piffer, Davide. 2012. “Can creativity be measured? An attempt to clarify the notion of creativity and general directions for future research.” <em>Thinking Skills and Creativity</em> 7 (3). Elsevier Ltd: 258–64.</p>
</div>
<div id="ref-Ritchie2001">
<p>Ritchie, Graeme. 2001. “Assessing creativity.” In <em>Proceedings of Symposium on Artificial Intelligence and Creativity in Arts and Science</em>, 3–11.</p>
</div>
<div id="ref-Ritchie2007">
<p>———. 2007. “Some Empirical Criteria for Attributing Creativity to a Computer Program.” <em>Minds and Machines</em> 17 (1): 67–99.</p>
</div>
<div id="ref-Ritchie2012">
<p>———. 2012. “A closer look at creativity as search.” In <em>Proceedings of the International Conference on Computational Creativity</em>, 41–48.</p>
</div>
<div id="ref-Sawle2011">
<p>Sawle, James, Fania Raczinski, and Hongji Yang. 2011. “A Framework for Creativity in Search Results.” In <em>Proceedings of the 3rd International Conference on Creative Content Technologies</em>, 54–57. Rome.</p>
</div>
<div id="ref-Schmidhuber2006a">
<p>Schmidhuber, Jürgen. 2006. “New Millennium Ai and the Convergence of History.” In <em>Singularity Hypotheses</em>, edited by Amnon Eden, James Moor, Johnny Soraker, and Eric Steinhart, 60–82.</p>
</div>
<div id="ref-Nist2016">
<p>“Text Retrieval Conference (Trec).” 2016. National Institute of Standards; Technology. <a href="http://trec.nist.gov/" class="uri">http://trec.nist.gov/</a>.</p>
</div>
<div id="ref-Trec2011">
<p>“TREC Web, Terabyte &amp; Blog Tracks. Web Research Collections.” 2011. University of Glasgow. <a href="http://ir.dcs.gla.ac.uk/test\_collections/" class="uri">http://ir.dcs.gla.ac.uk/test\_collections/</a>.</p>
</div>
<div id="ref-Velde2015">
<p>van der Velde, Frank, Roger Wolf, Martin Schmettow, and Deniece Nazareth. 2015. “A Semantic Map for Evaluating Creativity.” In <em>Proceedings of the Sixth International Conference on Computational Creativity</em>, 94–101.</p>
</div>
<div id="ref-Ventura2008">
<p>Ventura, Dan. 2008. “A Reductio Ad Absurdum Experiment in Sufficiency for Evaluating (Computational) Creative Systems.” In <em>Proceedings of the 5th International Joint Workshop on Computational Creativty</em>. Madrid, Spain.</p>
</div>
<div id="ref-Wikimedia2014">
<p>Walber. 2014. “File:Precisionrecall.svg.” Wikimedia Commons. <a href="https://commons.wikimedia.org/wiki/File:Precisionrecall.svg" class="uri">https://commons.wikimedia.org/wiki/File:Precisionrecall.svg</a>.</p>
</div>
<div id="ref-Wiggins2006">
<p>Wiggins, Geraint. 2006. “A preliminary framework for description, analysis and comparison of creative systems.” <em>Knowledge Based Systems</em> 19 (7): 449–58.</p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is problematic until the posited singularity <span class="citation" data-cites="Schmidhuber2006a">(Schmidhuber <a href="#ref-Schmidhuber2006a">2006</a>)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</section>
<nav class="mainnav">
  <a class="left" href="technology.html">Prev</a>
  <a class="centre" href="contents.html">Contents</a>
  <a class="right" href="part3.html">Next</a>
</nav>
</body>
</html>
